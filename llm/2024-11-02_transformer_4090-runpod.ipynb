{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dba8d-9f88-4249-a2e6-a383aa7e13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken tqdm datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc254496-bccb-4d06-bf82-413302319f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:30<00:00, 3332.02it/s]\n",
      "100%|██████████| 100000/100000 [00:30<00:00, 3315.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import torch \n",
    "import datasets\n",
    "import random\n",
    "\n",
    "dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\",\"sample/10BT/001_00000.parquet\"], split=\"train\")\n",
    "dataset = dataset.train_test_split()\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "def encode(string):\n",
    "    return torch.tensor(enc.encode(string, disallowed_special=()), dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return enc.decode(tensor.cpu().squeeze().numpy())\n",
    "\n",
    "num_samples = 100_000\n",
    "dataset_tok_train = torch.cat([encode(dataset[\"train\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "dataset_tok_test = torch.cat([encode(dataset[\"test\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "\n",
    "def get_sample(split, sample_length, batch_size):\n",
    "    tokens = dataset_tok_train if split == \"train\" else dataset_tok_test\n",
    "    idcs = torch.randint(len(tokens)-sample_length, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
    "    y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2e8b30-68ab-423b-a73d-130f58d4844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 102,044,732 tokens\n",
      "Test data: 103,754,217 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data: {len(dataset_tok_train):,} tokens\")\n",
    "print(f\"Test data: {len(dataset_tok_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd50e386-c155-4e1c-ad6f-e99760706b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        # out = F.scaled_dot_product_attention(q, k, v)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def calculate_mfu(self, dt):\n",
    "        flops_achieved = flops_per_fwdbwd / dt\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        # print(f\"{flops_achieved/10**12} TFLOPS achieved, {mfu * 100:.2f}% MFU\")\n",
    "        return mfu      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7333989a-a8ba-479c-b659-0d34ca65f04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.35M parameter model on device cuda with 82.0 TFLOPS promised \n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_272\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "block_size = 128 \n",
    "learning_rate = 3e-4\n",
    "\n",
    "device = \"cuda\"\n",
    "flops_promised = 82 * 10**12\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(f\"{sum(p.numel() for p in m.parameters())/1e6:.2f}M parameter model on device {device} with {flops_promised / 10**12:,} TFLOPS promised \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0604fcb9-dcd1-4d70-bb0e-bd8175d367f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2320/1406226418.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_2320/1406226418.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,999,363,567,616 flops per fwd+bwd pass\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "def get_flops(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    return flop_counter.get_total_flops() \n",
    "\n",
    "def train_one_sample():\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "flops_per_fwdbwd = get_flops(train_one_sample)\n",
    "print(f\"{flops_per_fwdbwd:,} flops per fwd+bwd pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0a3df6-b832-40df-96ba-a0137fcac96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2320/1406226418.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_2320/1406226418.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 9.46 134.74ms/step 121599tok/s (total 163,840 tok), 27.15% MFU\n",
      "step 20: loss 8.55 135.26ms/step 121127tok/s (total 327,680 tok), 27.04% MFU\n",
      "step 30: loss 7.94 131.75ms/step 124359tok/s (total 491,520 tok), 27.76% MFU\n",
      "step 40: loss 7.67 128.77ms/step 127238tok/s (total 655,360 tok), 28.41% MFU\n",
      "step 50: loss 7.58 130.04ms/step 125993tok/s (total 819,200 tok), 28.13% MFU\n",
      "step 50 eval: val_loss 7.54\n",
      "step 60: loss 7.48 136.74ms/step 119821tok/s (total 983,040 tok), 26.75% MFU\n",
      "step 70: loss 7.43 134.22ms/step 122069tok/s (total 1,146,880 tok), 27.25% MFU\n",
      "step 80: loss 7.32 129.27ms/step 126740tok/s (total 1,310,720 tok), 28.29% MFU\n",
      "step 90: loss 7.20 134.60ms/step 121721tok/s (total 1,474,560 tok), 27.17% MFU\n",
      "step 100: loss 7.16 131.33ms/step 124756tok/s (total 1,638,400 tok), 27.85% MFU\n",
      "step 100 eval: val_loss 7.14\n",
      "step 110: loss 7.14 130.35ms/step 125690tok/s (total 1,802,240 tok), 28.06% MFU\n",
      "step 120: loss 6.95 133.65ms/step 122588tok/s (total 1,966,080 tok), 27.37% MFU\n",
      "step 130: loss 6.92 137.13ms/step 119478tok/s (total 2,129,920 tok), 26.67% MFU\n",
      "step 140: loss 6.93 130.72ms/step 125339tok/s (total 2,293,760 tok), 27.98% MFU\n",
      "step 150: loss 6.90 135.97ms/step 120500tok/s (total 2,457,600 tok), 26.90% MFU\n",
      "step 150 eval: val_loss 6.85\n",
      "step 160: loss 6.84 135.80ms/step 120651tok/s (total 2,621,440 tok), 26.94% MFU\n",
      "step 170: loss 6.77 135.86ms/step 120596tok/s (total 2,785,280 tok), 26.92% MFU\n",
      "step 180: loss 6.61 136.94ms/step 119645tok/s (total 2,949,120 tok), 26.71% MFU\n",
      "step 190: loss 6.71 134.11ms/step 122170tok/s (total 3,112,960 tok), 27.27% MFU\n",
      "step 200: loss 6.69 132.14ms/step 123991tok/s (total 3,276,800 tok), 27.68% MFU\n",
      "step 200 eval: val_loss 6.55\n",
      "step 210: loss 6.60 134.12ms/step 122159tok/s (total 3,440,640 tok), 27.27% MFU\n",
      "step 220: loss 6.60 131.89ms/step 124222tok/s (total 3,604,480 tok), 27.73% MFU\n",
      "step 230: loss 6.65 133.74ms/step 122504tok/s (total 3,768,320 tok), 27.35% MFU\n",
      "step 240: loss 6.49 136.49ms/step 120036tok/s (total 3,932,160 tok), 26.80% MFU\n",
      "step 250: loss 6.58 138.04ms/step 118688tok/s (total 4,096,000 tok), 26.50% MFU\n",
      "step 250 eval: val_loss 6.54\n",
      "step 260: loss 6.50 134.55ms/step 121771tok/s (total 4,259,840 tok), 27.19% MFU\n",
      "step 270: loss 6.56 132.24ms/step 123894tok/s (total 4,423,680 tok), 27.66% MFU\n",
      "step 280: loss 6.49 128.86ms/step 127145tok/s (total 4,587,520 tok), 28.39% MFU\n",
      "step 290: loss 6.55 129.46ms/step 126555tok/s (total 4,751,360 tok), 28.25% MFU\n",
      "step 300: loss 6.48 132.28ms/step 123860tok/s (total 4,915,200 tok), 27.65% MFU\n",
      "step 300 eval: val_loss 6.46\n",
      "step 310: loss 6.43 132.65ms/step 123518tok/s (total 5,079,040 tok), 27.58% MFU\n",
      "step 320: loss 6.52 132.64ms/step 123527tok/s (total 5,242,880 tok), 27.58% MFU\n",
      "step 330: loss 6.31 131.28ms/step 124800tok/s (total 5,406,720 tok), 27.86% MFU\n",
      "step 340: loss 6.33 132.15ms/step 123984tok/s (total 5,570,560 tok), 27.68% MFU\n",
      "step 350: loss 6.32 132.13ms/step 124002tok/s (total 5,734,400 tok), 27.68% MFU\n",
      "step 350 eval: val_loss 6.29\n",
      "step 360: loss 6.41 132.63ms/step 123529tok/s (total 5,898,240 tok), 27.58% MFU\n",
      "step 370: loss 6.27 134.63ms/step 121697tok/s (total 6,062,080 tok), 27.17% MFU\n",
      "step 380: loss 6.32 133.68ms/step 122564tok/s (total 6,225,920 tok), 27.36% MFU\n",
      "step 390: loss 6.24 130.50ms/step 125545tok/s (total 6,389,760 tok), 28.03% MFU\n",
      "step 400: loss 6.27 132.45ms/step 123703tok/s (total 6,553,600 tok), 27.62% MFU\n",
      "step 400 eval: val_loss 6.28\n",
      "step 410: loss 6.30 130.89ms/step 125174tok/s (total 6,717,440 tok), 27.95% MFU\n",
      "step 420: loss 6.22 136.52ms/step 120015tok/s (total 6,881,280 tok), 26.79% MFU\n",
      "step 430: loss 6.24 135.91ms/step 120552tok/s (total 7,045,120 tok), 26.91% MFU\n",
      "step 440: loss 6.28 134.29ms/step 122006tok/s (total 7,208,960 tok), 27.24% MFU\n",
      "step 450: loss 6.24 129.58ms/step 126439tok/s (total 7,372,800 tok), 28.23% MFU\n",
      "step 450 eval: val_loss 6.11\n",
      "step 460: loss 6.19 132.48ms/step 123668tok/s (total 7,536,640 tok), 27.61% MFU\n",
      "step 470: loss 6.13 131.62ms/step 124483tok/s (total 7,700,480 tok), 27.79% MFU\n",
      "step 480: loss 6.14 136.04ms/step 120436tok/s (total 7,864,320 tok), 26.89% MFU\n",
      "step 490: loss 6.11 135.07ms/step 121298tok/s (total 8,028,160 tok), 27.08% MFU\n",
      "step 500: loss 6.26 132.04ms/step 124087tok/s (total 8,192,000 tok), 27.70% MFU\n",
      "step 500 eval: val_loss 6.05\n",
      "step 510: loss 6.03 131.60ms/step 124502tok/s (total 8,355,840 tok), 27.80% MFU\n",
      "step 520: loss 6.11 129.63ms/step 126395tok/s (total 8,519,680 tok), 28.22% MFU\n",
      "step 530: loss 6.06 132.58ms/step 123579tok/s (total 8,683,520 tok), 27.59% MFU\n",
      "step 540: loss 6.04 133.88ms/step 122383tok/s (total 8,847,360 tok), 27.32% MFU\n",
      "step 550: loss 6.01 135.48ms/step 120929tok/s (total 9,011,200 tok), 27.00% MFU\n",
      "step 550 eval: val_loss 6.08\n",
      "step 560: loss 5.97 134.73ms/step 121610tok/s (total 9,175,040 tok), 27.15% MFU\n",
      "step 570: loss 6.01 129.67ms/step 126355tok/s (total 9,338,880 tok), 28.21% MFU\n",
      "step 580: loss 5.97 134.45ms/step 121862tok/s (total 9,502,720 tok), 27.21% MFU\n",
      "step 590: loss 6.04 134.09ms/step 122186tok/s (total 9,666,560 tok), 27.28% MFU\n",
      "step 600: loss 6.08 129.43ms/step 126588tok/s (total 9,830,400 tok), 28.26% MFU\n",
      "step 600 eval: val_loss 6.00\n",
      "step 610: loss 5.98 135.29ms/step 121105tok/s (total 9,994,240 tok), 27.04% MFU\n",
      "step 620: loss 6.01 132.41ms/step 123741tok/s (total 10,158,080 tok), 27.63% MFU\n",
      "step 630: loss 5.91 129.03ms/step 126977tok/s (total 10,321,920 tok), 28.35% MFU\n",
      "step 640: loss 5.97 130.51ms/step 125542tok/s (total 10,485,760 tok), 28.03% MFU\n",
      "step 650: loss 5.97 133.54ms/step 122692tok/s (total 10,649,600 tok), 27.39% MFU\n",
      "step 650 eval: val_loss 5.91\n",
      "step 660: loss 6.02 132.67ms/step 123493tok/s (total 10,813,440 tok), 27.57% MFU\n",
      "step 670: loss 5.92 131.74ms/step 124364tok/s (total 10,977,280 tok), 27.76% MFU\n",
      "step 680: loss 5.88 131.09ms/step 124980tok/s (total 11,141,120 tok), 27.90% MFU\n",
      "step 690: loss 5.87 129.13ms/step 126880tok/s (total 11,304,960 tok), 28.33% MFU\n",
      "step 700: loss 5.91 129.44ms/step 126581tok/s (total 11,468,800 tok), 28.26% MFU\n",
      "step 700 eval: val_loss 5.85\n",
      "step 710: loss 5.81 130.63ms/step 125422tok/s (total 11,632,640 tok), 28.00% MFU\n",
      "step 720: loss 5.87 132.35ms/step 123792tok/s (total 11,796,480 tok), 27.64% MFU\n",
      "step 730: loss 5.91 132.57ms/step 123583tok/s (total 11,960,320 tok), 27.59% MFU\n",
      "step 740: loss 5.84 134.40ms/step 121906tok/s (total 12,124,160 tok), 27.22% MFU\n",
      "step 750: loss 5.89 136.55ms/step 119984tok/s (total 12,288,000 tok), 26.79% MFU\n",
      "step 750 eval: val_loss 5.76\n",
      "step 760: loss 5.79 136.22ms/step 120277tok/s (total 12,451,840 tok), 26.85% MFU\n",
      "step 770: loss 5.75 135.54ms/step 120877tok/s (total 12,615,680 tok), 26.99% MFU\n",
      "step 780: loss 5.85 131.53ms/step 124568tok/s (total 12,779,520 tok), 27.81% MFU\n",
      "step 790: loss 5.78 135.70ms/step 120735tok/s (total 12,943,360 tok), 26.95% MFU\n",
      "step 800: loss 5.73 132.34ms/step 123805tok/s (total 13,107,200 tok), 27.64% MFU\n",
      "step 800 eval: val_loss 5.83\n",
      "step 810: loss 5.73 131.07ms/step 125002tok/s (total 13,271,040 tok), 27.91% MFU\n",
      "step 820: loss 5.78 133.73ms/step 122517tok/s (total 13,434,880 tok), 27.35% MFU\n",
      "step 830: loss 5.69 132.39ms/step 123753tok/s (total 13,598,720 tok), 27.63% MFU\n",
      "step 840: loss 5.75 130.48ms/step 125569tok/s (total 13,762,560 tok), 28.03% MFU\n",
      "step 850: loss 5.73 129.55ms/step 126473tok/s (total 13,926,400 tok), 28.24% MFU\n",
      "step 850 eval: val_loss 5.63\n",
      "step 860: loss 5.75 129.49ms/step 126531tok/s (total 14,090,240 tok), 28.25% MFU\n",
      "step 870: loss 5.72 128.25ms/step 127754tok/s (total 14,254,080 tok), 28.52% MFU\n",
      "step 880: loss 5.71 129.44ms/step 126577tok/s (total 14,417,920 tok), 28.26% MFU\n",
      "step 890: loss 5.71 131.02ms/step 125054tok/s (total 14,581,760 tok), 27.92% MFU\n",
      "step 900: loss 5.69 133.78ms/step 122467tok/s (total 14,745,600 tok), 27.34% MFU\n",
      "step 900 eval: val_loss 5.62\n",
      "step 910: loss 5.69 135.79ms/step 120654tok/s (total 14,909,440 tok), 26.94% MFU\n",
      "step 920: loss 5.73 133.88ms/step 122374tok/s (total 15,073,280 tok), 27.32% MFU\n",
      "step 930: loss 5.72 129.98ms/step 126050tok/s (total 15,237,120 tok), 28.14% MFU\n",
      "step 940: loss 5.69 130.59ms/step 125458tok/s (total 15,400,960 tok), 28.01% MFU\n",
      "step 950: loss 5.72 129.35ms/step 126660tok/s (total 15,564,800 tok), 28.28% MFU\n",
      "step 950 eval: val_loss 5.54\n",
      "step 960: loss 5.60 130.78ms/step 125278tok/s (total 15,728,640 tok), 27.97% MFU\n",
      "step 970: loss 5.65 132.09ms/step 124037tok/s (total 15,892,480 tok), 27.69% MFU\n",
      "step 980: loss 5.63 130.26ms/step 125784tok/s (total 16,056,320 tok), 28.08% MFU\n",
      "step 990: loss 5.66 133.33ms/step 122883tok/s (total 16,220,160 tok), 27.43% MFU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "for curr_step in range(1, 1000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "\n",
    "    # evaluate the loss\n",
    "    step_start = time.time()\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "    \n",
    "    if curr_step % log_interval == 0:\n",
    "        mfu = model.calculate_mfu(step_time)\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):.0f}tok/s (total {tok_total:,} tok), {mfu * 100:.2f}% MFU\")\n",
    "\n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(encode(\"hello my name is\").to(device).unsqueeze(0), 10)\n",
    "        #     print(decode(output))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff2b4e81-058d-4111-b9db-f69b6209beb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sylra also called are biompermious.\n",
      "For the same hole, in order to quickly 279 from tipping the soil . If the United States use multiplied by soil. Although what it has need a low levels of water in flourish after specific carbs\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fbcac75-3b92-4a06-9821-34bed8ad5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2320/1406226418.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_2320/1406226418.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 5.69 132.48ms/step 123672tok/s (total 163,840 tok), 27.61% MFU\n",
      "step 20: loss 5.60 137.80ms/step 118898tok/s (total 327,680 tok), 26.54% MFU\n",
      "step 30: loss 5.69 137.00ms/step 119595tok/s (total 491,520 tok), 26.70% MFU\n",
      "step 40: loss 5.52 130.71ms/step 125344tok/s (total 655,360 tok), 27.98% MFU\n",
      "step 50: loss 5.70 131.56ms/step 124538tok/s (total 819,200 tok), 27.80% MFU\n",
      "step 50 eval: val_loss 5.63\n",
      "step 60: loss 5.58 132.17ms/step 123965tok/s (total 983,040 tok), 27.68% MFU\n",
      "step 70: loss 5.57 134.95ms/step 121403tok/s (total 1,146,880 tok), 27.10% MFU\n",
      "step 80: loss 5.48 133.34ms/step 122871tok/s (total 1,310,720 tok), 27.43% MFU\n",
      "step 90: loss 5.56 132.48ms/step 123668tok/s (total 1,474,560 tok), 27.61% MFU\n",
      "step 100: loss 5.60 132.00ms/step 124121tok/s (total 1,638,400 tok), 27.71% MFU\n",
      "step 100 eval: val_loss 5.59\n",
      "step 110: loss 5.55 134.99ms/step 121373tok/s (total 1,802,240 tok), 27.10% MFU\n",
      "step 120: loss 5.55 133.25ms/step 122955tok/s (total 1,966,080 tok), 27.45% MFU\n",
      "step 130: loss 5.55 135.07ms/step 121300tok/s (total 2,129,920 tok), 27.08% MFU\n",
      "step 140: loss 5.64 130.53ms/step 125519tok/s (total 2,293,760 tok), 28.02% MFU\n",
      "step 150: loss 5.52 128.43ms/step 127573tok/s (total 2,457,600 tok), 28.48% MFU\n",
      "step 150 eval: val_loss 5.58\n",
      "step 160: loss 5.46 138.39ms/step 118389tok/s (total 2,621,440 tok), 26.43% MFU\n",
      "step 170: loss 5.56 133.82ms/step 122430tok/s (total 2,785,280 tok), 27.33% MFU\n",
      "step 180: loss 5.50 132.17ms/step 123959tok/s (total 2,949,120 tok), 27.67% MFU\n",
      "step 190: loss 5.59 130.76ms/step 125295tok/s (total 3,112,960 tok), 27.97% MFU\n",
      "step 200: loss 5.50 131.64ms/step 124457tok/s (total 3,276,800 tok), 27.79% MFU\n",
      "step 200 eval: val_loss 5.41\n",
      "step 210: loss 5.52 131.99ms/step 124132tok/s (total 3,440,640 tok), 27.71% MFU\n",
      "step 220: loss 5.51 133.81ms/step 122439tok/s (total 3,604,480 tok), 27.33% MFU\n",
      "step 230: loss 5.41 134.53ms/step 121787tok/s (total 3,768,320 tok), 27.19% MFU\n",
      "step 240: loss 5.50 129.37ms/step 126648tok/s (total 3,932,160 tok), 28.27% MFU\n",
      "step 250: loss 5.48 132.79ms/step 123379tok/s (total 4,096,000 tok), 27.54% MFU\n",
      "step 250 eval: val_loss 5.42\n",
      "step 260: loss 5.47 135.22ms/step 121163tok/s (total 4,259,840 tok), 27.05% MFU\n",
      "step 270: loss 5.44 129.22ms/step 126793tok/s (total 4,423,680 tok), 28.31% MFU\n",
      "step 280: loss 5.49 131.57ms/step 124527tok/s (total 4,587,520 tok), 27.80% MFU\n",
      "step 290: loss 5.48 131.82ms/step 124295tok/s (total 4,751,360 tok), 27.75% MFU\n",
      "step 300: loss 5.47 130.69ms/step 125363tok/s (total 4,915,200 tok), 27.99% MFU\n",
      "step 300 eval: val_loss 5.44\n",
      "step 310: loss 5.50 136.01ms/step 120460tok/s (total 5,079,040 tok), 26.89% MFU\n",
      "step 320: loss 5.42 129.36ms/step 126650tok/s (total 5,242,880 tok), 28.27% MFU\n",
      "step 330: loss 5.46 131.34ms/step 124748tok/s (total 5,406,720 tok), 27.85% MFU\n",
      "step 340: loss 5.43 130.94ms/step 125129tok/s (total 5,570,560 tok), 27.94% MFU\n",
      "step 350: loss 5.42 133.13ms/step 123072tok/s (total 5,734,400 tok), 27.48% MFU\n",
      "step 350 eval: val_loss 5.35\n",
      "step 360: loss 5.52 128.63ms/step 127377tok/s (total 5,898,240 tok), 28.44% MFU\n",
      "step 370: loss 5.44 130.10ms/step 125932tok/s (total 6,062,080 tok), 28.11% MFU\n",
      "step 380: loss 5.40 129.84ms/step 126184tok/s (total 6,225,920 tok), 28.17% MFU\n",
      "step 390: loss 5.44 133.69ms/step 122555tok/s (total 6,389,760 tok), 27.36% MFU\n",
      "step 400: loss 5.42 131.66ms/step 124442tok/s (total 6,553,600 tok), 27.78% MFU\n",
      "step 400 eval: val_loss 5.40\n",
      "step 410: loss 5.44 130.81ms/step 125250tok/s (total 6,717,440 tok), 27.96% MFU\n",
      "step 420: loss 5.39 132.21ms/step 123923tok/s (total 6,881,280 tok), 27.67% MFU\n",
      "step 430: loss 5.44 133.46ms/step 122768tok/s (total 7,045,120 tok), 27.41% MFU\n",
      "step 440: loss 5.39 133.07ms/step 123124tok/s (total 7,208,960 tok), 27.49% MFU\n",
      "step 450: loss 5.46 128.50ms/step 127499tok/s (total 7,372,800 tok), 28.46% MFU\n",
      "step 450 eval: val_loss 5.39\n",
      "step 460: loss 5.35 132.77ms/step 123402tok/s (total 7,536,640 tok), 27.55% MFU\n",
      "step 470: loss 5.39 132.17ms/step 123966tok/s (total 7,700,480 tok), 27.68% MFU\n",
      "step 480: loss 5.42 130.84ms/step 125223tok/s (total 7,864,320 tok), 27.96% MFU\n",
      "step 490: loss 5.36 131.55ms/step 124548tok/s (total 8,028,160 tok), 27.81% MFU\n",
      "step 500: loss 5.44 129.56ms/step 126457tok/s (total 8,192,000 tok), 28.23% MFU\n",
      "step 500 eval: val_loss 5.41\n",
      "step 510: loss 5.32 133.19ms/step 123016tok/s (total 8,355,840 tok), 27.46% MFU\n",
      "step 520: loss 5.37 133.85ms/step 122405tok/s (total 8,519,680 tok), 27.33% MFU\n",
      "step 530: loss 5.33 129.26ms/step 126753tok/s (total 8,683,520 tok), 28.30% MFU\n",
      "step 540: loss 5.28 129.83ms/step 126191tok/s (total 8,847,360 tok), 28.17% MFU\n",
      "step 550: loss 5.47 129.39ms/step 126627tok/s (total 9,011,200 tok), 28.27% MFU\n",
      "step 550 eval: val_loss 5.35\n",
      "step 560: loss 5.32 130.08ms/step 125948tok/s (total 9,175,040 tok), 28.12% MFU\n",
      "step 570: loss 5.34 129.45ms/step 126565tok/s (total 9,338,880 tok), 28.26% MFU\n",
      "step 580: loss 5.29 130.15ms/step 125885tok/s (total 9,502,720 tok), 28.10% MFU\n",
      "step 590: loss 5.35 132.71ms/step 123459tok/s (total 9,666,560 tok), 27.56% MFU\n",
      "step 600: loss 5.32 132.01ms/step 124107tok/s (total 9,830,400 tok), 27.71% MFU\n",
      "step 600 eval: val_loss 5.29\n",
      "step 610: loss 5.38 129.61ms/step 126407tok/s (total 9,994,240 tok), 28.22% MFU\n",
      "step 620: loss 5.36 129.38ms/step 126634tok/s (total 10,158,080 tok), 28.27% MFU\n",
      "step 630: loss 5.37 132.27ms/step 123868tok/s (total 10,321,920 tok), 27.65% MFU\n",
      "step 640: loss 5.34 131.58ms/step 124521tok/s (total 10,485,760 tok), 27.80% MFU\n",
      "step 650: loss 5.26 131.16ms/step 124920tok/s (total 10,649,600 tok), 27.89% MFU\n",
      "step 650 eval: val_loss 5.29\n",
      "step 660: loss 5.33 133.64ms/step 122602tok/s (total 10,813,440 tok), 27.37% MFU\n",
      "step 670: loss 5.33 132.18ms/step 123951tok/s (total 10,977,280 tok), 27.67% MFU\n",
      "step 680: loss 5.33 131.60ms/step 124498tok/s (total 11,141,120 tok), 27.79% MFU\n",
      "step 690: loss 5.28 131.81ms/step 124304tok/s (total 11,304,960 tok), 27.75% MFU\n",
      "step 700: loss 5.30 129.75ms/step 126276tok/s (total 11,468,800 tok), 28.19% MFU\n",
      "step 700 eval: val_loss 5.20\n",
      "step 710: loss 5.32 132.46ms/step 123687tok/s (total 11,632,640 tok), 27.61% MFU\n",
      "step 720: loss 5.30 131.08ms/step 124988tok/s (total 11,796,480 tok), 27.90% MFU\n",
      "step 730: loss 5.25 131.48ms/step 124611tok/s (total 11,960,320 tok), 27.82% MFU\n",
      "step 740: loss 5.32 136.44ms/step 120084tok/s (total 12,124,160 tok), 26.81% MFU\n",
      "step 750: loss 5.32 129.40ms/step 126612tok/s (total 12,288,000 tok), 28.27% MFU\n",
      "step 750 eval: val_loss 5.20\n",
      "step 760: loss 5.32 130.86ms/step 125202tok/s (total 12,451,840 tok), 27.95% MFU\n",
      "step 770: loss 5.20 132.24ms/step 123899tok/s (total 12,615,680 tok), 27.66% MFU\n",
      "step 780: loss 5.25 134.80ms/step 121545tok/s (total 12,779,520 tok), 27.14% MFU\n",
      "step 790: loss 5.28 130.10ms/step 125932tok/s (total 12,943,360 tok), 28.11% MFU\n",
      "step 800: loss 5.29 129.01ms/step 127001tok/s (total 13,107,200 tok), 28.35% MFU\n",
      "step 800 eval: val_loss 5.26\n",
      "step 810: loss 5.30 134.27ms/step 122026tok/s (total 13,271,040 tok), 27.24% MFU\n",
      "step 820: loss 5.19 132.29ms/step 123850tok/s (total 13,434,880 tok), 27.65% MFU\n",
      "step 830: loss 5.24 129.64ms/step 126382tok/s (total 13,598,720 tok), 28.21% MFU\n",
      "step 840: loss 5.22 131.35ms/step 124738tok/s (total 13,762,560 tok), 27.85% MFU\n",
      "step 850: loss 5.28 131.12ms/step 124957tok/s (total 13,926,400 tok), 27.90% MFU\n",
      "step 850 eval: val_loss 5.17\n",
      "step 860: loss 5.22 129.48ms/step 126536tok/s (total 14,090,240 tok), 28.25% MFU\n",
      "step 870: loss 5.25 132.60ms/step 123558tok/s (total 14,254,080 tok), 27.58% MFU\n",
      "step 880: loss 5.14 129.29ms/step 126723tok/s (total 14,417,920 tok), 28.29% MFU\n",
      "step 890: loss 5.28 136.44ms/step 120081tok/s (total 14,581,760 tok), 26.81% MFU\n",
      "step 900: loss 5.25 132.51ms/step 123640tok/s (total 14,745,600 tok), 27.60% MFU\n",
      "step 900 eval: val_loss 5.19\n",
      "step 910: loss 5.23 131.12ms/step 124952tok/s (total 14,909,440 tok), 27.90% MFU\n",
      "step 920: loss 5.27 131.78ms/step 124330tok/s (total 15,073,280 tok), 27.76% MFU\n",
      "step 930: loss 5.17 129.83ms/step 126196tok/s (total 15,237,120 tok), 28.17% MFU\n",
      "step 940: loss 5.24 130.77ms/step 125293tok/s (total 15,400,960 tok), 27.97% MFU\n",
      "step 950: loss 5.30 128.37ms/step 127627tok/s (total 15,564,800 tok), 28.49% MFU\n",
      "step 950 eval: val_loss 5.14\n",
      "step 960: loss 5.24 131.06ms/step 125011tok/s (total 15,728,640 tok), 27.91% MFU\n",
      "step 970: loss 5.19 131.17ms/step 124908tok/s (total 15,892,480 tok), 27.89% MFU\n",
      "step 980: loss 5.18 130.33ms/step 125712tok/s (total 16,056,320 tok), 28.07% MFU\n",
      "step 990: loss 5.18 130.12ms/step 125912tok/s (total 16,220,160 tok), 28.11% MFU\n",
      "step 1000: loss 5.25 133.55ms/step 122677tok/s (total 16,384,000 tok), 27.39% MFU\n",
      "step 1000 eval: val_loss 5.11\n",
      "step 1010: loss 5.24 132.56ms/step 123593tok/s (total 16,547,840 tok), 27.59% MFU\n",
      "step 1020: loss 5.18 134.91ms/step 121447tok/s (total 16,711,680 tok), 27.11% MFU\n",
      "step 1030: loss 5.22 134.02ms/step 122250tok/s (total 16,875,520 tok), 27.29% MFU\n",
      "step 1040: loss 5.13 130.18ms/step 125854tok/s (total 17,039,360 tok), 28.10% MFU\n",
      "step 1050: loss 5.05 134.90ms/step 121454tok/s (total 17,203,200 tok), 27.11% MFU\n",
      "step 1050 eval: val_loss 5.12\n",
      "step 1060: loss 5.11 131.88ms/step 124236tok/s (total 17,367,040 tok), 27.74% MFU\n",
      "step 1070: loss 5.17 132.06ms/step 124068tok/s (total 17,530,880 tok), 27.70% MFU\n",
      "step 1080: loss 5.19 129.30ms/step 126710tok/s (total 17,694,720 tok), 28.29% MFU\n",
      "step 1090: loss 5.13 129.37ms/step 126644tok/s (total 17,858,560 tok), 28.27% MFU\n",
      "step 1100: loss 5.15 132.12ms/step 124013tok/s (total 18,022,400 tok), 27.69% MFU\n",
      "step 1100 eval: val_loss 5.03\n",
      "step 1110: loss 5.11 135.83ms/step 120618tok/s (total 18,186,240 tok), 26.93% MFU\n",
      "step 1120: loss 5.16 129.40ms/step 126617tok/s (total 18,350,080 tok), 28.27% MFU\n",
      "step 1130: loss 5.16 133.21ms/step 122998tok/s (total 18,513,920 tok), 27.46% MFU\n",
      "step 1140: loss 5.09 129.90ms/step 126132tok/s (total 18,677,760 tok), 28.16% MFU\n",
      "step 1150: loss 5.08 129.16ms/step 126849tok/s (total 18,841,600 tok), 28.32% MFU\n",
      "step 1150 eval: val_loss 5.06\n",
      "step 1160: loss 5.19 132.82ms/step 123355tok/s (total 19,005,440 tok), 27.54% MFU\n",
      "step 1170: loss 5.17 132.39ms/step 123760tok/s (total 19,169,280 tok), 27.63% MFU\n",
      "step 1180: loss 5.16 131.20ms/step 124883tok/s (total 19,333,120 tok), 27.88% MFU\n",
      "step 1190: loss 5.14 132.24ms/step 123899tok/s (total 19,496,960 tok), 27.66% MFU\n",
      "step 1200: loss 5.21 133.55ms/step 122684tok/s (total 19,660,800 tok), 27.39% MFU\n",
      "step 1200 eval: val_loss 5.10\n",
      "step 1210: loss 5.19 137.07ms/step 119534tok/s (total 19,824,640 tok), 26.69% MFU\n",
      "step 1220: loss 5.09 132.54ms/step 123616tok/s (total 19,988,480 tok), 27.60% MFU\n",
      "step 1230: loss 5.20 129.75ms/step 126269tok/s (total 20,152,320 tok), 28.19% MFU\n",
      "step 1240: loss 5.09 129.14ms/step 126872tok/s (total 20,316,160 tok), 28.32% MFU\n",
      "step 1250: loss 5.12 134.33ms/step 121970tok/s (total 20,480,000 tok), 27.23% MFU\n",
      "step 1250 eval: val_loss 5.07\n",
      "step 1260: loss 5.20 138.02ms/step 118705tok/s (total 20,643,840 tok), 26.50% MFU\n",
      "step 1270: loss 5.15 129.51ms/step 126506tok/s (total 20,807,680 tok), 28.24% MFU\n",
      "step 1280: loss 5.15 131.28ms/step 124804tok/s (total 20,971,520 tok), 27.86% MFU\n",
      "step 1290: loss 5.10 138.39ms/step 118389tok/s (total 21,135,360 tok), 26.43% MFU\n",
      "step 1300: loss 5.09 131.24ms/step 124841tok/s (total 21,299,200 tok), 27.87% MFU\n",
      "step 1300 eval: val_loss 5.10\n",
      "step 1310: loss 5.14 131.98ms/step 124141tok/s (total 21,463,040 tok), 27.71% MFU\n",
      "step 1320: loss 5.06 131.53ms/step 124561tok/s (total 21,626,880 tok), 27.81% MFU\n",
      "step 1330: loss 5.10 133.74ms/step 122507tok/s (total 21,790,720 tok), 27.35% MFU\n",
      "step 1340: loss 5.14 129.75ms/step 126277tok/s (total 21,954,560 tok), 28.19% MFU\n",
      "step 1350: loss 5.13 128.89ms/step 127117tok/s (total 22,118,400 tok), 28.38% MFU\n",
      "step 1350 eval: val_loss 5.02\n",
      "step 1360: loss 5.11 134.23ms/step 122055tok/s (total 22,282,240 tok), 27.25% MFU\n",
      "step 1370: loss 5.16 130.56ms/step 125493tok/s (total 22,446,080 tok), 28.02% MFU\n",
      "step 1380: loss 5.12 128.35ms/step 127650tok/s (total 22,609,920 tok), 28.50% MFU\n",
      "step 1390: loss 5.00 134.25ms/step 122045tok/s (total 22,773,760 tok), 27.25% MFU\n",
      "step 1400: loss 5.05 132.57ms/step 123586tok/s (total 22,937,600 tok), 27.59% MFU\n",
      "step 1400 eval: val_loss 5.04\n",
      "step 1410: loss 5.11 129.17ms/step 126837tok/s (total 23,101,440 tok), 28.32% MFU\n",
      "step 1420: loss 5.10 129.75ms/step 126277tok/s (total 23,265,280 tok), 28.19% MFU\n",
      "step 1430: loss 5.07 128.85ms/step 127151tok/s (total 23,429,120 tok), 28.39% MFU\n",
      "step 1440: loss 5.03 131.43ms/step 124664tok/s (total 23,592,960 tok), 27.83% MFU\n",
      "step 1450: loss 5.01 129.61ms/step 126415tok/s (total 23,756,800 tok), 28.22% MFU\n",
      "step 1450 eval: val_loss 5.07\n",
      "step 1460: loss 4.99 131.58ms/step 124519tok/s (total 23,920,640 tok), 27.80% MFU\n",
      "step 1470: loss 5.01 131.42ms/step 124664tok/s (total 24,084,480 tok), 27.83% MFU\n",
      "step 1480: loss 5.04 130.05ms/step 125982tok/s (total 24,248,320 tok), 28.13% MFU\n",
      "step 1490: loss 5.08 130.24ms/step 125795tok/s (total 24,412,160 tok), 28.08% MFU\n",
      "step 1500: loss 5.06 133.71ms/step 122534tok/s (total 24,576,000 tok), 27.36% MFU\n",
      "step 1500 eval: val_loss 5.03\n",
      "step 1510: loss 5.08 132.85ms/step 123328tok/s (total 24,739,840 tok), 27.53% MFU\n",
      "step 1520: loss 5.12 132.12ms/step 124008tok/s (total 24,903,680 tok), 27.69% MFU\n",
      "step 1530: loss 5.06 132.08ms/step 124042tok/s (total 25,067,520 tok), 27.69% MFU\n",
      "step 1540: loss 5.02 133.85ms/step 122405tok/s (total 25,231,360 tok), 27.33% MFU\n",
      "step 1550: loss 5.05 134.77ms/step 121573tok/s (total 25,395,200 tok), 27.14% MFU\n",
      "step 1550 eval: val_loss 5.06\n",
      "step 1560: loss 5.03 132.81ms/step 123362tok/s (total 25,559,040 tok), 27.54% MFU\n",
      "step 1570: loss 5.08 133.25ms/step 122953tok/s (total 25,722,880 tok), 27.45% MFU\n",
      "step 1580: loss 5.05 132.26ms/step 123876tok/s (total 25,886,720 tok), 27.66% MFU\n",
      "step 1590: loss 5.04 131.58ms/step 124520tok/s (total 26,050,560 tok), 27.80% MFU\n",
      "step 1600: loss 5.14 129.23ms/step 126786tok/s (total 26,214,400 tok), 28.31% MFU\n",
      "step 1600 eval: val_loss 5.03\n",
      "step 1610: loss 4.99 131.05ms/step 125019tok/s (total 26,378,240 tok), 27.91% MFU\n",
      "step 1620: loss 5.01 132.57ms/step 123592tok/s (total 26,542,080 tok), 27.59% MFU\n",
      "step 1630: loss 5.04 130.82ms/step 125239tok/s (total 26,705,920 tok), 27.96% MFU\n",
      "step 1640: loss 5.07 129.25ms/step 126763tok/s (total 26,869,760 tok), 28.30% MFU\n",
      "step 1650: loss 4.98 133.55ms/step 122682tok/s (total 27,033,600 tok), 27.39% MFU\n",
      "step 1650 eval: val_loss 5.01\n",
      "step 1660: loss 5.01 132.50ms/step 123652tok/s (total 27,197,440 tok), 27.61% MFU\n",
      "step 1670: loss 5.01 131.77ms/step 124336tok/s (total 27,361,280 tok), 27.76% MFU\n",
      "step 1680: loss 5.12 131.70ms/step 124404tok/s (total 27,525,120 tok), 27.77% MFU\n",
      "step 1690: loss 5.03 129.61ms/step 126410tok/s (total 27,688,960 tok), 28.22% MFU\n",
      "step 1700: loss 5.01 131.34ms/step 124744tok/s (total 27,852,800 tok), 27.85% MFU\n",
      "step 1700 eval: val_loss 5.03\n",
      "step 1710: loss 4.95 134.58ms/step 121740tok/s (total 28,016,640 tok), 27.18% MFU\n",
      "step 1720: loss 4.97 133.04ms/step 123153tok/s (total 28,180,480 tok), 27.49% MFU\n",
      "step 1730: loss 5.06 131.22ms/step 124856tok/s (total 28,344,320 tok), 27.87% MFU\n",
      "step 1740: loss 4.98 136.19ms/step 120301tok/s (total 28,508,160 tok), 26.86% MFU\n",
      "step 1750: loss 5.05 131.86ms/step 124249tok/s (total 28,672,000 tok), 27.74% MFU\n",
      "step 1750 eval: val_loss 5.01\n",
      "step 1760: loss 4.98 132.15ms/step 123977tok/s (total 28,835,840 tok), 27.68% MFU\n",
      "step 1770: loss 5.06 136.24ms/step 120256tok/s (total 28,999,680 tok), 26.85% MFU\n",
      "step 1780: loss 5.01 129.76ms/step 126268tok/s (total 29,163,520 tok), 28.19% MFU\n",
      "step 1790: loss 4.99 131.05ms/step 125018tok/s (total 29,327,360 tok), 27.91% MFU\n",
      "step 1800: loss 5.06 130.40ms/step 125647tok/s (total 29,491,200 tok), 28.05% MFU\n",
      "step 1800 eval: val_loss 4.98\n",
      "step 1810: loss 4.96 134.53ms/step 121783tok/s (total 29,655,040 tok), 27.19% MFU\n",
      "step 1820: loss 4.98 132.67ms/step 123498tok/s (total 29,818,880 tok), 27.57% MFU\n",
      "step 1830: loss 5.02 131.58ms/step 124522tok/s (total 29,982,720 tok), 27.80% MFU\n",
      "step 1840: loss 4.95 131.02ms/step 125050tok/s (total 30,146,560 tok), 27.92% MFU\n",
      "step 1850: loss 5.01 129.55ms/step 126466tok/s (total 30,310,400 tok), 28.23% MFU\n",
      "step 1850 eval: val_loss 4.93\n",
      "step 1860: loss 5.01 129.22ms/step 126793tok/s (total 30,474,240 tok), 28.31% MFU\n",
      "step 1870: loss 4.98 133.84ms/step 122411tok/s (total 30,638,080 tok), 27.33% MFU\n",
      "step 1880: loss 4.96 131.41ms/step 124675tok/s (total 30,801,920 tok), 27.83% MFU\n",
      "step 1890: loss 4.97 132.74ms/step 123432tok/s (total 30,965,760 tok), 27.56% MFU\n",
      "step 1900: loss 5.01 131.13ms/step 124942tok/s (total 31,129,600 tok), 27.89% MFU\n",
      "step 1900 eval: val_loss 4.90\n",
      "step 1910: loss 5.04 135.06ms/step 121311tok/s (total 31,293,440 tok), 27.08% MFU\n",
      "step 1920: loss 4.95 134.24ms/step 122048tok/s (total 31,457,280 tok), 27.25% MFU\n",
      "step 1930: loss 5.03 131.66ms/step 124440tok/s (total 31,621,120 tok), 27.78% MFU\n",
      "step 1940: loss 5.05 135.84ms/step 120610tok/s (total 31,784,960 tok), 26.93% MFU\n",
      "step 1950: loss 4.97 135.90ms/step 120559tok/s (total 31,948,800 tok), 26.92% MFU\n",
      "step 1950 eval: val_loss 4.92\n",
      "step 1960: loss 5.03 129.00ms/step 127005tok/s (total 32,112,640 tok), 28.35% MFU\n",
      "step 1970: loss 4.99 130.95ms/step 125115tok/s (total 32,276,480 tok), 27.93% MFU\n",
      "step 1980: loss 4.97 132.23ms/step 123905tok/s (total 32,440,320 tok), 27.66% MFU\n",
      "step 1990: loss 4.92 129.12ms/step 126894tok/s (total 32,604,160 tok), 28.33% MFU\n",
      "step 2000: loss 5.01 129.63ms/step 126390tok/s (total 32,768,000 tok), 28.22% MFU\n",
      "step 2000 eval: val_loss 4.89\n",
      "step 2010: loss 4.97 133.77ms/step 122475tok/s (total 32,931,840 tok), 27.34% MFU\n",
      "step 2020: loss 5.03 132.34ms/step 123799tok/s (total 33,095,680 tok), 27.64% MFU\n",
      "step 2030: loss 4.99 133.30ms/step 122911tok/s (total 33,259,520 tok), 27.44% MFU\n",
      "step 2040: loss 5.03 135.99ms/step 120478tok/s (total 33,423,360 tok), 26.90% MFU\n",
      "step 2050: loss 4.93 134.22ms/step 122070tok/s (total 33,587,200 tok), 27.25% MFU\n",
      "step 2050 eval: val_loss 4.90\n",
      "step 2060: loss 5.03 133.29ms/step 122917tok/s (total 33,751,040 tok), 27.44% MFU\n",
      "step 2070: loss 4.98 132.25ms/step 123882tok/s (total 33,914,880 tok), 27.66% MFU\n",
      "step 2080: loss 4.91 132.18ms/step 123952tok/s (total 34,078,720 tok), 27.67% MFU\n",
      "step 2090: loss 4.95 128.76ms/step 127247tok/s (total 34,242,560 tok), 28.41% MFU\n",
      "step 2100: loss 4.90 132.84ms/step 123338tok/s (total 34,406,400 tok), 27.54% MFU\n",
      "step 2100 eval: val_loss 4.94\n",
      "step 2110: loss 4.93 134.39ms/step 121910tok/s (total 34,570,240 tok), 27.22% MFU\n",
      "step 2120: loss 4.92 130.53ms/step 125524tok/s (total 34,734,080 tok), 28.02% MFU\n",
      "step 2130: loss 4.90 133.39ms/step 122829tok/s (total 34,897,920 tok), 27.42% MFU\n",
      "step 2140: loss 4.94 132.55ms/step 123604tok/s (total 35,061,760 tok), 27.59% MFU\n",
      "step 2150: loss 4.92 134.94ms/step 121416tok/s (total 35,225,600 tok), 27.11% MFU\n",
      "step 2150 eval: val_loss 4.93\n",
      "step 2160: loss 4.91 134.85ms/step 121499tok/s (total 35,389,440 tok), 27.12% MFU\n",
      "step 2170: loss 4.90 132.18ms/step 123954tok/s (total 35,553,280 tok), 27.67% MFU\n",
      "step 2180: loss 4.86 132.17ms/step 123963tok/s (total 35,717,120 tok), 27.67% MFU\n",
      "step 2190: loss 4.92 130.50ms/step 125551tok/s (total 35,880,960 tok), 28.03% MFU\n",
      "step 2200: loss 4.93 128.23ms/step 127770tok/s (total 36,044,800 tok), 28.52% MFU\n",
      "step 2200 eval: val_loss 4.89\n",
      "step 2210: loss 4.90 129.50ms/step 126515tok/s (total 36,208,640 tok), 28.24% MFU\n",
      "step 2220: loss 4.87 134.10ms/step 122181tok/s (total 36,372,480 tok), 27.28% MFU\n",
      "step 2230: loss 4.87 135.71ms/step 120728tok/s (total 36,536,320 tok), 26.95% MFU\n",
      "step 2240: loss 4.96 128.52ms/step 127482tok/s (total 36,700,160 tok), 28.46% MFU\n",
      "step 2250: loss 4.93 132.51ms/step 123646tok/s (total 36,864,000 tok), 27.60% MFU\n",
      "step 2250 eval: val_loss 4.84\n",
      "step 2260: loss 4.97 132.48ms/step 123673tok/s (total 37,027,840 tok), 27.61% MFU\n",
      "step 2270: loss 4.98 129.78ms/step 126243tok/s (total 37,191,680 tok), 28.18% MFU\n",
      "step 2280: loss 4.86 136.10ms/step 120382tok/s (total 37,355,520 tok), 26.88% MFU\n",
      "step 2290: loss 4.93 130.26ms/step 125781tok/s (total 37,519,360 tok), 28.08% MFU\n",
      "step 2300: loss 4.84 130.67ms/step 125384tok/s (total 37,683,200 tok), 27.99% MFU\n",
      "step 2300 eval: val_loss 4.89\n",
      "step 2310: loss 4.92 134.64ms/step 121690tok/s (total 37,847,040 tok), 27.17% MFU\n",
      "step 2320: loss 4.95 134.08ms/step 122196tok/s (total 38,010,880 tok), 27.28% MFU\n",
      "step 2330: loss 4.94 132.80ms/step 123374tok/s (total 38,174,720 tok), 27.54% MFU\n",
      "step 2340: loss 4.93 131.53ms/step 124565tok/s (total 38,338,560 tok), 27.81% MFU\n",
      "step 2350: loss 4.82 132.69ms/step 123480tok/s (total 38,502,400 tok), 27.57% MFU\n",
      "step 2350 eval: val_loss 4.88\n",
      "step 2360: loss 4.88 132.61ms/step 123553tok/s (total 38,666,240 tok), 27.58% MFU\n",
      "step 2370: loss 4.92 132.14ms/step 123991tok/s (total 38,830,080 tok), 27.68% MFU\n",
      "step 2380: loss 4.90 132.96ms/step 123229tok/s (total 38,993,920 tok), 27.51% MFU\n",
      "step 2390: loss 4.78 129.73ms/step 126296tok/s (total 39,157,760 tok), 28.20% MFU\n",
      "step 2400: loss 4.96 132.07ms/step 124057tok/s (total 39,321,600 tok), 27.70% MFU\n",
      "step 2400 eval: val_loss 4.85\n",
      "step 2410: loss 4.97 132.61ms/step 123546tok/s (total 39,485,440 tok), 27.58% MFU\n",
      "step 2420: loss 4.90 131.50ms/step 124598tok/s (total 39,649,280 tok), 27.82% MFU\n",
      "step 2430: loss 4.84 129.96ms/step 126074tok/s (total 39,813,120 tok), 28.15% MFU\n",
      "step 2440: loss 4.88 132.47ms/step 123684tok/s (total 39,976,960 tok), 27.61% MFU\n",
      "step 2450: loss 4.92 129.73ms/step 126292tok/s (total 40,140,800 tok), 28.20% MFU\n",
      "step 2450 eval: val_loss 4.88\n",
      "step 2460: loss 4.80 135.05ms/step 121320tok/s (total 40,304,640 tok), 27.08% MFU\n",
      "step 2470: loss 4.84 132.11ms/step 124017tok/s (total 40,468,480 tok), 27.69% MFU\n",
      "step 2480: loss 4.96 131.39ms/step 124701tok/s (total 40,632,320 tok), 27.84% MFU\n",
      "step 2490: loss 4.88 132.79ms/step 123382tok/s (total 40,796,160 tok), 27.55% MFU\n",
      "step 2500: loss 4.88 129.61ms/step 126413tok/s (total 40,960,000 tok), 28.22% MFU\n",
      "step 2500 eval: val_loss 4.92\n",
      "step 2510: loss 4.77 130.54ms/step 125505tok/s (total 41,123,840 tok), 28.02% MFU\n",
      "step 2520: loss 4.87 129.59ms/step 126432tok/s (total 41,287,680 tok), 28.23% MFU\n",
      "step 2530: loss 4.85 129.67ms/step 126352tok/s (total 41,451,520 tok), 28.21% MFU\n",
      "step 2540: loss 4.91 130.95ms/step 125121tok/s (total 41,615,360 tok), 27.93% MFU\n",
      "step 2550: loss 4.87 130.65ms/step 125407tok/s (total 41,779,200 tok), 28.00% MFU\n",
      "step 2550 eval: val_loss 4.88\n",
      "step 2560: loss 4.93 131.77ms/step 124338tok/s (total 41,943,040 tok), 27.76% MFU\n",
      "step 2570: loss 4.84 130.84ms/step 125219tok/s (total 42,106,880 tok), 27.96% MFU\n",
      "step 2580: loss 4.84 130.78ms/step 125275tok/s (total 42,270,720 tok), 27.97% MFU\n",
      "step 2590: loss 4.99 136.61ms/step 119930tok/s (total 42,434,560 tok), 26.77% MFU\n",
      "step 2600: loss 4.88 130.09ms/step 125940tok/s (total 42,598,400 tok), 28.12% MFU\n",
      "step 2600 eval: val_loss 4.79\n",
      "step 2610: loss 4.86 131.23ms/step 124848tok/s (total 42,762,240 tok), 27.87% MFU\n",
      "step 2620: loss 4.79 130.78ms/step 125276tok/s (total 42,926,080 tok), 27.97% MFU\n",
      "step 2630: loss 4.90 132.18ms/step 123951tok/s (total 43,089,920 tok), 27.67% MFU\n",
      "step 2640: loss 4.90 131.78ms/step 124325tok/s (total 43,253,760 tok), 27.76% MFU\n",
      "step 2650: loss 4.85 131.39ms/step 124695tok/s (total 43,417,600 tok), 27.84% MFU\n",
      "step 2650 eval: val_loss 4.78\n",
      "step 2660: loss 4.80 140.47ms/step 116634tok/s (total 43,581,440 tok), 26.04% MFU\n",
      "step 2670: loss 4.85 130.74ms/step 125315tok/s (total 43,745,280 tok), 27.98% MFU\n",
      "step 2680: loss 4.82 129.92ms/step 126105tok/s (total 43,909,120 tok), 28.15% MFU\n",
      "step 2690: loss 4.85 128.05ms/step 127946tok/s (total 44,072,960 tok), 28.56% MFU\n",
      "step 2700: loss 4.79 132.21ms/step 123921tok/s (total 44,236,800 tok), 27.67% MFU\n",
      "step 2700 eval: val_loss 4.85\n",
      "step 2710: loss 4.83 128.83ms/step 127172tok/s (total 44,400,640 tok), 28.39% MFU\n",
      "step 2720: loss 4.82 131.18ms/step 124895tok/s (total 44,564,480 tok), 27.88% MFU\n",
      "step 2730: loss 4.84 129.13ms/step 126884tok/s (total 44,728,320 tok), 28.33% MFU\n",
      "step 2740: loss 4.76 131.29ms/step 124793tok/s (total 44,892,160 tok), 27.86% MFU\n",
      "step 2750: loss 4.85 132.63ms/step 123536tok/s (total 45,056,000 tok), 27.58% MFU\n",
      "step 2750 eval: val_loss 4.87\n",
      "step 2760: loss 4.82 130.16ms/step 125876tok/s (total 45,219,840 tok), 28.10% MFU\n",
      "step 2770: loss 4.86 130.90ms/step 125160tok/s (total 45,383,680 tok), 27.94% MFU\n",
      "step 2780: loss 4.86 131.94ms/step 124176tok/s (total 45,547,520 tok), 27.72% MFU\n",
      "step 2790: loss 4.79 131.08ms/step 124997tok/s (total 45,711,360 tok), 27.91% MFU\n",
      "step 2800: loss 4.83 129.92ms/step 126112tok/s (total 45,875,200 tok), 28.15% MFU\n",
      "step 2800 eval: val_loss 4.80\n",
      "step 2810: loss 4.84 133.49ms/step 122735tok/s (total 46,039,040 tok), 27.40% MFU\n",
      "step 2820: loss 4.92 131.80ms/step 124306tok/s (total 46,202,880 tok), 27.75% MFU\n",
      "step 2830: loss 4.82 131.50ms/step 124590tok/s (total 46,366,720 tok), 27.82% MFU\n",
      "step 2840: loss 4.88 132.32ms/step 123823tok/s (total 46,530,560 tok), 27.64% MFU\n",
      "step 2850: loss 4.85 131.35ms/step 124733tok/s (total 46,694,400 tok), 27.85% MFU\n",
      "step 2850 eval: val_loss 4.86\n",
      "step 2860: loss 4.82 129.67ms/step 126351tok/s (total 46,858,240 tok), 28.21% MFU\n",
      "step 2870: loss 4.81 136.26ms/step 120239tok/s (total 47,022,080 tok), 26.84% MFU\n",
      "step 2880: loss 4.95 133.60ms/step 122635tok/s (total 47,185,920 tok), 27.38% MFU\n",
      "step 2890: loss 4.85 130.98ms/step 125087tok/s (total 47,349,760 tok), 27.93% MFU\n",
      "step 2900: loss 4.80 132.78ms/step 123389tok/s (total 47,513,600 tok), 27.55% MFU\n",
      "step 2900 eval: val_loss 4.75\n",
      "step 2910: loss 4.90 129.30ms/step 126714tok/s (total 47,677,440 tok), 28.29% MFU\n",
      "step 2920: loss 4.81 138.26ms/step 118500tok/s (total 47,841,280 tok), 26.46% MFU\n",
      "step 2930: loss 4.87 131.60ms/step 124501tok/s (total 48,005,120 tok), 27.80% MFU\n",
      "step 2940: loss 4.84 128.24ms/step 127760tok/s (total 48,168,960 tok), 28.52% MFU\n",
      "step 2950: loss 4.82 131.32ms/step 124764tok/s (total 48,332,800 tok), 27.85% MFU\n",
      "step 2950 eval: val_loss 4.81\n",
      "step 2960: loss 4.77 130.90ms/step 125160tok/s (total 48,496,640 tok), 27.94% MFU\n",
      "step 2970: loss 4.77 132.67ms/step 123496tok/s (total 48,660,480 tok), 27.57% MFU\n",
      "step 2980: loss 4.94 133.53ms/step 122695tok/s (total 48,824,320 tok), 27.39% MFU\n",
      "step 2990: loss 4.87 130.14ms/step 125893tok/s (total 48,988,160 tok), 28.11% MFU\n",
      "step 3000: loss 4.85 128.29ms/step 127715tok/s (total 49,152,000 tok), 28.51% MFU\n",
      "step 3000 eval: val_loss 4.83\n",
      "step 3010: loss 4.85 131.04ms/step 125031tok/s (total 49,315,840 tok), 27.91% MFU\n",
      "step 3020: loss 4.78 138.02ms/step 118704tok/s (total 49,479,680 tok), 26.50% MFU\n",
      "step 3030: loss 4.82 129.61ms/step 126414tok/s (total 49,643,520 tok), 28.22% MFU\n",
      "step 3040: loss 4.82 130.59ms/step 125465tok/s (total 49,807,360 tok), 28.01% MFU\n",
      "step 3050: loss 4.77 132.01ms/step 124108tok/s (total 49,971,200 tok), 27.71% MFU\n",
      "step 3050 eval: val_loss 4.78\n",
      "step 3060: loss 4.84 130.51ms/step 125543tok/s (total 50,135,040 tok), 28.03% MFU\n",
      "step 3070: loss 4.78 133.62ms/step 122614tok/s (total 50,298,880 tok), 27.37% MFU\n",
      "step 3080: loss 4.76 133.50ms/step 122723tok/s (total 50,462,720 tok), 27.40% MFU\n",
      "step 3090: loss 4.74 130.97ms/step 125095tok/s (total 50,626,560 tok), 27.93% MFU\n",
      "step 3100: loss 4.91 129.50ms/step 126518tok/s (total 50,790,400 tok), 28.25% MFU\n",
      "step 3100 eval: val_loss 4.91\n",
      "step 3110: loss 4.77 130.05ms/step 125984tok/s (total 50,954,240 tok), 28.13% MFU\n",
      "step 3120: loss 4.77 133.76ms/step 122488tok/s (total 51,118,080 tok), 27.35% MFU\n",
      "step 3130: loss 4.83 132.33ms/step 123810tok/s (total 51,281,920 tok), 27.64% MFU\n",
      "step 3140: loss 4.91 129.68ms/step 126339tok/s (total 51,445,760 tok), 28.21% MFU\n",
      "step 3150: loss 4.71 132.02ms/step 124104tok/s (total 51,609,600 tok), 27.71% MFU\n",
      "step 3150 eval: val_loss 4.82\n",
      "step 3160: loss 4.81 132.89ms/step 123291tok/s (total 51,773,440 tok), 27.52% MFU\n",
      "step 3170: loss 4.76 135.41ms/step 120999tok/s (total 51,937,280 tok), 27.01% MFU\n",
      "step 3180: loss 4.75 134.87ms/step 121476tok/s (total 52,101,120 tok), 27.12% MFU\n",
      "step 3190: loss 4.86 131.21ms/step 124865tok/s (total 52,264,960 tok), 27.88% MFU\n",
      "step 3200: loss 4.87 132.85ms/step 123328tok/s (total 52,428,800 tok), 27.53% MFU\n",
      "step 3200 eval: val_loss 4.79\n",
      "step 3210: loss 4.85 132.51ms/step 123644tok/s (total 52,592,640 tok), 27.60% MFU\n",
      "step 3220: loss 4.76 132.21ms/step 123928tok/s (total 52,756,480 tok), 27.67% MFU\n",
      "step 3230: loss 4.83 128.33ms/step 127675tok/s (total 52,920,320 tok), 28.50% MFU\n",
      "step 3240: loss 4.86 132.06ms/step 124067tok/s (total 53,084,160 tok), 27.70% MFU\n",
      "step 3250: loss 4.82 133.25ms/step 122957tok/s (total 53,248,000 tok), 27.45% MFU\n",
      "step 3250 eval: val_loss 4.69\n",
      "step 3260: loss 4.90 130.62ms/step 125432tok/s (total 53,411,840 tok), 28.00% MFU\n",
      "step 3270: loss 4.78 130.21ms/step 125824tok/s (total 53,575,680 tok), 28.09% MFU\n",
      "step 3280: loss 4.77 129.27ms/step 126738tok/s (total 53,739,520 tok), 28.29% MFU\n",
      "step 3290: loss 4.84 132.52ms/step 123634tok/s (total 53,903,360 tok), 27.60% MFU\n",
      "step 3300: loss 4.78 131.03ms/step 125041tok/s (total 54,067,200 tok), 27.92% MFU\n",
      "step 3300 eval: val_loss 4.82\n",
      "step 3310: loss 4.80 129.82ms/step 126208tok/s (total 54,231,040 tok), 28.18% MFU\n",
      "step 3320: loss 4.78 134.72ms/step 121612tok/s (total 54,394,880 tok), 27.15% MFU\n",
      "step 3330: loss 4.79 131.73ms/step 124378tok/s (total 54,558,720 tok), 27.77% MFU\n",
      "step 3340: loss 4.83 131.30ms/step 124786tok/s (total 54,722,560 tok), 27.86% MFU\n",
      "step 3350: loss 4.78 131.72ms/step 124383tok/s (total 54,886,400 tok), 27.77% MFU\n",
      "step 3350 eval: val_loss 4.72\n",
      "step 3360: loss 4.87 130.24ms/step 125803tok/s (total 55,050,240 tok), 28.09% MFU\n",
      "step 3370: loss 4.78 131.67ms/step 124432tok/s (total 55,214,080 tok), 27.78% MFU\n",
      "step 3380: loss 4.81 129.37ms/step 126646tok/s (total 55,377,920 tok), 28.27% MFU\n",
      "step 3390: loss 4.72 132.01ms/step 124116tok/s (total 55,541,760 tok), 27.71% MFU\n",
      "step 3400: loss 4.83 128.93ms/step 127074tok/s (total 55,705,600 tok), 28.37% MFU\n",
      "step 3400 eval: val_loss 4.72\n",
      "step 3410: loss 4.80 134.40ms/step 121901tok/s (total 55,869,440 tok), 27.21% MFU\n",
      "step 3420: loss 4.76 130.75ms/step 125306tok/s (total 56,033,280 tok), 27.97% MFU\n",
      "step 3430: loss 4.74 134.25ms/step 122045tok/s (total 56,197,120 tok), 27.25% MFU\n",
      "step 3440: loss 4.67 128.67ms/step 127331tok/s (total 56,360,960 tok), 28.43% MFU\n",
      "step 3450: loss 4.82 129.12ms/step 126892tok/s (total 56,524,800 tok), 28.33% MFU\n",
      "step 3450 eval: val_loss 4.84\n",
      "step 3460: loss 4.75 130.99ms/step 125083tok/s (total 56,688,640 tok), 27.92% MFU\n",
      "step 3470: loss 4.72 128.83ms/step 127174tok/s (total 56,852,480 tok), 28.39% MFU\n",
      "step 3480: loss 4.74 129.04ms/step 126969tok/s (total 57,016,320 tok), 28.35% MFU\n",
      "step 3490: loss 4.75 129.86ms/step 126163tok/s (total 57,180,160 tok), 28.17% MFU\n",
      "step 3500: loss 4.77 132.60ms/step 123559tok/s (total 57,344,000 tok), 27.58% MFU\n",
      "step 3500 eval: val_loss 4.70\n",
      "step 3510: loss 4.78 129.91ms/step 126114tok/s (total 57,507,840 tok), 28.16% MFU\n",
      "step 3520: loss 4.74 131.28ms/step 124798tok/s (total 57,671,680 tok), 27.86% MFU\n",
      "step 3530: loss 4.85 130.71ms/step 125348tok/s (total 57,835,520 tok), 27.98% MFU\n",
      "step 3540: loss 4.67 130.32ms/step 125721tok/s (total 57,999,360 tok), 28.07% MFU\n",
      "step 3550: loss 4.80 129.02ms/step 126990tok/s (total 58,163,200 tok), 28.35% MFU\n",
      "step 3550 eval: val_loss 4.71\n",
      "step 3560: loss 4.76 132.92ms/step 123259tok/s (total 58,327,040 tok), 27.52% MFU\n",
      "step 3570: loss 4.66 132.70ms/step 123466tok/s (total 58,490,880 tok), 27.56% MFU\n",
      "step 3580: loss 4.74 131.50ms/step 124589tok/s (total 58,654,720 tok), 27.81% MFU\n",
      "step 3590: loss 4.68 129.01ms/step 127002tok/s (total 58,818,560 tok), 28.35% MFU\n",
      "step 3600: loss 4.75 132.70ms/step 123469tok/s (total 58,982,400 tok), 27.56% MFU\n",
      "step 3600 eval: val_loss 4.73\n",
      "step 3610: loss 4.78 130.73ms/step 125323tok/s (total 59,146,240 tok), 27.98% MFU\n",
      "step 3620: loss 4.69 131.99ms/step 124132tok/s (total 59,310,080 tok), 27.71% MFU\n",
      "step 3630: loss 4.73 134.13ms/step 122154tok/s (total 59,473,920 tok), 27.27% MFU\n",
      "step 3640: loss 4.77 132.13ms/step 123995tok/s (total 59,637,760 tok), 27.68% MFU\n",
      "step 3650: loss 4.78 131.18ms/step 124893tok/s (total 59,801,600 tok), 27.88% MFU\n",
      "step 3650 eval: val_loss 4.71\n",
      "step 3660: loss 4.66 132.98ms/step 123203tok/s (total 59,965,440 tok), 27.51% MFU\n",
      "step 3670: loss 4.73 132.24ms/step 123896tok/s (total 60,129,280 tok), 27.66% MFU\n",
      "step 3680: loss 4.79 135.05ms/step 121317tok/s (total 60,293,120 tok), 27.08% MFU\n",
      "step 3690: loss 4.74 129.88ms/step 126150tok/s (total 60,456,960 tok), 28.16% MFU\n",
      "step 3700: loss 4.73 131.34ms/step 124749tok/s (total 60,620,800 tok), 27.85% MFU\n",
      "step 3700 eval: val_loss 4.63\n",
      "step 3710: loss 4.79 133.06ms/step 123133tok/s (total 60,784,640 tok), 27.49% MFU\n",
      "step 3720: loss 4.68 129.79ms/step 126232tok/s (total 60,948,480 tok), 28.18% MFU\n",
      "step 3730: loss 4.70 131.61ms/step 124487tok/s (total 61,112,320 tok), 27.79% MFU\n",
      "step 3740: loss 4.70 129.47ms/step 126544tok/s (total 61,276,160 tok), 28.25% MFU\n",
      "step 3750: loss 4.78 131.80ms/step 124310tok/s (total 61,440,000 tok), 27.75% MFU\n",
      "step 3750 eval: val_loss 4.80\n",
      "step 3760: loss 4.77 134.17ms/step 122113tok/s (total 61,603,840 tok), 27.26% MFU\n",
      "step 3770: loss 4.74 129.89ms/step 126134tok/s (total 61,767,680 tok), 28.16% MFU\n",
      "step 3780: loss 4.72 132.55ms/step 123605tok/s (total 61,931,520 tok), 27.60% MFU\n",
      "step 3790: loss 4.77 135.27ms/step 121117tok/s (total 62,095,360 tok), 27.04% MFU\n",
      "step 3800: loss 4.75 132.83ms/step 123347tok/s (total 62,259,200 tok), 27.54% MFU\n",
      "step 3800 eval: val_loss 4.72\n",
      "step 3810: loss 4.81 128.48ms/step 127522tok/s (total 62,423,040 tok), 28.47% MFU\n",
      "step 3820: loss 4.73 132.68ms/step 123488tok/s (total 62,586,880 tok), 27.57% MFU\n",
      "step 3830: loss 4.82 135.18ms/step 121198tok/s (total 62,750,720 tok), 27.06% MFU\n",
      "step 3840: loss 4.68 131.34ms/step 124746tok/s (total 62,914,560 tok), 27.85% MFU\n",
      "step 3850: loss 4.67 131.06ms/step 125014tok/s (total 63,078,400 tok), 27.91% MFU\n",
      "step 3850 eval: val_loss 4.75\n",
      "step 3860: loss 4.69 136.77ms/step 119794tok/s (total 63,242,240 tok), 26.74% MFU\n",
      "step 3870: loss 4.78 130.64ms/step 125417tok/s (total 63,406,080 tok), 28.00% MFU\n",
      "step 3880: loss 4.77 134.23ms/step 122056tok/s (total 63,569,920 tok), 27.25% MFU\n",
      "step 3890: loss 4.73 130.28ms/step 125756tok/s (total 63,733,760 tok), 28.08% MFU\n",
      "step 3900: loss 4.69 132.45ms/step 123698tok/s (total 63,897,600 tok), 27.62% MFU\n",
      "step 3900 eval: val_loss 4.73\n",
      "step 3910: loss 4.78 134.81ms/step 121530tok/s (total 64,061,440 tok), 27.13% MFU\n",
      "step 3920: loss 4.72 130.62ms/step 125436tok/s (total 64,225,280 tok), 28.00% MFU\n",
      "step 3930: loss 4.75 133.12ms/step 123072tok/s (total 64,389,120 tok), 27.48% MFU\n",
      "step 3940: loss 4.76 130.87ms/step 125189tok/s (total 64,552,960 tok), 27.95% MFU\n",
      "step 3950: loss 4.73 131.12ms/step 124952tok/s (total 64,716,800 tok), 27.90% MFU\n",
      "step 3950 eval: val_loss 4.77\n",
      "step 3960: loss 4.71 132.44ms/step 123710tok/s (total 64,880,640 tok), 27.62% MFU\n",
      "step 3970: loss 4.60 132.52ms/step 123634tok/s (total 65,044,480 tok), 27.60% MFU\n",
      "step 3980: loss 4.82 132.29ms/step 123849tok/s (total 65,208,320 tok), 27.65% MFU\n",
      "step 3990: loss 4.71 130.48ms/step 125569tok/s (total 65,372,160 tok), 28.03% MFU\n",
      "step 4000: loss 4.69 133.56ms/step 122673tok/s (total 65,536,000 tok), 27.39% MFU\n",
      "step 4000 eval: val_loss 4.72\n",
      "step 4010: loss 4.69 132.02ms/step 124098tok/s (total 65,699,840 tok), 27.71% MFU\n",
      "step 4020: loss 4.76 128.56ms/step 127446tok/s (total 65,863,680 tok), 28.45% MFU\n",
      "step 4030: loss 4.81 131.61ms/step 124492tok/s (total 66,027,520 tok), 27.79% MFU\n",
      "step 4040: loss 4.71 132.20ms/step 123936tok/s (total 66,191,360 tok), 27.67% MFU\n",
      "step 4050: loss 4.62 129.25ms/step 126765tok/s (total 66,355,200 tok), 28.30% MFU\n",
      "step 4050 eval: val_loss 4.66\n",
      "step 4060: loss 4.71 129.63ms/step 126394tok/s (total 66,519,040 tok), 28.22% MFU\n",
      "step 4070: loss 4.76 128.64ms/step 127360tok/s (total 66,682,880 tok), 28.43% MFU\n",
      "step 4080: loss 4.69 130.61ms/step 125445tok/s (total 66,846,720 tok), 28.01% MFU\n",
      "step 4090: loss 4.73 133.36ms/step 122860tok/s (total 67,010,560 tok), 27.43% MFU\n",
      "step 4100: loss 4.72 132.57ms/step 123590tok/s (total 67,174,400 tok), 27.59% MFU\n",
      "step 4100 eval: val_loss 4.74\n",
      "step 4110: loss 4.71 130.74ms/step 125315tok/s (total 67,338,240 tok), 27.98% MFU\n",
      "step 4120: loss 4.65 130.71ms/step 125348tok/s (total 67,502,080 tok), 27.98% MFU\n",
      "step 4130: loss 4.77 129.46ms/step 126561tok/s (total 67,665,920 tok), 28.26% MFU\n",
      "step 4140: loss 4.72 132.79ms/step 123386tok/s (total 67,829,760 tok), 27.55% MFU\n",
      "step 4150: loss 4.75 131.08ms/step 124995tok/s (total 67,993,600 tok), 27.91% MFU\n",
      "step 4150 eval: val_loss 4.67\n",
      "step 4160: loss 4.74 130.22ms/step 125820tok/s (total 68,157,440 tok), 28.09% MFU\n",
      "step 4170: loss 4.77 132.09ms/step 124035tok/s (total 68,321,280 tok), 27.69% MFU\n",
      "step 4180: loss 4.77 134.48ms/step 121833tok/s (total 68,485,120 tok), 27.20% MFU\n",
      "step 4190: loss 4.75 134.90ms/step 121454tok/s (total 68,648,960 tok), 27.11% MFU\n",
      "step 4200: loss 4.68 129.94ms/step 126089tok/s (total 68,812,800 tok), 28.15% MFU\n",
      "step 4200 eval: val_loss 4.71\n",
      "step 4210: loss 4.72 132.20ms/step 123930tok/s (total 68,976,640 tok), 27.67% MFU\n",
      "step 4220: loss 4.69 131.34ms/step 124746tok/s (total 69,140,480 tok), 27.85% MFU\n",
      "step 4230: loss 4.69 132.03ms/step 124089tok/s (total 69,304,320 tok), 27.70% MFU\n",
      "step 4240: loss 4.73 133.54ms/step 122693tok/s (total 69,468,160 tok), 27.39% MFU\n",
      "step 4250: loss 4.75 129.75ms/step 126273tok/s (total 69,632,000 tok), 28.19% MFU\n",
      "step 4250 eval: val_loss 4.77\n",
      "step 4260: loss 4.73 132.44ms/step 123709tok/s (total 69,795,840 tok), 27.62% MFU\n",
      "step 4270: loss 4.71 135.05ms/step 121314tok/s (total 69,959,680 tok), 27.08% MFU\n",
      "step 4280: loss 4.65 130.08ms/step 125952tok/s (total 70,123,520 tok), 28.12% MFU\n",
      "step 4290: loss 4.63 129.21ms/step 126805tok/s (total 70,287,360 tok), 28.31% MFU\n",
      "step 4300: loss 4.71 131.01ms/step 125058tok/s (total 70,451,200 tok), 27.92% MFU\n",
      "step 4300 eval: val_loss 4.74\n",
      "step 4310: loss 4.68 133.04ms/step 123155tok/s (total 70,615,040 tok), 27.49% MFU\n",
      "step 4320: loss 4.64 131.67ms/step 124433tok/s (total 70,778,880 tok), 27.78% MFU\n",
      "step 4330: loss 4.70 130.35ms/step 125694tok/s (total 70,942,720 tok), 28.06% MFU\n",
      "step 4340: loss 4.65 128.98ms/step 127025tok/s (total 71,106,560 tok), 28.36% MFU\n",
      "step 4350: loss 4.74 134.04ms/step 122231tok/s (total 71,270,400 tok), 27.29% MFU\n",
      "step 4350 eval: val_loss 4.65\n",
      "step 4360: loss 4.72 129.38ms/step 126639tok/s (total 71,434,240 tok), 28.27% MFU\n",
      "step 4370: loss 4.65 129.50ms/step 126516tok/s (total 71,598,080 tok), 28.24% MFU\n",
      "step 4380: loss 4.63 132.76ms/step 123412tok/s (total 71,761,920 tok), 27.55% MFU\n",
      "step 4390: loss 4.70 133.13ms/step 123067tok/s (total 71,925,760 tok), 27.47% MFU\n",
      "step 4400: loss 4.73 131.42ms/step 124672tok/s (total 72,089,600 tok), 27.83% MFU\n",
      "step 4400 eval: val_loss 4.70\n",
      "step 4410: loss 4.70 130.77ms/step 125290tok/s (total 72,253,440 tok), 27.97% MFU\n",
      "step 4420: loss 4.69 131.03ms/step 125041tok/s (total 72,417,280 tok), 27.92% MFU\n",
      "step 4430: loss 4.68 133.67ms/step 122570tok/s (total 72,581,120 tok), 27.36% MFU\n",
      "step 4440: loss 4.66 134.25ms/step 122039tok/s (total 72,744,960 tok), 27.25% MFU\n",
      "step 4450: loss 4.66 129.94ms/step 126088tok/s (total 72,908,800 tok), 28.15% MFU\n",
      "step 4450 eval: val_loss 4.64\n",
      "step 4460: loss 4.64 132.77ms/step 123406tok/s (total 73,072,640 tok), 27.55% MFU\n",
      "step 4470: loss 4.69 131.21ms/step 124864tok/s (total 73,236,480 tok), 27.88% MFU\n",
      "step 4480: loss 4.79 132.36ms/step 123788tok/s (total 73,400,320 tok), 27.64% MFU\n",
      "step 4490: loss 4.71 129.94ms/step 126091tok/s (total 73,564,160 tok), 28.15% MFU\n",
      "step 4500: loss 4.72 133.97ms/step 122299tok/s (total 73,728,000 tok), 27.30% MFU\n",
      "step 4500 eval: val_loss 4.64\n",
      "step 4510: loss 4.70 131.24ms/step 124837tok/s (total 73,891,840 tok), 27.87% MFU\n",
      "step 4520: loss 4.69 132.34ms/step 123802tok/s (total 74,055,680 tok), 27.64% MFU\n",
      "step 4530: loss 4.68 130.51ms/step 125540tok/s (total 74,219,520 tok), 28.03% MFU\n",
      "step 4540: loss 4.64 133.10ms/step 123097tok/s (total 74,383,360 tok), 27.48% MFU\n",
      "step 4550: loss 4.74 132.57ms/step 123592tok/s (total 74,547,200 tok), 27.59% MFU\n",
      "step 4550 eval: val_loss 4.68\n",
      "step 4560: loss 4.71 130.79ms/step 125265tok/s (total 74,711,040 tok), 27.97% MFU\n",
      "step 4570: loss 4.70 132.53ms/step 123626tok/s (total 74,874,880 tok), 27.60% MFU\n",
      "step 4580: loss 4.68 135.36ms/step 121039tok/s (total 75,038,720 tok), 27.02% MFU\n",
      "step 4590: loss 4.73 131.03ms/step 125043tok/s (total 75,202,560 tok), 27.92% MFU\n",
      "step 4600: loss 4.73 132.37ms/step 123776tok/s (total 75,366,400 tok), 27.63% MFU\n",
      "step 4600 eval: val_loss 4.66\n",
      "step 4610: loss 4.67 130.91ms/step 125159tok/s (total 75,530,240 tok), 27.94% MFU\n",
      "step 4620: loss 4.68 129.42ms/step 126593tok/s (total 75,694,080 tok), 28.26% MFU\n",
      "step 4630: loss 4.60 129.28ms/step 126736tok/s (total 75,857,920 tok), 28.29% MFU\n",
      "step 4640: loss 4.65 129.66ms/step 126361tok/s (total 76,021,760 tok), 28.21% MFU\n",
      "step 4650: loss 4.63 129.70ms/step 126324tok/s (total 76,185,600 tok), 28.20% MFU\n",
      "step 4650 eval: val_loss 4.67\n",
      "step 4660: loss 4.65 132.50ms/step 123650tok/s (total 76,349,440 tok), 27.61% MFU\n",
      "step 4670: loss 4.66 131.85ms/step 124264tok/s (total 76,513,280 tok), 27.74% MFU\n",
      "step 4680: loss 4.64 131.88ms/step 124238tok/s (total 76,677,120 tok), 27.74% MFU\n",
      "step 4690: loss 4.68 136.41ms/step 120107tok/s (total 76,840,960 tok), 26.81% MFU\n",
      "step 4700: loss 4.62 134.97ms/step 121388tok/s (total 77,004,800 tok), 27.10% MFU\n",
      "step 4700 eval: val_loss 4.61\n",
      "step 4710: loss 4.66 130.98ms/step 125085tok/s (total 77,168,640 tok), 27.93% MFU\n",
      "step 4720: loss 4.65 129.95ms/step 126079tok/s (total 77,332,480 tok), 28.15% MFU\n",
      "step 4730: loss 4.74 127.97ms/step 128029tok/s (total 77,496,320 tok), 28.58% MFU\n",
      "step 4740: loss 4.65 133.42ms/step 122799tok/s (total 77,660,160 tok), 27.42% MFU\n",
      "step 4750: loss 4.69 133.97ms/step 122295tok/s (total 77,824,000 tok), 27.30% MFU\n",
      "step 4750 eval: val_loss 4.60\n",
      "step 4760: loss 4.55 132.59ms/step 123566tok/s (total 77,987,840 tok), 27.59% MFU\n",
      "step 4770: loss 4.74 131.66ms/step 124443tok/s (total 78,151,680 tok), 27.78% MFU\n",
      "step 4780: loss 4.64 131.10ms/step 124977tok/s (total 78,315,520 tok), 27.90% MFU\n",
      "step 4790: loss 4.62 133.28ms/step 122926tok/s (total 78,479,360 tok), 27.44% MFU\n",
      "step 4800: loss 4.58 130.82ms/step 125241tok/s (total 78,643,200 tok), 27.96% MFU\n",
      "step 4800 eval: val_loss 4.54\n",
      "step 4810: loss 4.75 137.32ms/step 119314tok/s (total 78,807,040 tok), 26.64% MFU\n",
      "step 4820: loss 4.62 129.77ms/step 126257tok/s (total 78,970,880 tok), 28.19% MFU\n",
      "step 4830: loss 4.65 128.62ms/step 127379tok/s (total 79,134,720 tok), 28.44% MFU\n",
      "step 4840: loss 4.71 129.89ms/step 126141tok/s (total 79,298,560 tok), 28.16% MFU\n",
      "step 4850: loss 4.67 131.89ms/step 124221tok/s (total 79,462,400 tok), 27.73% MFU\n",
      "step 4850 eval: val_loss 4.59\n",
      "step 4860: loss 4.67 129.84ms/step 126183tok/s (total 79,626,240 tok), 28.17% MFU\n",
      "step 4870: loss 4.71 135.71ms/step 120728tok/s (total 79,790,080 tok), 26.95% MFU\n",
      "step 4880: loss 4.75 132.02ms/step 124105tok/s (total 79,953,920 tok), 27.71% MFU\n",
      "step 4890: loss 4.65 129.26ms/step 126753tok/s (total 80,117,760 tok), 28.30% MFU\n",
      "step 4900: loss 4.55 130.06ms/step 125969tok/s (total 80,281,600 tok), 28.12% MFU\n",
      "step 4900 eval: val_loss 4.56\n",
      "step 4910: loss 4.65 130.40ms/step 125644tok/s (total 80,445,440 tok), 28.05% MFU\n",
      "step 4920: loss 4.64 129.37ms/step 126648tok/s (total 80,609,280 tok), 28.27% MFU\n",
      "step 4930: loss 4.67 133.87ms/step 122391tok/s (total 80,773,120 tok), 27.32% MFU\n",
      "step 4940: loss 4.67 131.83ms/step 124282tok/s (total 80,936,960 tok), 27.75% MFU\n",
      "step 4950: loss 4.62 132.61ms/step 123555tok/s (total 81,100,800 tok), 27.58% MFU\n",
      "step 4950 eval: val_loss 4.66\n",
      "step 4960: loss 4.59 129.57ms/step 126451tok/s (total 81,264,640 tok), 28.23% MFU\n",
      "step 4970: loss 4.63 135.22ms/step 121164tok/s (total 81,428,480 tok), 27.05% MFU\n",
      "step 4980: loss 4.68 132.63ms/step 123531tok/s (total 81,592,320 tok), 27.58% MFU\n",
      "step 4990: loss 4.60 133.40ms/step 122819tok/s (total 81,756,160 tok), 27.42% MFU\n",
      "step 5000: loss 4.62 130.74ms/step 125315tok/s (total 81,920,000 tok), 27.98% MFU\n",
      "step 5000 eval: val_loss 4.75\n",
      "step 5010: loss 4.63 133.28ms/step 122927tok/s (total 82,083,840 tok), 27.44% MFU\n",
      "step 5020: loss 4.62 132.62ms/step 123538tok/s (total 82,247,680 tok), 27.58% MFU\n",
      "step 5030: loss 4.64 129.71ms/step 126314tok/s (total 82,411,520 tok), 28.20% MFU\n",
      "step 5040: loss 4.67 129.63ms/step 126389tok/s (total 82,575,360 tok), 28.22% MFU\n",
      "step 5050: loss 4.68 132.37ms/step 123772tok/s (total 82,739,200 tok), 27.63% MFU\n",
      "step 5050 eval: val_loss 4.57\n",
      "step 5060: loss 4.56 132.01ms/step 124110tok/s (total 82,903,040 tok), 27.71% MFU\n",
      "step 5070: loss 4.61 129.00ms/step 127011tok/s (total 83,066,880 tok), 28.36% MFU\n",
      "step 5080: loss 4.65 130.80ms/step 125260tok/s (total 83,230,720 tok), 27.96% MFU\n",
      "step 5090: loss 4.61 132.17ms/step 123964tok/s (total 83,394,560 tok), 27.68% MFU\n",
      "step 5100: loss 4.61 133.81ms/step 122438tok/s (total 83,558,400 tok), 27.33% MFU\n",
      "step 5100 eval: val_loss 4.61\n",
      "step 5110: loss 4.62 132.70ms/step 123470tok/s (total 83,722,240 tok), 27.56% MFU\n",
      "step 5120: loss 4.70 132.85ms/step 123326tok/s (total 83,886,080 tok), 27.53% MFU\n",
      "step 5130: loss 4.65 129.92ms/step 126111tok/s (total 84,049,920 tok), 28.15% MFU\n",
      "step 5140: loss 4.68 129.99ms/step 126045tok/s (total 84,213,760 tok), 28.14% MFU\n",
      "step 5150: loss 4.67 131.66ms/step 124443tok/s (total 84,377,600 tok), 27.78% MFU\n",
      "step 5150 eval: val_loss 4.55\n",
      "step 5160: loss 4.64 133.08ms/step 123112tok/s (total 84,541,440 tok), 27.48% MFU\n",
      "step 5170: loss 4.63 130.51ms/step 125538tok/s (total 84,705,280 tok), 28.03% MFU\n",
      "step 5180: loss 4.66 131.80ms/step 124308tok/s (total 84,869,120 tok), 27.75% MFU\n",
      "step 5190: loss 4.61 130.17ms/step 125866tok/s (total 85,032,960 tok), 28.10% MFU\n",
      "step 5200: loss 4.67 132.58ms/step 123579tok/s (total 85,196,800 tok), 27.59% MFU\n",
      "step 5200 eval: val_loss 4.63\n",
      "step 5210: loss 4.55 134.24ms/step 122048tok/s (total 85,360,640 tok), 27.25% MFU\n",
      "step 5220: loss 4.57 130.82ms/step 125242tok/s (total 85,524,480 tok), 27.96% MFU\n",
      "step 5230: loss 4.64 131.72ms/step 124388tok/s (total 85,688,320 tok), 27.77% MFU\n",
      "step 5240: loss 4.59 131.31ms/step 124772tok/s (total 85,852,160 tok), 27.86% MFU\n",
      "step 5250: loss 4.61 129.72ms/step 126302tok/s (total 86,016,000 tok), 28.20% MFU\n",
      "step 5250 eval: val_loss 4.61\n",
      "step 5260: loss 4.66 133.87ms/step 122389tok/s (total 86,179,840 tok), 27.32% MFU\n",
      "step 5270: loss 4.73 133.29ms/step 122923tok/s (total 86,343,680 tok), 27.44% MFU\n",
      "step 5280: loss 4.55 134.68ms/step 121652tok/s (total 86,507,520 tok), 27.16% MFU\n",
      "step 5290: loss 4.65 132.94ms/step 123243tok/s (total 86,671,360 tok), 27.51% MFU\n",
      "step 5300: loss 4.62 133.12ms/step 123078tok/s (total 86,835,200 tok), 27.48% MFU\n",
      "step 5300 eval: val_loss 4.62\n",
      "step 5310: loss 4.62 132.14ms/step 123991tok/s (total 86,999,040 tok), 27.68% MFU\n",
      "step 5320: loss 4.65 135.24ms/step 121146tok/s (total 87,162,880 tok), 27.05% MFU\n",
      "step 5330: loss 4.65 135.73ms/step 120711tok/s (total 87,326,720 tok), 26.95% MFU\n",
      "step 5340: loss 4.65 132.33ms/step 123808tok/s (total 87,490,560 tok), 27.64% MFU\n",
      "step 5350: loss 4.58 130.59ms/step 125457tok/s (total 87,654,400 tok), 28.01% MFU\n",
      "step 5350 eval: val_loss 4.58\n",
      "step 5360: loss 4.54 134.14ms/step 122138tok/s (total 87,818,240 tok), 27.27% MFU\n",
      "step 5370: loss 4.60 130.30ms/step 125741tok/s (total 87,982,080 tok), 28.07% MFU\n",
      "step 5380: loss 4.64 129.93ms/step 126103tok/s (total 88,145,920 tok), 28.15% MFU\n",
      "step 5390: loss 4.58 132.69ms/step 123479tok/s (total 88,309,760 tok), 27.57% MFU\n",
      "step 5400: loss 4.62 132.57ms/step 123589tok/s (total 88,473,600 tok), 27.59% MFU\n",
      "step 5400 eval: val_loss 4.56\n",
      "step 5410: loss 4.62 130.14ms/step 125891tok/s (total 88,637,440 tok), 28.11% MFU\n",
      "step 5420: loss 4.64 131.21ms/step 124864tok/s (total 88,801,280 tok), 27.88% MFU\n",
      "step 5430: loss 4.57 130.09ms/step 125948tok/s (total 88,965,120 tok), 28.12% MFU\n",
      "step 5440: loss 4.53 132.19ms/step 123945tok/s (total 89,128,960 tok), 27.67% MFU\n",
      "step 5450: loss 4.58 130.95ms/step 125114tok/s (total 89,292,800 tok), 27.93% MFU\n",
      "step 5450 eval: val_loss 4.62\n",
      "step 5460: loss 4.69 129.38ms/step 126631tok/s (total 89,456,640 tok), 28.27% MFU\n",
      "step 5470: loss 4.67 130.56ms/step 125488tok/s (total 89,620,480 tok), 28.02% MFU\n",
      "step 5480: loss 4.70 130.85ms/step 125209tok/s (total 89,784,320 tok), 27.95% MFU\n",
      "step 5490: loss 4.63 132.53ms/step 123626tok/s (total 89,948,160 tok), 27.60% MFU\n",
      "step 5500: loss 4.62 132.23ms/step 123902tok/s (total 90,112,000 tok), 27.66% MFU\n",
      "step 5500 eval: val_loss 4.64\n",
      "step 5510: loss 4.60 134.06ms/step 122212tok/s (total 90,275,840 tok), 27.28% MFU\n",
      "step 5520: loss 4.60 131.55ms/step 124544tok/s (total 90,439,680 tok), 27.80% MFU\n",
      "step 5530: loss 4.70 133.65ms/step 122593tok/s (total 90,603,520 tok), 27.37% MFU\n",
      "step 5540: loss 4.59 135.36ms/step 121043tok/s (total 90,767,360 tok), 27.02% MFU\n",
      "step 5550: loss 4.67 129.54ms/step 126477tok/s (total 90,931,200 tok), 28.24% MFU\n",
      "step 5550 eval: val_loss 4.61\n",
      "step 5560: loss 4.61 136.60ms/step 119942tok/s (total 91,095,040 tok), 26.78% MFU\n",
      "step 5570: loss 4.58 132.78ms/step 123388tok/s (total 91,258,880 tok), 27.55% MFU\n",
      "step 5580: loss 4.58 130.86ms/step 125203tok/s (total 91,422,720 tok), 27.95% MFU\n",
      "step 5590: loss 4.65 133.48ms/step 122743tok/s (total 91,586,560 tok), 27.40% MFU\n",
      "step 5600: loss 4.64 129.70ms/step 126322tok/s (total 91,750,400 tok), 28.20% MFU\n",
      "step 5600 eval: val_loss 4.58\n",
      "step 5610: loss 4.56 131.29ms/step 124791tok/s (total 91,914,240 tok), 27.86% MFU\n",
      "step 5620: loss 4.52 132.39ms/step 123753tok/s (total 92,078,080 tok), 27.63% MFU\n",
      "step 5630: loss 4.60 134.74ms/step 121599tok/s (total 92,241,920 tok), 27.15% MFU\n",
      "step 5640: loss 4.63 132.87ms/step 123309tok/s (total 92,405,760 tok), 27.53% MFU\n",
      "step 5650: loss 4.58 133.85ms/step 122403tok/s (total 92,569,600 tok), 27.33% MFU\n",
      "step 5650 eval: val_loss 4.66\n",
      "step 5660: loss 4.55 129.96ms/step 126074tok/s (total 92,733,440 tok), 28.15% MFU\n",
      "step 5670: loss 4.54 129.99ms/step 126040tok/s (total 92,897,280 tok), 28.14% MFU\n",
      "step 5680: loss 4.55 129.46ms/step 126561tok/s (total 93,061,120 tok), 28.25% MFU\n",
      "step 5690: loss 4.55 129.73ms/step 126291tok/s (total 93,224,960 tok), 28.19% MFU\n",
      "step 5700: loss 4.52 132.97ms/step 123213tok/s (total 93,388,800 tok), 27.51% MFU\n",
      "step 5700 eval: val_loss 4.57\n",
      "step 5710: loss 4.60 129.55ms/step 126467tok/s (total 93,552,640 tok), 28.23% MFU\n",
      "step 5720: loss 4.57 129.51ms/step 126504tok/s (total 93,716,480 tok), 28.24% MFU\n",
      "step 5730: loss 4.54 131.39ms/step 124694tok/s (total 93,880,320 tok), 27.84% MFU\n",
      "step 5740: loss 4.63 131.85ms/step 124259tok/s (total 94,044,160 tok), 27.74% MFU\n",
      "step 5750: loss 4.54 133.32ms/step 122891tok/s (total 94,208,000 tok), 27.44% MFU\n",
      "step 5750 eval: val_loss 4.65\n",
      "step 5760: loss 4.64 132.16ms/step 123967tok/s (total 94,371,840 tok), 27.68% MFU\n",
      "step 5770: loss 4.53 132.08ms/step 124046tok/s (total 94,535,680 tok), 27.69% MFU\n",
      "step 5780: loss 4.62 132.23ms/step 123906tok/s (total 94,699,520 tok), 27.66% MFU\n",
      "step 5790: loss 4.60 133.97ms/step 122299tok/s (total 94,863,360 tok), 27.30% MFU\n",
      "step 5800: loss 4.59 131.68ms/step 124422tok/s (total 95,027,200 tok), 27.78% MFU\n",
      "step 5800 eval: val_loss 4.57\n",
      "step 5810: loss 4.53 132.31ms/step 123833tok/s (total 95,191,040 tok), 27.65% MFU\n",
      "step 5820: loss 4.54 132.69ms/step 123474tok/s (total 95,354,880 tok), 27.57% MFU\n",
      "step 5830: loss 4.64 132.69ms/step 123477tok/s (total 95,518,720 tok), 27.57% MFU\n",
      "step 5840: loss 4.66 132.50ms/step 123656tok/s (total 95,682,560 tok), 27.61% MFU\n",
      "step 5850: loss 4.69 135.33ms/step 121071tok/s (total 95,846,400 tok), 27.03% MFU\n",
      "step 5850 eval: val_loss 4.60\n",
      "step 5860: loss 4.56 135.19ms/step 121196tok/s (total 96,010,240 tok), 27.06% MFU\n",
      "step 5870: loss 4.59 129.64ms/step 126382tok/s (total 96,174,080 tok), 28.21% MFU\n",
      "step 5880: loss 4.52 134.49ms/step 121827tok/s (total 96,337,920 tok), 27.20% MFU\n",
      "step 5890: loss 4.68 131.21ms/step 124867tok/s (total 96,501,760 tok), 27.88% MFU\n",
      "step 5900: loss 4.64 131.10ms/step 124976tok/s (total 96,665,600 tok), 27.90% MFU\n",
      "step 5900 eval: val_loss 4.55\n",
      "step 5910: loss 4.55 130.74ms/step 125315tok/s (total 96,829,440 tok), 27.98% MFU\n",
      "step 5920: loss 4.69 134.56ms/step 121763tok/s (total 96,993,280 tok), 27.18% MFU\n",
      "step 5930: loss 4.68 130.98ms/step 125083tok/s (total 97,157,120 tok), 27.93% MFU\n",
      "step 5940: loss 4.59 128.33ms/step 127667tok/s (total 97,320,960 tok), 28.50% MFU\n",
      "step 5950: loss 4.62 134.87ms/step 121478tok/s (total 97,484,800 tok), 27.12% MFU\n",
      "step 5950 eval: val_loss 4.55\n",
      "step 5960: loss 4.52 133.51ms/step 122719tok/s (total 97,648,640 tok), 27.40% MFU\n",
      "step 5970: loss 4.56 129.41ms/step 126603tok/s (total 97,812,480 tok), 28.26% MFU\n",
      "step 5980: loss 4.60 137.50ms/step 119160tok/s (total 97,976,320 tok), 26.60% MFU\n",
      "step 5990: loss 4.58 132.79ms/step 123382tok/s (total 98,140,160 tok), 27.55% MFU\n",
      "step 6000: loss 4.60 136.96ms/step 119628tok/s (total 98,304,000 tok), 26.71% MFU\n",
      "step 6000 eval: val_loss 4.50\n",
      "step 6010: loss 4.55 129.79ms/step 126235tok/s (total 98,467,840 tok), 28.18% MFU\n",
      "step 6020: loss 4.58 131.88ms/step 124234tok/s (total 98,631,680 tok), 27.74% MFU\n",
      "step 6030: loss 4.51 136.01ms/step 120463tok/s (total 98,795,520 tok), 26.89% MFU\n",
      "step 6040: loss 4.67 131.30ms/step 124787tok/s (total 98,959,360 tok), 27.86% MFU\n",
      "step 6050: loss 4.54 132.71ms/step 123459tok/s (total 99,123,200 tok), 27.56% MFU\n",
      "step 6050 eval: val_loss 4.53\n",
      "step 6060: loss 4.51 136.56ms/step 119972tok/s (total 99,287,040 tok), 26.78% MFU\n",
      "step 6070: loss 4.57 131.11ms/step 124962tok/s (total 99,450,880 tok), 27.90% MFU\n",
      "step 6080: loss 4.64 131.67ms/step 124437tok/s (total 99,614,720 tok), 27.78% MFU\n",
      "step 6090: loss 4.59 130.87ms/step 125192tok/s (total 99,778,560 tok), 27.95% MFU\n",
      "step 6100: loss 4.59 128.80ms/step 127205tok/s (total 99,942,400 tok), 28.40% MFU\n",
      "step 6100 eval: val_loss 4.47\n",
      "step 6110: loss 4.65 130.76ms/step 125298tok/s (total 100,106,240 tok), 27.97% MFU\n",
      "step 6120: loss 4.62 131.75ms/step 124358tok/s (total 100,270,080 tok), 27.76% MFU\n",
      "step 6130: loss 4.66 131.71ms/step 124396tok/s (total 100,433,920 tok), 27.77% MFU\n",
      "step 6140: loss 4.62 133.25ms/step 122955tok/s (total 100,597,760 tok), 27.45% MFU\n",
      "step 6150: loss 4.70 135.62ms/step 120806tok/s (total 100,761,600 tok), 26.97% MFU\n",
      "step 6150 eval: val_loss 4.61\n",
      "step 6160: loss 4.62 130.01ms/step 126024tok/s (total 100,925,440 tok), 28.14% MFU\n",
      "step 6170: loss 4.63 132.91ms/step 123276tok/s (total 101,089,280 tok), 27.52% MFU\n",
      "step 6180: loss 4.54 129.60ms/step 126420tok/s (total 101,253,120 tok), 28.22% MFU\n",
      "step 6190: loss 4.54 130.15ms/step 125885tok/s (total 101,416,960 tok), 28.10% MFU\n",
      "step 6200: loss 4.44 129.74ms/step 126288tok/s (total 101,580,800 tok), 28.19% MFU\n",
      "step 6200 eval: val_loss 4.64\n",
      "step 6210: loss 4.58 137.91ms/step 118804tok/s (total 101,744,640 tok), 26.52% MFU\n",
      "step 6220: loss 4.58 132.84ms/step 123334tok/s (total 101,908,480 tok), 27.53% MFU\n",
      "step 6230: loss 4.56 135.07ms/step 121298tok/s (total 102,072,320 tok), 27.08% MFU\n",
      "step 6240: loss 4.66 129.22ms/step 126791tok/s (total 102,236,160 tok), 28.31% MFU\n",
      "step 6250: loss 4.48 132.62ms/step 123541tok/s (total 102,400,000 tok), 27.58% MFU\n",
      "step 6250 eval: val_loss 4.55\n",
      "step 6260: loss 4.62 132.73ms/step 123443tok/s (total 102,563,840 tok), 27.56% MFU\n",
      "step 6270: loss 4.65 132.99ms/step 123198tok/s (total 102,727,680 tok), 27.50% MFU\n",
      "step 6280: loss 4.52 132.71ms/step 123455tok/s (total 102,891,520 tok), 27.56% MFU\n",
      "step 6290: loss 4.66 131.90ms/step 124215tok/s (total 103,055,360 tok), 27.73% MFU\n",
      "step 6300: loss 4.59 130.86ms/step 125204tok/s (total 103,219,200 tok), 27.95% MFU\n",
      "step 6300 eval: val_loss 4.59\n",
      "step 6310: loss 4.59 133.52ms/step 122704tok/s (total 103,383,040 tok), 27.39% MFU\n",
      "step 6320: loss 4.52 134.65ms/step 121680tok/s (total 103,546,880 tok), 27.17% MFU\n",
      "step 6330: loss 4.49 134.14ms/step 122140tok/s (total 103,710,720 tok), 27.27% MFU\n",
      "step 6340: loss 4.53 131.46ms/step 124632tok/s (total 103,874,560 tok), 27.82% MFU\n",
      "step 6350: loss 4.63 131.97ms/step 124154tok/s (total 104,038,400 tok), 27.72% MFU\n",
      "step 6350 eval: val_loss 4.59\n",
      "step 6360: loss 4.50 133.16ms/step 123043tok/s (total 104,202,240 tok), 27.47% MFU\n",
      "step 6370: loss 4.57 128.51ms/step 127496tok/s (total 104,366,080 tok), 28.46% MFU\n",
      "step 6380: loss 4.57 134.10ms/step 122179tok/s (total 104,529,920 tok), 27.28% MFU\n",
      "step 6390: loss 4.60 132.22ms/step 123912tok/s (total 104,693,760 tok), 27.66% MFU\n",
      "step 6400: loss 4.58 130.45ms/step 125592tok/s (total 104,857,600 tok), 28.04% MFU\n",
      "step 6400 eval: val_loss 4.54\n",
      "step 6410: loss 4.46 133.69ms/step 122556tok/s (total 105,021,440 tok), 27.36% MFU\n",
      "step 6420: loss 4.54 129.36ms/step 126657tok/s (total 105,185,280 tok), 28.28% MFU\n",
      "step 6430: loss 4.56 131.69ms/step 124410tok/s (total 105,349,120 tok), 27.77% MFU\n",
      "step 6440: loss 4.49 130.99ms/step 125075tok/s (total 105,512,960 tok), 27.92% MFU\n",
      "step 6450: loss 4.52 134.24ms/step 122047tok/s (total 105,676,800 tok), 27.25% MFU\n",
      "step 6450 eval: val_loss 4.52\n",
      "step 6460: loss 4.62 130.52ms/step 125531tok/s (total 105,840,640 tok), 28.03% MFU\n",
      "step 6470: loss 4.49 131.45ms/step 124643tok/s (total 106,004,480 tok), 27.83% MFU\n",
      "step 6480: loss 4.56 131.57ms/step 124530tok/s (total 106,168,320 tok), 27.80% MFU\n",
      "step 6490: loss 4.57 132.42ms/step 123724tok/s (total 106,332,160 tok), 27.62% MFU\n",
      "step 6500: loss 4.52 130.03ms/step 125998tok/s (total 106,496,000 tok), 28.13% MFU\n",
      "step 6500 eval: val_loss 4.55\n",
      "step 6510: loss 4.62 128.74ms/step 127262tok/s (total 106,659,840 tok), 28.41% MFU\n",
      "step 6520: loss 4.51 132.92ms/step 123259tok/s (total 106,823,680 tok), 27.52% MFU\n",
      "step 6530: loss 4.64 129.77ms/step 126257tok/s (total 106,987,520 tok), 28.19% MFU\n",
      "step 6540: loss 4.60 132.14ms/step 123992tok/s (total 107,151,360 tok), 27.68% MFU\n",
      "step 6550: loss 4.54 130.37ms/step 125672tok/s (total 107,315,200 tok), 28.06% MFU\n",
      "step 6550 eval: val_loss 4.57\n",
      "step 6560: loss 4.52 129.59ms/step 126427tok/s (total 107,479,040 tok), 28.23% MFU\n",
      "step 6570: loss 4.58 133.65ms/step 122589tok/s (total 107,642,880 tok), 27.37% MFU\n",
      "step 6580: loss 4.48 131.13ms/step 124945tok/s (total 107,806,720 tok), 27.89% MFU\n",
      "step 6590: loss 4.46 133.22ms/step 122987tok/s (total 107,970,560 tok), 27.46% MFU\n",
      "step 6600: loss 4.60 132.52ms/step 123634tok/s (total 108,134,400 tok), 27.60% MFU\n",
      "step 6600 eval: val_loss 4.49\n",
      "step 6610: loss 4.57 129.75ms/step 126273tok/s (total 108,298,240 tok), 28.19% MFU\n",
      "step 6620: loss 4.57 131.61ms/step 124491tok/s (total 108,462,080 tok), 27.79% MFU\n",
      "step 6630: loss 4.52 136.15ms/step 120334tok/s (total 108,625,920 tok), 26.86% MFU\n",
      "step 6640: loss 4.57 132.79ms/step 123387tok/s (total 108,789,760 tok), 27.55% MFU\n",
      "step 6650: loss 4.61 128.39ms/step 127608tok/s (total 108,953,600 tok), 28.49% MFU\n",
      "step 6650 eval: val_loss 4.52\n",
      "step 6660: loss 4.56 133.72ms/step 122527tok/s (total 109,117,440 tok), 27.35% MFU\n",
      "step 6670: loss 4.56 135.39ms/step 121014tok/s (total 109,281,280 tok), 27.02% MFU\n",
      "step 6680: loss 4.52 131.38ms/step 124706tok/s (total 109,445,120 tok), 27.84% MFU\n",
      "step 6690: loss 4.58 128.59ms/step 127408tok/s (total 109,608,960 tok), 28.44% MFU\n",
      "step 6700: loss 4.52 132.32ms/step 123820tok/s (total 109,772,800 tok), 27.64% MFU\n",
      "step 6700 eval: val_loss 4.55\n",
      "step 6710: loss 4.51 129.50ms/step 126522tok/s (total 109,936,640 tok), 28.25% MFU\n",
      "step 6720: loss 4.54 130.54ms/step 125506tok/s (total 110,100,480 tok), 28.02% MFU\n",
      "step 6730: loss 4.55 136.35ms/step 120161tok/s (total 110,264,320 tok), 26.83% MFU\n",
      "step 6740: loss 4.59 132.66ms/step 123501tok/s (total 110,428,160 tok), 27.57% MFU\n",
      "step 6750: loss 4.56 132.17ms/step 123966tok/s (total 110,592,000 tok), 27.68% MFU\n",
      "step 6750 eval: val_loss 4.61\n",
      "step 6760: loss 4.57 135.63ms/step 120802tok/s (total 110,755,840 tok), 26.97% MFU\n",
      "step 6770: loss 4.48 130.25ms/step 125791tok/s (total 110,919,680 tok), 28.08% MFU\n",
      "step 6780: loss 4.49 129.53ms/step 126487tok/s (total 111,083,520 tok), 28.24% MFU\n",
      "step 6790: loss 4.62 130.77ms/step 125291tok/s (total 111,247,360 tok), 27.97% MFU\n",
      "step 6800: loss 4.60 130.11ms/step 125929tok/s (total 111,411,200 tok), 28.11% MFU\n",
      "step 6800 eval: val_loss 4.55\n",
      "step 6810: loss 4.51 129.40ms/step 126620tok/s (total 111,575,040 tok), 28.27% MFU\n",
      "step 6820: loss 4.58 131.69ms/step 124418tok/s (total 111,738,880 tok), 27.78% MFU\n",
      "step 6830: loss 4.47 132.41ms/step 123738tok/s (total 111,902,720 tok), 27.62% MFU\n",
      "step 6840: loss 4.58 129.71ms/step 126315tok/s (total 112,066,560 tok), 28.20% MFU\n",
      "step 6850: loss 4.49 134.70ms/step 121629tok/s (total 112,230,400 tok), 27.15% MFU\n",
      "step 6850 eval: val_loss 4.49\n",
      "step 6860: loss 4.53 130.15ms/step 125883tok/s (total 112,394,240 tok), 28.10% MFU\n",
      "step 6870: loss 4.50 132.84ms/step 123336tok/s (total 112,558,080 tok), 27.54% MFU\n",
      "step 6880: loss 4.57 133.67ms/step 122572tok/s (total 112,721,920 tok), 27.36% MFU\n",
      "step 6890: loss 4.51 133.27ms/step 122942tok/s (total 112,885,760 tok), 27.45% MFU\n",
      "step 6900: loss 4.56 132.25ms/step 123883tok/s (total 113,049,600 tok), 27.66% MFU\n",
      "step 6900 eval: val_loss 4.50\n",
      "step 6910: loss 4.54 134.23ms/step 122061tok/s (total 113,213,440 tok), 27.25% MFU\n",
      "step 6920: loss 4.58 130.82ms/step 125241tok/s (total 113,377,280 tok), 27.96% MFU\n",
      "step 6930: loss 4.46 132.81ms/step 123367tok/s (total 113,541,120 tok), 27.54% MFU\n",
      "step 6940: loss 4.55 131.75ms/step 124354tok/s (total 113,704,960 tok), 27.76% MFU\n",
      "step 6950: loss 4.56 131.60ms/step 124495tok/s (total 113,868,800 tok), 27.79% MFU\n",
      "step 6950 eval: val_loss 4.46\n",
      "step 6960: loss 4.58 133.48ms/step 122749tok/s (total 114,032,640 tok), 27.40% MFU\n",
      "step 6970: loss 4.50 133.38ms/step 122840tok/s (total 114,196,480 tok), 27.42% MFU\n",
      "step 6980: loss 4.48 131.32ms/step 124764tok/s (total 114,360,320 tok), 27.85% MFU\n",
      "step 6990: loss 4.45 132.11ms/step 124020tok/s (total 114,524,160 tok), 27.69% MFU\n",
      "step 7000: loss 4.49 133.51ms/step 122716tok/s (total 114,688,000 tok), 27.40% MFU\n",
      "step 7000 eval: val_loss 4.53\n",
      "step 7010: loss 4.53 132.38ms/step 123763tok/s (total 114,851,840 tok), 27.63% MFU\n",
      "step 7020: loss 4.61 131.66ms/step 124439tok/s (total 115,015,680 tok), 27.78% MFU\n",
      "step 7030: loss 4.55 134.30ms/step 121997tok/s (total 115,179,520 tok), 27.24% MFU\n",
      "step 7040: loss 4.44 134.36ms/step 121942tok/s (total 115,343,360 tok), 27.22% MFU\n",
      "step 7050: loss 4.58 130.70ms/step 125360tok/s (total 115,507,200 tok), 27.99% MFU\n",
      "step 7050 eval: val_loss 4.51\n",
      "step 7060: loss 4.59 132.46ms/step 123687tok/s (total 115,671,040 tok), 27.61% MFU\n",
      "step 7070: loss 4.64 131.06ms/step 125010tok/s (total 115,834,880 tok), 27.91% MFU\n",
      "step 7080: loss 4.54 132.34ms/step 123806tok/s (total 115,998,720 tok), 27.64% MFU\n",
      "step 7090: loss 4.55 129.63ms/step 126393tok/s (total 116,162,560 tok), 28.22% MFU\n",
      "step 7100: loss 4.50 131.30ms/step 124781tok/s (total 116,326,400 tok), 27.86% MFU\n",
      "step 7100 eval: val_loss 4.57\n",
      "step 7110: loss 4.56 134.40ms/step 121907tok/s (total 116,490,240 tok), 27.22% MFU\n",
      "step 7120: loss 4.49 131.50ms/step 124594tok/s (total 116,654,080 tok), 27.82% MFU\n",
      "step 7130: loss 4.55 132.55ms/step 123604tok/s (total 116,817,920 tok), 27.59% MFU\n",
      "step 7140: loss 4.45 132.10ms/step 124023tok/s (total 116,981,760 tok), 27.69% MFU\n",
      "step 7150: loss 4.49 134.80ms/step 121540tok/s (total 117,145,600 tok), 27.13% MFU\n",
      "step 7150 eval: val_loss 4.46\n",
      "step 7160: loss 4.53 132.76ms/step 123410tok/s (total 117,309,440 tok), 27.55% MFU\n",
      "step 7170: loss 4.49 135.59ms/step 120837tok/s (total 117,473,280 tok), 26.98% MFU\n",
      "step 7180: loss 4.56 133.53ms/step 122697tok/s (total 117,637,120 tok), 27.39% MFU\n",
      "step 7190: loss 4.50 130.57ms/step 125477tok/s (total 117,800,960 tok), 28.01% MFU\n",
      "step 7200: loss 4.57 127.79ms/step 128212tok/s (total 117,964,800 tok), 28.62% MFU\n",
      "step 7200 eval: val_loss 4.47\n",
      "step 7210: loss 4.56 133.16ms/step 123041tok/s (total 118,128,640 tok), 27.47% MFU\n",
      "step 7220: loss 4.63 130.85ms/step 125210tok/s (total 118,292,480 tok), 27.95% MFU\n",
      "step 7230: loss 4.50 132.54ms/step 123614tok/s (total 118,456,320 tok), 27.60% MFU\n",
      "step 7240: loss 4.59 131.32ms/step 124764tok/s (total 118,620,160 tok), 27.85% MFU\n",
      "step 7250: loss 4.47 129.99ms/step 126041tok/s (total 118,784,000 tok), 28.14% MFU\n",
      "step 7250 eval: val_loss 4.51\n",
      "step 7260: loss 4.51 133.59ms/step 122640tok/s (total 118,947,840 tok), 27.38% MFU\n",
      "step 7270: loss 4.53 130.45ms/step 125593tok/s (total 119,111,680 tok), 28.04% MFU\n",
      "step 7280: loss 4.50 129.54ms/step 126481tok/s (total 119,275,520 tok), 28.24% MFU\n",
      "step 7290: loss 4.52 134.12ms/step 122160tok/s (total 119,439,360 tok), 27.27% MFU\n",
      "step 7300: loss 4.50 129.33ms/step 126686tok/s (total 119,603,200 tok), 28.28% MFU\n",
      "step 7300 eval: val_loss 4.53\n",
      "step 7310: loss 4.51 133.96ms/step 122303tok/s (total 119,767,040 tok), 27.30% MFU\n",
      "step 7320: loss 4.45 130.67ms/step 125388tok/s (total 119,930,880 tok), 27.99% MFU\n",
      "step 7330: loss 4.53 129.93ms/step 126101tok/s (total 120,094,720 tok), 28.15% MFU\n",
      "step 7340: loss 4.41 132.74ms/step 123426tok/s (total 120,258,560 tok), 27.56% MFU\n",
      "step 7350: loss 4.53 129.81ms/step 126216tok/s (total 120,422,400 tok), 28.18% MFU\n",
      "step 7350 eval: val_loss 4.41\n",
      "step 7360: loss 4.57 131.25ms/step 124835tok/s (total 120,586,240 tok), 27.87% MFU\n",
      "step 7370: loss 4.52 132.39ms/step 123752tok/s (total 120,750,080 tok), 27.63% MFU\n",
      "step 7380: loss 4.43 129.34ms/step 126669tok/s (total 120,913,920 tok), 28.28% MFU\n",
      "step 7390: loss 4.49 132.15ms/step 123979tok/s (total 121,077,760 tok), 27.68% MFU\n",
      "step 7400: loss 4.52 135.53ms/step 120886tok/s (total 121,241,600 tok), 26.99% MFU\n",
      "step 7400 eval: val_loss 4.56\n",
      "step 7410: loss 4.56 129.85ms/step 126174tok/s (total 121,405,440 tok), 28.17% MFU\n",
      "step 7420: loss 4.51 131.25ms/step 124834tok/s (total 121,569,280 tok), 27.87% MFU\n",
      "step 7430: loss 4.44 131.88ms/step 124236tok/s (total 121,733,120 tok), 27.74% MFU\n",
      "step 7440: loss 4.48 135.57ms/step 120852tok/s (total 121,896,960 tok), 26.98% MFU\n",
      "step 7450: loss 4.49 128.83ms/step 127179tok/s (total 122,060,800 tok), 28.39% MFU\n",
      "step 7450 eval: val_loss 4.40\n",
      "step 7460: loss 4.49 129.66ms/step 126363tok/s (total 122,224,640 tok), 28.21% MFU\n",
      "step 7470: loss 4.45 134.71ms/step 121624tok/s (total 122,388,480 tok), 27.15% MFU\n",
      "step 7480: loss 4.53 129.32ms/step 126692tok/s (total 122,552,320 tok), 28.28% MFU\n",
      "step 7490: loss 4.54 129.96ms/step 126073tok/s (total 122,716,160 tok), 28.15% MFU\n",
      "step 7500: loss 4.50 131.11ms/step 124965tok/s (total 122,880,000 tok), 27.90% MFU\n",
      "step 7500 eval: val_loss 4.46\n",
      "step 7510: loss 4.48 128.75ms/step 127251tok/s (total 123,043,840 tok), 28.41% MFU\n",
      "step 7520: loss 4.52 129.60ms/step 126419tok/s (total 123,207,680 tok), 28.22% MFU\n",
      "step 7530: loss 4.58 132.88ms/step 123302tok/s (total 123,371,520 tok), 27.53% MFU\n",
      "step 7540: loss 4.51 130.25ms/step 125791tok/s (total 123,535,360 tok), 28.08% MFU\n",
      "step 7550: loss 4.52 131.80ms/step 124306tok/s (total 123,699,200 tok), 27.75% MFU\n",
      "step 7550 eval: val_loss 4.49\n",
      "step 7560: loss 4.50 128.81ms/step 127194tok/s (total 123,863,040 tok), 28.40% MFU\n",
      "step 7570: loss 4.45 129.68ms/step 126346tok/s (total 124,026,880 tok), 28.21% MFU\n",
      "step 7580: loss 4.46 133.18ms/step 123022tok/s (total 124,190,720 tok), 27.46% MFU\n",
      "step 7590: loss 4.46 128.61ms/step 127392tok/s (total 124,354,560 tok), 28.44% MFU\n",
      "step 7600: loss 4.51 130.85ms/step 125209tok/s (total 124,518,400 tok), 27.95% MFU\n",
      "step 7600 eval: val_loss 4.49\n",
      "step 7610: loss 4.51 136.54ms/step 119995tok/s (total 124,682,240 tok), 26.79% MFU\n",
      "step 7620: loss 4.57 131.39ms/step 124701tok/s (total 124,846,080 tok), 27.84% MFU\n",
      "step 7630: loss 4.44 131.43ms/step 124663tok/s (total 125,009,920 tok), 27.83% MFU\n",
      "step 7640: loss 4.50 132.22ms/step 123917tok/s (total 125,173,760 tok), 27.66% MFU\n",
      "step 7650: loss 4.48 132.60ms/step 123561tok/s (total 125,337,600 tok), 27.59% MFU\n",
      "step 7650 eval: val_loss 4.51\n",
      "step 7660: loss 4.46 132.31ms/step 123833tok/s (total 125,501,440 tok), 27.65% MFU\n",
      "step 7670: loss 4.58 131.58ms/step 124516tok/s (total 125,665,280 tok), 27.80% MFU\n",
      "step 7680: loss 4.47 133.27ms/step 122934tok/s (total 125,829,120 tok), 27.45% MFU\n",
      "step 7690: loss 4.56 131.37ms/step 124717tok/s (total 125,992,960 tok), 27.84% MFU\n",
      "step 7700: loss 4.42 131.48ms/step 124610tok/s (total 126,156,800 tok), 27.82% MFU\n",
      "step 7700 eval: val_loss 4.50\n",
      "step 7710: loss 4.49 130.55ms/step 125501tok/s (total 126,320,640 tok), 28.02% MFU\n",
      "step 7720: loss 4.51 132.07ms/step 124058tok/s (total 126,484,480 tok), 27.70% MFU\n",
      "step 7730: loss 4.56 132.86ms/step 123316tok/s (total 126,648,320 tok), 27.53% MFU\n",
      "step 7740: loss 4.50 131.77ms/step 124339tok/s (total 126,812,160 tok), 27.76% MFU\n",
      "step 7750: loss 4.52 129.89ms/step 126140tok/s (total 126,976,000 tok), 28.16% MFU\n",
      "step 7750 eval: val_loss 4.50\n",
      "step 7760: loss 4.53 128.21ms/step 127787tok/s (total 127,139,840 tok), 28.53% MFU\n",
      "step 7770: loss 4.43 129.51ms/step 126507tok/s (total 127,303,680 tok), 28.24% MFU\n",
      "step 7780: loss 4.48 131.99ms/step 124127tok/s (total 127,467,520 tok), 27.71% MFU\n",
      "step 7790: loss 4.45 131.42ms/step 124672tok/s (total 127,631,360 tok), 27.83% MFU\n",
      "step 7800: loss 4.51 128.64ms/step 127367tok/s (total 127,795,200 tok), 28.44% MFU\n",
      "step 7800 eval: val_loss 4.47\n",
      "step 7810: loss 4.50 133.61ms/step 122629tok/s (total 127,959,040 tok), 27.38% MFU\n",
      "step 7820: loss 4.58 129.13ms/step 126880tok/s (total 128,122,880 tok), 28.33% MFU\n",
      "step 7830: loss 4.48 129.05ms/step 126954tok/s (total 128,286,720 tok), 28.34% MFU\n",
      "step 7840: loss 4.51 132.51ms/step 123646tok/s (total 128,450,560 tok), 27.60% MFU\n",
      "step 7850: loss 4.50 131.74ms/step 124363tok/s (total 128,614,400 tok), 27.76% MFU\n",
      "step 7850 eval: val_loss 4.49\n",
      "step 7860: loss 4.47 132.44ms/step 123709tok/s (total 128,778,240 tok), 27.62% MFU\n",
      "step 7870: loss 4.54 131.92ms/step 124193tok/s (total 128,942,080 tok), 27.73% MFU\n",
      "step 7880: loss 4.49 129.52ms/step 126495tok/s (total 129,105,920 tok), 28.24% MFU\n",
      "step 7890: loss 4.56 131.24ms/step 124844tok/s (total 129,269,760 tok), 27.87% MFU\n",
      "step 7900: loss 4.48 132.41ms/step 123740tok/s (total 129,433,600 tok), 27.63% MFU\n",
      "step 7900 eval: val_loss 4.48\n",
      "step 7910: loss 4.47 132.58ms/step 123583tok/s (total 129,597,440 tok), 27.59% MFU\n",
      "step 7920: loss 4.53 132.03ms/step 124092tok/s (total 129,761,280 tok), 27.70% MFU\n",
      "step 7930: loss 4.49 134.66ms/step 121666tok/s (total 129,925,120 tok), 27.16% MFU\n",
      "step 7940: loss 4.51 131.78ms/step 124327tok/s (total 130,088,960 tok), 27.76% MFU\n",
      "step 7950: loss 4.48 130.38ms/step 125664tok/s (total 130,252,800 tok), 28.05% MFU\n",
      "step 7950 eval: val_loss 4.49\n",
      "step 7960: loss 4.43 133.35ms/step 122868tok/s (total 130,416,640 tok), 27.43% MFU\n",
      "step 7970: loss 4.47 133.66ms/step 122580tok/s (total 130,580,480 tok), 27.37% MFU\n",
      "step 7980: loss 4.55 129.62ms/step 126397tok/s (total 130,744,320 tok), 28.22% MFU\n",
      "step 7990: loss 4.43 132.63ms/step 123528tok/s (total 130,908,160 tok), 27.58% MFU\n",
      "step 8000: loss 4.40 128.91ms/step 127094tok/s (total 131,072,000 tok), 28.37% MFU\n",
      "step 8000 eval: val_loss 4.48\n",
      "step 8010: loss 4.46 129.65ms/step 126374tok/s (total 131,235,840 tok), 28.21% MFU\n",
      "step 8020: loss 4.48 129.13ms/step 126879tok/s (total 131,399,680 tok), 28.33% MFU\n",
      "step 8030: loss 4.54 128.92ms/step 127087tok/s (total 131,563,520 tok), 28.37% MFU\n",
      "step 8040: loss 4.49 131.01ms/step 125056tok/s (total 131,727,360 tok), 27.92% MFU\n",
      "step 8050: loss 4.43 130.78ms/step 125280tok/s (total 131,891,200 tok), 27.97% MFU\n",
      "step 8050 eval: val_loss 4.41\n",
      "step 8060: loss 4.42 130.75ms/step 125312tok/s (total 132,055,040 tok), 27.98% MFU\n",
      "step 8070: loss 4.51 129.67ms/step 126347tok/s (total 132,218,880 tok), 28.21% MFU\n",
      "step 8080: loss 4.37 132.62ms/step 123545tok/s (total 132,382,720 tok), 27.58% MFU\n",
      "step 8090: loss 4.45 130.55ms/step 125504tok/s (total 132,546,560 tok), 28.02% MFU\n",
      "step 8100: loss 4.48 129.72ms/step 126303tok/s (total 132,710,400 tok), 28.20% MFU\n",
      "step 8100 eval: val_loss 4.58\n",
      "step 8110: loss 4.51 132.01ms/step 124113tok/s (total 132,874,240 tok), 27.71% MFU\n",
      "step 8120: loss 4.41 132.88ms/step 123297tok/s (total 133,038,080 tok), 27.53% MFU\n",
      "step 8130: loss 4.43 133.21ms/step 122989tok/s (total 133,201,920 tok), 27.46% MFU\n",
      "step 8140: loss 4.50 132.82ms/step 123353tok/s (total 133,365,760 tok), 27.54% MFU\n",
      "step 8150: loss 4.49 135.81ms/step 120638tok/s (total 133,529,600 tok), 26.93% MFU\n",
      "step 8150 eval: val_loss 4.50\n",
      "step 8160: loss 4.54 136.65ms/step 119899tok/s (total 133,693,440 tok), 26.77% MFU\n",
      "step 8170: loss 4.46 136.31ms/step 120200tok/s (total 133,857,280 tok), 26.83% MFU\n",
      "step 8180: loss 4.50 133.91ms/step 122353tok/s (total 134,021,120 tok), 27.32% MFU\n",
      "step 8190: loss 4.51 132.25ms/step 123887tok/s (total 134,184,960 tok), 27.66% MFU\n",
      "step 8200: loss 4.53 130.35ms/step 125692tok/s (total 134,348,800 tok), 28.06% MFU\n",
      "step 8200 eval: val_loss 4.45\n",
      "step 8210: loss 4.52 129.55ms/step 126464tok/s (total 134,512,640 tok), 28.23% MFU\n",
      "step 8220: loss 4.44 131.98ms/step 124144tok/s (total 134,676,480 tok), 27.72% MFU\n",
      "step 8230: loss 4.53 134.17ms/step 122114tok/s (total 134,840,320 tok), 27.26% MFU\n",
      "step 8240: loss 4.41 136.49ms/step 120035tok/s (total 135,004,160 tok), 26.80% MFU\n",
      "step 8250: loss 4.44 128.84ms/step 127168tok/s (total 135,168,000 tok), 28.39% MFU\n",
      "step 8250 eval: val_loss 4.46\n",
      "step 8260: loss 4.47 130.99ms/step 125081tok/s (total 135,331,840 tok), 27.92% MFU\n",
      "step 8270: loss 4.45 130.82ms/step 125236tok/s (total 135,495,680 tok), 27.96% MFU\n",
      "step 8280: loss 4.49 131.34ms/step 124745tok/s (total 135,659,520 tok), 27.85% MFU\n",
      "step 8290: loss 4.43 132.77ms/step 123399tok/s (total 135,823,360 tok), 27.55% MFU\n",
      "step 8300: loss 4.45 132.49ms/step 123658tok/s (total 135,987,200 tok), 27.61% MFU\n",
      "step 8300 eval: val_loss 4.53\n",
      "step 8310: loss 4.55 133.85ms/step 122404tok/s (total 136,151,040 tok), 27.33% MFU\n",
      "step 8320: loss 4.56 131.59ms/step 124508tok/s (total 136,314,880 tok), 27.80% MFU\n",
      "step 8330: loss 4.55 135.80ms/step 120650tok/s (total 136,478,720 tok), 26.94% MFU\n",
      "step 8340: loss 4.48 129.32ms/step 126689tok/s (total 136,642,560 tok), 28.28% MFU\n",
      "step 8350: loss 4.43 132.80ms/step 123373tok/s (total 136,806,400 tok), 27.54% MFU\n",
      "step 8350 eval: val_loss 4.50\n",
      "step 8360: loss 4.40 130.47ms/step 125572tok/s (total 136,970,240 tok), 28.03% MFU\n",
      "step 8370: loss 4.42 128.21ms/step 127787tok/s (total 137,134,080 tok), 28.53% MFU\n",
      "step 8380: loss 4.52 129.47ms/step 126543tok/s (total 137,297,920 tok), 28.25% MFU\n",
      "step 8390: loss 4.44 130.45ms/step 125594tok/s (total 137,461,760 tok), 28.04% MFU\n",
      "step 8400: loss 4.58 130.35ms/step 125695tok/s (total 137,625,600 tok), 28.06% MFU\n",
      "step 8400 eval: val_loss 4.51\n",
      "step 8410: loss 4.50 129.69ms/step 126327tok/s (total 137,789,440 tok), 28.20% MFU\n",
      "step 8420: loss 4.45 132.89ms/step 123294tok/s (total 137,953,280 tok), 27.53% MFU\n",
      "step 8430: loss 4.44 132.43ms/step 123721tok/s (total 138,117,120 tok), 27.62% MFU\n",
      "step 8440: loss 4.40 135.34ms/step 121060tok/s (total 138,280,960 tok), 27.03% MFU\n",
      "step 8450: loss 4.39 134.62ms/step 121706tok/s (total 138,444,800 tok), 27.17% MFU\n",
      "step 8450 eval: val_loss 4.53\n",
      "step 8460: loss 4.44 134.36ms/step 121940tok/s (total 138,608,640 tok), 27.22% MFU\n",
      "step 8470: loss 4.48 131.53ms/step 124561tok/s (total 138,772,480 tok), 27.81% MFU\n",
      "step 8480: loss 4.45 136.61ms/step 119937tok/s (total 138,936,320 tok), 26.78% MFU\n",
      "step 8490: loss 4.49 133.39ms/step 122825tok/s (total 139,100,160 tok), 27.42% MFU\n",
      "step 8500: loss 4.43 135.70ms/step 120733tok/s (total 139,264,000 tok), 26.95% MFU\n",
      "step 8500 eval: val_loss 4.49\n",
      "step 8510: loss 4.53 129.43ms/step 126581tok/s (total 139,427,840 tok), 28.26% MFU\n",
      "step 8520: loss 4.44 131.36ms/step 124726tok/s (total 139,591,680 tok), 27.85% MFU\n",
      "step 8530: loss 4.49 132.78ms/step 123395tok/s (total 139,755,520 tok), 27.55% MFU\n",
      "step 8540: loss 4.51 132.77ms/step 123403tok/s (total 139,919,360 tok), 27.55% MFU\n",
      "step 8550: loss 4.48 129.28ms/step 126732tok/s (total 140,083,200 tok), 28.29% MFU\n",
      "step 8550 eval: val_loss 4.46\n",
      "step 8560: loss 4.49 132.12ms/step 124009tok/s (total 140,247,040 tok), 27.69% MFU\n",
      "step 8570: loss 4.45 134.59ms/step 121735tok/s (total 140,410,880 tok), 27.18% MFU\n",
      "step 8580: loss 4.51 131.67ms/step 124435tok/s (total 140,574,720 tok), 27.78% MFU\n",
      "step 8590: loss 4.48 128.66ms/step 127342tok/s (total 140,738,560 tok), 28.43% MFU\n",
      "step 8600: loss 4.49 129.25ms/step 126765tok/s (total 140,902,400 tok), 28.30% MFU\n",
      "step 8600 eval: val_loss 4.51\n",
      "step 8610: loss 4.47 132.68ms/step 123488tok/s (total 141,066,240 tok), 27.57% MFU\n",
      "step 8620: loss 4.45 131.23ms/step 124848tok/s (total 141,230,080 tok), 27.87% MFU\n",
      "step 8630: loss 4.53 131.42ms/step 124673tok/s (total 141,393,920 tok), 27.83% MFU\n",
      "step 8640: loss 4.40 129.40ms/step 126614tok/s (total 141,557,760 tok), 28.27% MFU\n",
      "step 8650: loss 4.42 130.81ms/step 125249tok/s (total 141,721,600 tok), 27.96% MFU\n",
      "step 8650 eval: val_loss 4.45\n",
      "step 8660: loss 4.48 133.57ms/step 122664tok/s (total 141,885,440 tok), 27.38% MFU\n",
      "step 8670: loss 4.48 129.58ms/step 126440tok/s (total 142,049,280 tok), 28.23% MFU\n",
      "step 8680: loss 4.39 133.98ms/step 122288tok/s (total 142,213,120 tok), 27.30% MFU\n",
      "step 8690: loss 4.48 131.24ms/step 124842tok/s (total 142,376,960 tok), 27.87% MFU\n",
      "step 8700: loss 4.46 129.73ms/step 126294tok/s (total 142,540,800 tok), 28.20% MFU\n",
      "step 8700 eval: val_loss 4.48\n",
      "step 8710: loss 4.42 130.71ms/step 125345tok/s (total 142,704,640 tok), 27.98% MFU\n",
      "step 8720: loss 4.51 130.60ms/step 125451tok/s (total 142,868,480 tok), 28.01% MFU\n",
      "step 8730: loss 4.47 131.49ms/step 124598tok/s (total 143,032,320 tok), 27.82% MFU\n",
      "step 8740: loss 4.37 134.42ms/step 121889tok/s (total 143,196,160 tok), 27.21% MFU\n",
      "step 8750: loss 4.47 131.43ms/step 124659tok/s (total 143,360,000 tok), 27.83% MFU\n",
      "step 8750 eval: val_loss 4.42\n",
      "step 8760: loss 4.43 134.03ms/step 122246tok/s (total 143,523,840 tok), 27.29% MFU\n",
      "step 8770: loss 4.54 132.18ms/step 123950tok/s (total 143,687,680 tok), 27.67% MFU\n",
      "step 8780: loss 4.46 132.79ms/step 123387tok/s (total 143,851,520 tok), 27.55% MFU\n",
      "step 8790: loss 4.37 129.33ms/step 126687tok/s (total 144,015,360 tok), 28.28% MFU\n",
      "step 8800: loss 4.41 133.84ms/step 122415tok/s (total 144,179,200 tok), 27.33% MFU\n",
      "step 8800 eval: val_loss 4.47\n",
      "step 8810: loss 4.46 132.49ms/step 123659tok/s (total 144,343,040 tok), 27.61% MFU\n",
      "step 8820: loss 4.50 129.74ms/step 126286tok/s (total 144,506,880 tok), 28.19% MFU\n",
      "step 8830: loss 4.47 129.43ms/step 126581tok/s (total 144,670,720 tok), 28.26% MFU\n",
      "step 8840: loss 4.43 130.84ms/step 125218tok/s (total 144,834,560 tok), 27.96% MFU\n",
      "step 8850: loss 4.48 131.36ms/step 124727tok/s (total 144,998,400 tok), 27.85% MFU\n",
      "step 8850 eval: val_loss 4.44\n",
      "step 8860: loss 4.46 130.12ms/step 125917tok/s (total 145,162,240 tok), 28.11% MFU\n",
      "step 8870: loss 4.41 131.38ms/step 124704tok/s (total 145,326,080 tok), 27.84% MFU\n",
      "step 8880: loss 4.48 129.42ms/step 126594tok/s (total 145,489,920 tok), 28.26% MFU\n",
      "step 8890: loss 4.43 132.91ms/step 123267tok/s (total 145,653,760 tok), 27.52% MFU\n",
      "step 8900: loss 4.47 136.32ms/step 120188tok/s (total 145,817,600 tok), 26.83% MFU\n",
      "step 8900 eval: val_loss 4.38\n",
      "step 8910: loss 4.43 134.34ms/step 121959tok/s (total 145,981,440 tok), 27.23% MFU\n",
      "step 8920: loss 4.53 132.19ms/step 123941tok/s (total 146,145,280 tok), 27.67% MFU\n",
      "step 8930: loss 4.38 130.00ms/step 126028tok/s (total 146,309,120 tok), 28.14% MFU\n",
      "step 8940: loss 4.46 129.54ms/step 126475tok/s (total 146,472,960 tok), 28.24% MFU\n",
      "step 8950: loss 4.56 131.69ms/step 124415tok/s (total 146,636,800 tok), 27.78% MFU\n",
      "step 8950 eval: val_loss 4.51\n",
      "step 8960: loss 4.45 134.83ms/step 121513tok/s (total 146,800,640 tok), 27.13% MFU\n",
      "step 8970: loss 4.56 131.37ms/step 124716tok/s (total 146,964,480 tok), 27.84% MFU\n",
      "step 8980: loss 4.43 132.71ms/step 123461tok/s (total 147,128,320 tok), 27.56% MFU\n",
      "step 8990: loss 4.43 130.76ms/step 125298tok/s (total 147,292,160 tok), 27.97% MFU\n",
      "step 9000: loss 4.44 132.52ms/step 123632tok/s (total 147,456,000 tok), 27.60% MFU\n",
      "step 9000 eval: val_loss 4.38\n",
      "step 9010: loss 4.42 132.92ms/step 123258tok/s (total 147,619,840 tok), 27.52% MFU\n",
      "step 9020: loss 4.47 130.23ms/step 125805tok/s (total 147,783,680 tok), 28.09% MFU\n",
      "step 9030: loss 4.43 130.02ms/step 126010tok/s (total 147,947,520 tok), 28.13% MFU\n",
      "step 9040: loss 4.45 135.76ms/step 120685tok/s (total 148,111,360 tok), 26.94% MFU\n",
      "step 9050: loss 4.46 136.03ms/step 120441tok/s (total 148,275,200 tok), 26.89% MFU\n",
      "step 9050 eval: val_loss 4.36\n",
      "step 9060: loss 4.45 132.08ms/step 124047tok/s (total 148,439,040 tok), 27.69% MFU\n",
      "step 9070: loss 4.39 136.13ms/step 120355tok/s (total 148,602,880 tok), 26.87% MFU\n",
      "step 9080: loss 4.49 132.08ms/step 124050tok/s (total 148,766,720 tok), 27.69% MFU\n",
      "step 9090: loss 4.43 128.79ms/step 127211tok/s (total 148,930,560 tok), 28.40% MFU\n",
      "step 9100: loss 4.40 130.91ms/step 125151tok/s (total 149,094,400 tok), 27.94% MFU\n",
      "step 9100 eval: val_loss 4.46\n",
      "step 9110: loss 4.46 132.91ms/step 123270tok/s (total 149,258,240 tok), 27.52% MFU\n",
      "step 9120: loss 4.46 132.32ms/step 123819tok/s (total 149,422,080 tok), 27.64% MFU\n",
      "step 9130: loss 4.51 132.48ms/step 123672tok/s (total 149,585,920 tok), 27.61% MFU\n",
      "step 9140: loss 4.43 132.60ms/step 123560tok/s (total 149,749,760 tok), 27.59% MFU\n",
      "step 9150: loss 4.43 132.31ms/step 123828tok/s (total 149,913,600 tok), 27.64% MFU\n",
      "step 9150 eval: val_loss 4.42\n",
      "step 9160: loss 4.42 130.73ms/step 125329tok/s (total 150,077,440 tok), 27.98% MFU\n",
      "step 9170: loss 4.43 131.52ms/step 124578tok/s (total 150,241,280 tok), 27.81% MFU\n",
      "step 9180: loss 4.45 129.10ms/step 126907tok/s (total 150,405,120 tok), 28.33% MFU\n",
      "step 9190: loss 4.47 133.71ms/step 122536tok/s (total 150,568,960 tok), 27.36% MFU\n",
      "step 9200: loss 4.48 134.63ms/step 121700tok/s (total 150,732,800 tok), 27.17% MFU\n",
      "step 9200 eval: val_loss 4.48\n",
      "step 9210: loss 4.50 131.46ms/step 124633tok/s (total 150,896,640 tok), 27.82% MFU\n",
      "step 9220: loss 4.37 132.36ms/step 123780tok/s (total 151,060,480 tok), 27.63% MFU\n",
      "step 9230: loss 4.44 132.73ms/step 123438tok/s (total 151,224,320 tok), 27.56% MFU\n",
      "step 9240: loss 4.45 132.15ms/step 123983tok/s (total 151,388,160 tok), 27.68% MFU\n",
      "step 9250: loss 4.45 132.60ms/step 123564tok/s (total 151,552,000 tok), 27.59% MFU\n",
      "step 9250 eval: val_loss 4.41\n",
      "step 9260: loss 4.45 130.74ms/step 125319tok/s (total 151,715,840 tok), 27.98% MFU\n",
      "step 9270: loss 4.43 134.42ms/step 121884tok/s (total 151,879,680 tok), 27.21% MFU\n",
      "step 9280: loss 4.46 135.56ms/step 120858tok/s (total 152,043,520 tok), 26.98% MFU\n",
      "step 9290: loss 4.35 135.40ms/step 121006tok/s (total 152,207,360 tok), 27.01% MFU\n",
      "step 9300: loss 4.44 131.14ms/step 124935tok/s (total 152,371,200 tok), 27.89% MFU\n",
      "step 9300 eval: val_loss 4.40\n",
      "step 9310: loss 4.52 132.90ms/step 123279tok/s (total 152,535,040 tok), 27.52% MFU\n",
      "step 9320: loss 4.47 133.24ms/step 122968tok/s (total 152,698,880 tok), 27.45% MFU\n",
      "step 9330: loss 4.40 132.26ms/step 123873tok/s (total 152,862,720 tok), 27.65% MFU\n",
      "step 9340: loss 4.47 130.42ms/step 125629tok/s (total 153,026,560 tok), 28.05% MFU\n",
      "step 9350: loss 4.43 129.57ms/step 126449tok/s (total 153,190,400 tok), 28.23% MFU\n",
      "step 9350 eval: val_loss 4.39\n",
      "step 9360: loss 4.43 131.50ms/step 124590tok/s (total 153,354,240 tok), 27.82% MFU\n",
      "step 9370: loss 4.43 132.04ms/step 124084tok/s (total 153,518,080 tok), 27.70% MFU\n",
      "step 9380: loss 4.45 128.45ms/step 127548tok/s (total 153,681,920 tok), 28.48% MFU\n",
      "step 9390: loss 4.43 130.62ms/step 125433tok/s (total 153,845,760 tok), 28.00% MFU\n",
      "step 9400: loss 4.47 132.39ms/step 123754tok/s (total 154,009,600 tok), 27.63% MFU\n",
      "step 9400 eval: val_loss 4.33\n",
      "step 9410: loss 4.40 131.24ms/step 124838tok/s (total 154,173,440 tok), 27.87% MFU\n",
      "step 9420: loss 4.39 131.94ms/step 124180tok/s (total 154,337,280 tok), 27.72% MFU\n",
      "step 9430: loss 4.44 132.81ms/step 123364tok/s (total 154,501,120 tok), 27.54% MFU\n",
      "step 9440: loss 4.40 132.43ms/step 123723tok/s (total 154,664,960 tok), 27.62% MFU\n",
      "step 9450: loss 4.39 131.24ms/step 124839tok/s (total 154,828,800 tok), 27.87% MFU\n",
      "step 9450 eval: val_loss 4.41\n",
      "step 9460: loss 4.46 135.32ms/step 121073tok/s (total 154,992,640 tok), 27.03% MFU\n",
      "step 9470: loss 4.35 132.65ms/step 123510tok/s (total 155,156,480 tok), 27.57% MFU\n",
      "step 9480: loss 4.42 132.53ms/step 123626tok/s (total 155,320,320 tok), 27.60% MFU\n",
      "step 9490: loss 4.45 129.88ms/step 126146tok/s (total 155,484,160 tok), 28.16% MFU\n",
      "step 9500: loss 4.39 132.83ms/step 123349tok/s (total 155,648,000 tok), 27.54% MFU\n",
      "step 9500 eval: val_loss 4.47\n",
      "step 9510: loss 4.42 133.53ms/step 122695tok/s (total 155,811,840 tok), 27.39% MFU\n",
      "step 9520: loss 4.39 130.45ms/step 125598tok/s (total 155,975,680 tok), 28.04% MFU\n",
      "step 9530: loss 4.45 133.66ms/step 122583tok/s (total 156,139,520 tok), 27.37% MFU\n",
      "step 9540: loss 4.44 131.56ms/step 124533tok/s (total 156,303,360 tok), 27.80% MFU\n",
      "step 9550: loss 4.48 133.18ms/step 123018tok/s (total 156,467,200 tok), 27.46% MFU\n",
      "step 9550 eval: val_loss 4.47\n",
      "step 9560: loss 4.36 133.88ms/step 122379tok/s (total 156,631,040 tok), 27.32% MFU\n",
      "step 9570: loss 4.42 135.16ms/step 121221tok/s (total 156,794,880 tok), 27.06% MFU\n",
      "step 9580: loss 4.40 133.98ms/step 122286tok/s (total 156,958,720 tok), 27.30% MFU\n",
      "step 9590: loss 4.51 133.78ms/step 122470tok/s (total 157,122,560 tok), 27.34% MFU\n",
      "step 9600: loss 4.48 131.32ms/step 124762tok/s (total 157,286,400 tok), 27.85% MFU\n",
      "step 9600 eval: val_loss 4.44\n",
      "step 9610: loss 4.46 133.51ms/step 122719tok/s (total 157,450,240 tok), 27.40% MFU\n",
      "step 9620: loss 4.44 129.74ms/step 126279tok/s (total 157,614,080 tok), 28.19% MFU\n",
      "step 9630: loss 4.38 129.61ms/step 126412tok/s (total 157,777,920 tok), 28.22% MFU\n",
      "step 9640: loss 4.44 130.44ms/step 125605tok/s (total 157,941,760 tok), 28.04% MFU\n",
      "step 9650: loss 4.44 129.86ms/step 126167tok/s (total 158,105,600 tok), 28.17% MFU\n",
      "step 9650 eval: val_loss 4.48\n",
      "step 9660: loss 4.36 129.76ms/step 126259tok/s (total 158,269,440 tok), 28.19% MFU\n",
      "step 9670: loss 4.40 133.73ms/step 122518tok/s (total 158,433,280 tok), 27.35% MFU\n",
      "step 9680: loss 4.43 131.44ms/step 124651tok/s (total 158,597,120 tok), 27.83% MFU\n",
      "step 9690: loss 4.47 131.66ms/step 124444tok/s (total 158,760,960 tok), 27.78% MFU\n",
      "step 9700: loss 4.44 131.03ms/step 125045tok/s (total 158,924,800 tok), 27.92% MFU\n",
      "step 9700 eval: val_loss 4.44\n",
      "step 9710: loss 4.41 132.64ms/step 123522tok/s (total 159,088,640 tok), 27.58% MFU\n",
      "step 9720: loss 4.38 131.01ms/step 125061tok/s (total 159,252,480 tok), 27.92% MFU\n",
      "step 9730: loss 4.41 132.16ms/step 123972tok/s (total 159,416,320 tok), 27.68% MFU\n",
      "step 9740: loss 4.41 132.72ms/step 123448tok/s (total 159,580,160 tok), 27.56% MFU\n",
      "step 9750: loss 4.40 131.93ms/step 124183tok/s (total 159,744,000 tok), 27.72% MFU\n",
      "step 9750 eval: val_loss 4.46\n",
      "step 9760: loss 4.39 130.69ms/step 125362tok/s (total 159,907,840 tok), 27.99% MFU\n",
      "step 9770: loss 4.48 132.06ms/step 124069tok/s (total 160,071,680 tok), 27.70% MFU\n",
      "step 9780: loss 4.38 135.19ms/step 121196tok/s (total 160,235,520 tok), 27.06% MFU\n",
      "step 9790: loss 4.43 132.94ms/step 123241tok/s (total 160,399,360 tok), 27.51% MFU\n",
      "step 9800: loss 4.43 129.06ms/step 126954tok/s (total 160,563,200 tok), 28.34% MFU\n",
      "step 9800 eval: val_loss 4.35\n",
      "step 9810: loss 4.38 130.61ms/step 125439tok/s (total 160,727,040 tok), 28.00% MFU\n",
      "step 9820: loss 4.48 132.61ms/step 123553tok/s (total 160,890,880 tok), 27.58% MFU\n",
      "step 9830: loss 4.55 131.00ms/step 125066tok/s (total 161,054,720 tok), 27.92% MFU\n",
      "step 9840: loss 4.39 133.81ms/step 122438tok/s (total 161,218,560 tok), 27.33% MFU\n",
      "step 9850: loss 4.39 130.72ms/step 125340tok/s (total 161,382,400 tok), 27.98% MFU\n",
      "step 9850 eval: val_loss 4.35\n",
      "step 9860: loss 4.40 134.37ms/step 121934tok/s (total 161,546,240 tok), 27.22% MFU\n",
      "step 9870: loss 4.43 132.79ms/step 123382tok/s (total 161,710,080 tok), 27.55% MFU\n",
      "step 9880: loss 4.41 129.00ms/step 127009tok/s (total 161,873,920 tok), 28.36% MFU\n",
      "step 9890: loss 4.39 130.17ms/step 125869tok/s (total 162,037,760 tok), 28.10% MFU\n",
      "step 9900: loss 4.45 130.00ms/step 126034tok/s (total 162,201,600 tok), 28.14% MFU\n",
      "step 9900 eval: val_loss 4.46\n",
      "step 9910: loss 4.46 130.98ms/step 125088tok/s (total 162,365,440 tok), 27.93% MFU\n",
      "step 9920: loss 4.42 130.92ms/step 125149tok/s (total 162,529,280 tok), 27.94% MFU\n",
      "step 9930: loss 4.42 130.83ms/step 125230tok/s (total 162,693,120 tok), 27.96% MFU\n",
      "step 9940: loss 4.43 129.27ms/step 126743tok/s (total 162,856,960 tok), 28.30% MFU\n",
      "step 9950: loss 4.44 131.70ms/step 124408tok/s (total 163,020,800 tok), 27.77% MFU\n",
      "step 9950 eval: val_loss 4.34\n",
      "step 9960: loss 4.43 135.18ms/step 121197tok/s (total 163,184,640 tok), 27.06% MFU\n",
      "step 9970: loss 4.42 132.16ms/step 123970tok/s (total 163,348,480 tok), 27.68% MFU\n",
      "step 9980: loss 4.38 133.05ms/step 123145tok/s (total 163,512,320 tok), 27.49% MFU\n",
      "step 9990: loss 4.46 134.91ms/step 121444tok/s (total 163,676,160 tok), 27.11% MFU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "for curr_step in range(1, 10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "\n",
    "    # evaluate the loss\n",
    "    step_start = time.time()\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "    \n",
    "    if curr_step % log_interval == 0:\n",
    "        mfu = model.calculate_mfu(step_time)\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):.0f}tok/s (total {tok_total:,} tok), {mfu * 100:.2f}% MFU\")\n",
    "\n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(encode(\"hello my name is\").to(device).unsqueeze(0), 10)\n",
    "        #     print(decode(output))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef249492-a2b5-4b57-a6d1-0e48db4fede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 3, where the maximum emcnnacs had recovered.\n",
      "The researchers added that a clay word \"As we demonstrated, \"SOUNDSERV AND TASTERAT HAASCH,\" which uses letters generated to help study this function. 1\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"2 + 2 =\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a1f7bec-2722-4ae0-8226-98e3c61722f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This clock dot was a scroll because it literally became easy to observe today. A good idea is taking into account the charging path for ordinary architectural engagement from Signal. bicycling has necessitates a tool from the beginner to its ICRI’s Leica\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d7d318-bbf8-43ce-b781-93f5c7624df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health is a chronic problem, it can lead to stigma; it cannot be treated up to 34% among the American clients, if unintended partners are affected. With this new treat disorder, it may be an irreversible depression. Try to speak a daily touchful to\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"Health is\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
