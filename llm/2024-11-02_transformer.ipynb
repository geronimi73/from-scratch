{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b656c720-474c-4843-98be-51a17f86d322",
   "metadata": {},
   "source": [
    "* fineweb-edu\n",
    "* mfu calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7d9c4a-630c-40e0-94e4-86cbc08c0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 952.68it/s]\n",
      "100%|███████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1494.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import torch \n",
    "import datasets\n",
    "import random\n",
    "\n",
    "dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\"], split=\"train\")\n",
    "dataset = dataset.train_test_split()\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "def encode(string):\n",
    "    return torch.tensor(enc.encode(string), dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return enc.decode(tensor.cpu().squeeze().numpy())\n",
    "\n",
    "num_samples = 1_000\n",
    "dataset_tok_train = torch.cat([encode(dataset[\"train\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "dataset_tok_test = torch.cat([encode(dataset[\"test\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "\n",
    "def get_sample(split, sample_length, batch_size):\n",
    "    tokens = dataset_tok_train if split == \"train\" else dataset_tok_test\n",
    "    idcs = torch.randint(len(tokens)-sample_length, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
    "    y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869a87db-8bba-4726-aaea-0ce5603f8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 103,013,759 tokens\n",
      "Test data: 103,356,615 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data: {len(dataset_tok_train):,} tokens\")\n",
    "print(f\"Test data: {len(dataset_tok_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d5a8b7d-7ece-41bc-89fe-443d5f5d1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        # out = F.scaled_dot_product_attention(q, k, v)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def calculate_mfu(self, dt):\n",
    "        flops_achieved = flops_per_fwdbwd / dt\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        # print(f\"{flops_achieved/10**12} TFLOPS achieved, {mfu * 100:.2f}% MFU\")\n",
    "        return mfu      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b100d83e-dba3-43fd-9386-1fc1d9eacf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.35M parameter model on device mps with 4.1 TFLOPS promised \n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_272\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "block_size = 128 \n",
    "learning_rate = 3e-4\n",
    "\n",
    "device = \"mps\"\n",
    "flops_promised = 4.1 * 10**12\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(f\"{sum(p.numel() for p in m.parameters())/1e6:.2f}M parameter model on device {device} with {flops_promised / 10**12:,} TFLOPS promised \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28b8522f-1db1-43ad-8aa6-55a0103ce704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_52157/1395468099.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_52157/1395468099.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28,991,029,248 flops per fwd+bwd pass\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "def get_flops(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    return flop_counter.get_total_flops() \n",
    "\n",
    "def train_one_sample():\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "flops_per_fwdbwd = get_flops(train_one_sample)\n",
    "print(f\"{flops_per_fwdbwd:,} flops per fwd+bwd pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b09d994-9d8f-4a2a-8fa6-c2cf0fcb04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_52157/1395468099.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_52157/1395468099.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss nan 842.52ms/step 9,723tok/s (total 81,920 tok), 0.84% MFU\n",
      "step 20: loss nan 872.88ms/step 9,385tok/s (total 163,840 tok), 0.81% MFU\n",
      "step 30: loss nan 830.53ms/step 9,864tok/s (total 245,760 tok), 0.85% MFU\n",
      "step 40: loss nan 831.83ms/step 9,848tok/s (total 327,680 tok), 0.85% MFU\n",
      "step 50: loss nan 836.87ms/step 9,789tok/s (total 409,600 tok), 0.84% MFU\n",
      "step 50 eval: val_loss nan\n",
      "step 60: loss nan 830.30ms/step 9,866tok/s (total 491,520 tok), 0.85% MFU\n",
      "step 70: loss nan 840.94ms/step 9,741tok/s (total 573,440 tok), 0.84% MFU\n",
      "step 80: loss nan 841.42ms/step 9,736tok/s (total 655,360 tok), 0.84% MFU\n",
      "step 90: loss nan 848.08ms/step 9,659tok/s (total 737,280 tok), 0.83% MFU\n",
      "step 100: loss nan 832.71ms/step 9,838tok/s (total 819,200 tok), 0.85% MFU\n",
      "step 100 eval: val_loss nan\n",
      "step 110: loss nan 850.20ms/step 9,635tok/s (total 901,120 tok), 0.83% MFU\n",
      "step 120: loss nan 838.70ms/step 9,768tok/s (total 983,040 tok), 0.84% MFU\n",
      "step 130: loss nan 873.87ms/step 9,374tok/s (total 1,064,960 tok), 0.81% MFU\n",
      "step 140: loss nan 843.30ms/step 9,714tok/s (total 1,146,880 tok), 0.84% MFU\n",
      "step 150: loss nan 847.57ms/step 9,665tok/s (total 1,228,800 tok), 0.83% MFU\n",
      "step 150 eval: val_loss nan\n",
      "step 160: loss nan 865.23ms/step 9,468tok/s (total 1,310,720 tok), 0.82% MFU\n",
      "step 170: loss nan 854.25ms/step 9,590tok/s (total 1,392,640 tok), 0.83% MFU\n",
      "step 180: loss nan 836.26ms/step 9,796tok/s (total 1,474,560 tok), 0.85% MFU\n",
      "step 190: loss nan 839.00ms/step 9,764tok/s (total 1,556,480 tok), 0.84% MFU\n",
      "step 200: loss nan 836.66ms/step 9,791tok/s (total 1,638,400 tok), 0.85% MFU\n",
      "step 200 eval: val_loss nan\n",
      "step 210: loss nan 835.88ms/step 9,800tok/s (total 1,720,320 tok), 0.85% MFU\n",
      "step 220: loss nan 850.27ms/step 9,635tok/s (total 1,802,240 tok), 0.83% MFU\n",
      "step 230: loss nan 833.41ms/step 9,830tok/s (total 1,884,160 tok), 0.85% MFU\n",
      "step 240: loss nan 840.74ms/step 9,744tok/s (total 1,966,080 tok), 0.84% MFU\n",
      "step 250: loss nan 850.18ms/step 9,636tok/s (total 2,048,000 tok), 0.83% MFU\n",
      "step 250 eval: val_loss nan\n",
      "step 260: loss nan 834.25ms/step 9,820tok/s (total 2,129,920 tok), 0.85% MFU\n",
      "step 270: loss nan 834.18ms/step 9,820tok/s (total 2,211,840 tok), 0.85% MFU\n",
      "step 280: loss nan 840.45ms/step 9,747tok/s (total 2,293,760 tok), 0.84% MFU\n",
      "step 290: loss nan 833.01ms/step 9,834tok/s (total 2,375,680 tok), 0.85% MFU\n",
      "step 300: loss nan 837.92ms/step 9,777tok/s (total 2,457,600 tok), 0.84% MFU\n",
      "step 300 eval: val_loss nan\n",
      "step 310: loss nan 836.53ms/step 9,793tok/s (total 2,539,520 tok), 0.85% MFU\n",
      "step 320: loss nan 832.65ms/step 9,838tok/s (total 2,621,440 tok), 0.85% MFU\n",
      "step 330: loss nan 832.18ms/step 9,844tok/s (total 2,703,360 tok), 0.85% MFU\n",
      "step 340: loss nan 852.14ms/step 9,613tok/s (total 2,785,280 tok), 0.83% MFU\n",
      "step 350: loss nan 846.25ms/step 9,680tok/s (total 2,867,200 tok), 0.84% MFU\n",
      "step 350 eval: val_loss nan\n",
      "step 360: loss nan 830.21ms/step 9,867tok/s (total 2,949,120 tok), 0.85% MFU\n",
      "step 370: loss nan 838.21ms/step 9,773tok/s (total 3,031,040 tok), 0.84% MFU\n",
      "step 380: loss nan 832.83ms/step 9,836tok/s (total 3,112,960 tok), 0.85% MFU\n",
      "step 390: loss nan 841.35ms/step 9,737tok/s (total 3,194,880 tok), 0.84% MFU\n",
      "step 400: loss nan 839.76ms/step 9,755tok/s (total 3,276,800 tok), 0.84% MFU\n",
      "step 400 eval: val_loss nan\n",
      "step 410: loss nan 869.91ms/step 9,417tok/s (total 3,358,720 tok), 0.81% MFU\n",
      "step 420: loss nan 832.95ms/step 9,835tok/s (total 3,440,640 tok), 0.85% MFU\n",
      "step 430: loss nan 844.93ms/step 9,696tok/s (total 3,522,560 tok), 0.84% MFU\n",
      "step 440: loss nan 834.99ms/step 9,811tok/s (total 3,604,480 tok), 0.85% MFU\n",
      "step 450: loss nan 831.43ms/step 9,853tok/s (total 3,686,400 tok), 0.85% MFU\n",
      "step 450 eval: val_loss nan\n",
      "step 460: loss nan 832.71ms/step 9,838tok/s (total 3,768,320 tok), 0.85% MFU\n",
      "step 470: loss nan 843.73ms/step 9,709tok/s (total 3,850,240 tok), 0.84% MFU\n",
      "step 480: loss nan 865.94ms/step 9,460tok/s (total 3,932,160 tok), 0.82% MFU\n",
      "step 490: loss nan 847.63ms/step 9,665tok/s (total 4,014,080 tok), 0.83% MFU\n",
      "step 500: loss nan 857.14ms/step 9,557tok/s (total 4,096,000 tok), 0.82% MFU\n",
      "step 500 eval: val_loss nan\n",
      "step 510: loss nan 866.48ms/step 9,454tok/s (total 4,177,920 tok), 0.82% MFU\n",
      "step 520: loss nan 859.33ms/step 9,533tok/s (total 4,259,840 tok), 0.82% MFU\n",
      "step 530: loss nan 839.30ms/step 9,761tok/s (total 4,341,760 tok), 0.84% MFU\n",
      "step 540: loss nan 839.95ms/step 9,753tok/s (total 4,423,680 tok), 0.84% MFU\n",
      "step 550: loss nan 848.54ms/step 9,654tok/s (total 4,505,600 tok), 0.83% MFU\n",
      "step 550 eval: val_loss nan\n",
      "step 560: loss nan 862.63ms/step 9,497tok/s (total 4,587,520 tok), 0.82% MFU\n",
      "step 570: loss nan 878.88ms/step 9,321tok/s (total 4,669,440 tok), 0.80% MFU\n",
      "step 580: loss nan 917.31ms/step 8,930tok/s (total 4,751,360 tok), 0.77% MFU\n",
      "step 590: loss nan 938.81ms/step 8,726tok/s (total 4,833,280 tok), 0.75% MFU\n",
      "step 600: loss nan 935.20ms/step 8,760tok/s (total 4,915,200 tok), 0.76% MFU\n",
      "step 600 eval: val_loss nan\n",
      "step 610: loss nan 928.55ms/step 8,822tok/s (total 4,997,120 tok), 0.76% MFU\n",
      "step 620: loss nan 912.12ms/step 8,981tok/s (total 5,079,040 tok), 0.78% MFU\n",
      "step 630: loss nan 913.26ms/step 8,970tok/s (total 5,160,960 tok), 0.77% MFU\n",
      "step 640: loss nan 928.05ms/step 8,827tok/s (total 5,242,880 tok), 0.76% MFU\n",
      "step 650: loss nan 905.03ms/step 9,052tok/s (total 5,324,800 tok), 0.78% MFU\n",
      "step 650 eval: val_loss nan\n",
      "step 660: loss nan 974.70ms/step 8,405tok/s (total 5,406,720 tok), 0.73% MFU\n",
      "step 670: loss nan 1017.51ms/step 8,051tok/s (total 5,488,640 tok), 0.69% MFU\n",
      "step 680: loss nan 906.24ms/step 9,040tok/s (total 5,570,560 tok), 0.78% MFU\n",
      "step 690: loss nan 906.03ms/step 9,042tok/s (total 5,652,480 tok), 0.78% MFU\n",
      "step 700: loss nan 898.46ms/step 9,118tok/s (total 5,734,400 tok), 0.79% MFU\n",
      "step 700 eval: val_loss nan\n",
      "step 710: loss nan 910.78ms/step 8,995tok/s (total 5,816,320 tok), 0.78% MFU\n",
      "step 720: loss nan 908.26ms/step 9,019tok/s (total 5,898,240 tok), 0.78% MFU\n",
      "step 730: loss nan 905.59ms/step 9,046tok/s (total 5,980,160 tok), 0.78% MFU\n",
      "step 740: loss nan 881.36ms/step 9,295tok/s (total 6,062,080 tok), 0.80% MFU\n",
      "step 750: loss nan 880.47ms/step 9,304tok/s (total 6,144,000 tok), 0.80% MFU\n",
      "step 750 eval: val_loss nan\n",
      "step 760: loss nan 887.30ms/step 9,233tok/s (total 6,225,920 tok), 0.80% MFU\n",
      "step 770: loss nan 874.13ms/step 9,372tok/s (total 6,307,840 tok), 0.81% MFU\n",
      "step 780: loss nan 873.45ms/step 9,379tok/s (total 6,389,760 tok), 0.81% MFU\n",
      "step 790: loss nan 966.55ms/step 8,475tok/s (total 6,471,680 tok), 0.73% MFU\n",
      "step 800: loss nan 894.40ms/step 9,159tok/s (total 6,553,600 tok), 0.79% MFU\n",
      "step 800 eval: val_loss nan\n",
      "step 810: loss nan 907.88ms/step 9,023tok/s (total 6,635,520 tok), 0.78% MFU\n",
      "step 820: loss nan 937.43ms/step 8,739tok/s (total 6,717,440 tok), 0.75% MFU\n",
      "step 830: loss nan 926.07ms/step 8,846tok/s (total 6,799,360 tok), 0.76% MFU\n",
      "step 840: loss nan 917.58ms/step 8,928tok/s (total 6,881,280 tok), 0.77% MFU\n",
      "step 850: loss nan 947.97ms/step 8,642tok/s (total 6,963,200 tok), 0.75% MFU\n",
      "step 850 eval: val_loss nan\n",
      "step 860: loss nan 919.08ms/step 8,913tok/s (total 7,045,120 tok), 0.77% MFU\n",
      "step 870: loss nan 954.37ms/step 8,584tok/s (total 7,127,040 tok), 0.74% MFU\n",
      "step 880: loss nan 887.18ms/step 9,234tok/s (total 7,208,960 tok), 0.80% MFU\n",
      "step 890: loss nan 980.96ms/step 8,351tok/s (total 7,290,880 tok), 0.72% MFU\n",
      "step 900: loss nan 955.12ms/step 8,577tok/s (total 7,372,800 tok), 0.74% MFU\n",
      "step 900 eval: val_loss nan\n",
      "step 910: loss nan 970.49ms/step 8,441tok/s (total 7,454,720 tok), 0.73% MFU\n",
      "step 920: loss nan 973.45ms/step 8,415tok/s (total 7,536,640 tok), 0.73% MFU\n",
      "step 930: loss nan 940.83ms/step 8,707tok/s (total 7,618,560 tok), 0.75% MFU\n",
      "step 940: loss nan 1000.66ms/step 8,187tok/s (total 7,700,480 tok), 0.71% MFU\n",
      "step 950: loss nan 918.11ms/step 8,923tok/s (total 7,782,400 tok), 0.77% MFU\n",
      "step 950 eval: val_loss nan\n",
      "step 960: loss nan 918.43ms/step 8,920tok/s (total 7,864,320 tok), 0.77% MFU\n",
      "step 970: loss nan 950.01ms/step 8,623tok/s (total 7,946,240 tok), 0.74% MFU\n",
      "step 980: loss nan 909.60ms/step 9,006tok/s (total 8,028,160 tok), 0.78% MFU\n",
      "step 990: loss nan 1005.73ms/step 8,145tok/s (total 8,110,080 tok), 0.70% MFU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "for curr_step in range(1, 1000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "\n",
    "    # evaluate the loss\n",
    "    step_start = time.time()\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "    \n",
    "    if curr_step % log_interval == 0:\n",
    "        mfu = model.calculate_mfu(step_time)\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):,.0f}tok/s (total {tok_total:,} tok), {mfu * 100:.2f}% MFU\")\n",
    "\n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(encode(\"hello my name is\").to(device).unsqueeze(0), 10)\n",
    "        #     print(decode(output))\n",
    "        model.train()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b477c-97b4-48b4-834d-6f334ea078a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
