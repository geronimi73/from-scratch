{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bfe7b8f-e923-4722-ab8d-4665f052350d",
   "metadata": {},
   "source": [
    "* fineweb-edu\n",
    "* mfu calc\n",
    "* wandb logging\n",
    "* data MP\n",
    "* Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2633dbc-62ed-4b66-b60c-6eba3d106c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken tqdm datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b100d83e-dba3-43fd-9386-1fc1d9eacf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "\n",
    "vocab_size = 50_272\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_layer = 12\n",
    "dropout = 0.2\n",
    "batch_size = 4\n",
    "block_size = 1024 \n",
    "learning_rate = 3e-4\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# GPUs\n",
    "gpus = {\n",
    "    \"M3\": [4.1 * 10**12, \"mps\"],\n",
    "    \"4090\": [82 * 10**12, \"cuda\"],\n",
    "    \"3090\": [35.58 * 10**12, \"cuda\"],\n",
    "}\n",
    "gpu = \"4090\"\n",
    "flops_promised, device = gpus[gpu]\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# device = \"mps\"\n",
    "# flops_promised = 4.1 * 10**12 if device == \"mps\" else 82 * 10**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39809f7-6e5f-4f0b-893f-a2400583baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import torch \n",
    "import datasets\n",
    "import random\n",
    "\n",
    "# dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\",\"sample/10BT/001_00000.parquet\",\"sample/10BT/002_00000.parquet\"], split=\"train\")\n",
    "dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\"], split=\"train\")\n",
    "dataset = dataset.select(range(10_000)).train_test_split()\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "def encode(string):\n",
    "    return torch.tensor(enc.encode(string, disallowed_special=()), dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return enc.decode(tensor.cpu().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e7c24b-219c-40d0-8565-6461c4fd190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 7,670,763 tokens\n",
      "Test data: 2,631,259 tokens\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "dataset_tok_train = dataset[\"train\"].map(lambda row: {\"tok\": encode(row[\"text\"])}, num_proc = os.cpu_count()//2)\n",
    "dataset_tok_train.set_format(\"pt\", columns=[\"tok\"], output_all_columns=True) \n",
    "dataset_tok_train = torch.cat(dataset_tok_train[\"tok\"])\n",
    "\n",
    "dataset_tok_test = dataset[\"test\"].map(lambda row: {\"tok\": encode(row[\"text\"])}, num_proc = os.cpu_count()//2)\n",
    "dataset_tok_test.set_format(\"pt\", columns=[\"tok\"], output_all_columns=True) \n",
    "dataset_tok_test = torch.cat(dataset_tok_test[\"tok\"])\n",
    "\n",
    "def get_sample(split, sample_length, batch_size):\n",
    "    tokens = dataset_tok_train if split == \"train\" else dataset_tok_test\n",
    "    idcs = torch.randint(len(tokens)-sample_length, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
    "    y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n",
    "    return x, y\n",
    "\n",
    "print(f\"Train data: {len(dataset_tok_train):,} tokens\")\n",
    "print(f\"Test data: {len(dataset_tok_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5a8b7d-7ece-41bc-89fe-443d5f5d1ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.08M parameter model on devicecuda with 82.0 TFLOPS promised \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerald-stampfel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20241106_171444-ys3lx79c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gerald-stampfel/minimal-llm/runs/ys3lx79c' target=\"_blank\">GPT2-small-163.1M-7.7MT-4090-float32_matmul_precision-high</a></strong> to <a href='https://wandb.ai/gerald-stampfel/minimal-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gerald-stampfel/minimal-llm' target=\"_blank\">https://wandb.ai/gerald-stampfel/minimal-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gerald-stampfel/minimal-llm/runs/ys3lx79c' target=\"_blank\">https://wandb.ai/gerald-stampfel/minimal-llm/runs/ys3lx79c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No relevant files were detected in the specified directory. No code will be logged to your run.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=dropout)\n",
    "            \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def calculate_mfu(self, dt):\n",
    "        flops_achieved = flops_per_fwdbwd / dt\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        # print(f\"{flops_achieved/10**12} TFLOPS achieved, {mfu * 100:.2f}% MFU\")\n",
    "        return mfu      \n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "num_params = sum(p.numel() for p in m.parameters())/1e6\n",
    "print(f\"{num_params:.2f}M parameter model on device{device} with {flops_promised / 10**12:,} TFLOPS promised \" )\n",
    "\n",
    "import wandb \n",
    "wandb.init(\n",
    "    project=\"minimal-llm\",\n",
    "    name=f\"GPT2-small-{num_params:.1f}M-{len(dataset_tok_train)/10**6:,.1f}MT-{gpu}-float32_matmul_precision-high\"\n",
    ").log_code(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b8522f-1db1-43ad-8aa6-55a0103ce704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6984/1213494126.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_6984/1213494126.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,500,062,801,920 flops per fwd+bwd pass\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "def get_flops(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    return flop_counter.get_total_flops() \n",
    "\n",
    "def train_one_sample():\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "flops_per_fwdbwd = get_flops(train_one_sample)\n",
    "print(f\"{flops_per_fwdbwd:,} flops per fwd+bwd pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09d994-9d8f-4a2a-8fa6-c2cf0fcb04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6984/1213494126.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_6984/1213494126.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 8.41 130.47ms/step 31,395tok/s (total 40,960 tok), 32.72% MFU\n",
      "step 20: loss 7.94 130.84ms/step 31,305tok/s (total 81,920 tok), 32.62% MFU\n",
      "step 30: loss 7.92 130.79ms/step 31,316tok/s (total 122,880 tok), 32.63% MFU\n",
      "step 40: loss 7.71 129.78ms/step 31,562tok/s (total 163,840 tok), 32.89% MFU\n",
      "step 50: loss 7.87 129.79ms/step 31,558tok/s (total 204,800 tok), 32.89% MFU\n",
      "step 50 eval: val_loss 7.70\n",
      " SAMPLE OUTPUT #1: Why ass to environment\n",
      " and October Council13\n",
      " ofak a administrative, how, received\n",
      " while\n",
      " SAMPLE OUTPUT #2: Whymindstrings different than for\n",
      " its greater difficult I Hyp God Resources the of).Charges bed 18 sellers\n",
      " SAMPLE OUTPUT #3: Why): they:// focus book simple population and changing inOC on rein S. that , on Moving would\n",
      " SAMPLE OUTPUT #4: Why is increase and used or indoor. chief nem asJ felt Joseph Doc pink endbedulate- short\n",
      " SAMPLE OUTPUT #5: Why colon is without based same: tana adequate,343ka the Eag French sleeves made pressure for of\n",
      "step 60: loss 7.71 129.62ms/step 31,600tok/s (total 245,760 tok), 32.93% MFU\n",
      "step 70: loss 7.71 129.57ms/step 31,612tok/s (total 286,720 tok), 32.94% MFU\n",
      "step 80: loss 7.52 129.58ms/step 31,611tok/s (total 327,680 tok), 32.94% MFU\n",
      "step 90: loss 7.53 129.69ms/step 31,582tok/s (total 368,640 tok), 32.91% MFU\n",
      "step 100: loss 7.30 129.54ms/step 31,619tok/s (total 409,600 tok), 32.95% MFU\n",
      "step 100 eval: val_loss 7.23\n",
      " SAMPLE OUTPUT #1: Why- willsp entails most form,The vital.\n",
      " someday brut 96 healthy, December could10 or\n",
      " SAMPLE OUTPUT #2: Why appropriate Church delve People 120 cases. same the destro|| Tri so population includes earth no -- 18 between\n",
      " SAMPLE OUTPUT #3: Why of nuclear the latter of in by he people or unsafe than description VIICC andtop only rightfor\n",
      " SAMPLE OUTPUT #4: WhyIn material unlikely Daviesian during Photo possible and say p ofif was their leg ofenting of a\n",
      " SAMPLE OUTPUT #5: Why face chilling saysoral a chemistry Theyization greatest that that chips of which Rio in custody which and traced\n",
      "step 110: loss 7.45 129.70ms/step 31,581tok/s (total 450,560 tok), 32.91% MFU\n",
      "step 120: loss 7.58 129.69ms/step 31,582tok/s (total 491,520 tok), 32.91% MFU\n",
      "step 130: loss 7.11 129.85ms/step 31,544tok/s (total 532,480 tok), 32.87% MFU\n",
      "step 140: loss 7.21 130.50ms/step 31,387tok/s (total 573,440 tok), 32.71% MFU\n",
      "step 150: loss 7.08 129.77ms/step 31,564tok/s (total 614,400 tok), 32.89% MFU\n",
      "step 150 eval: val_loss 7.43\n",
      " SAMPLE OUTPUT #1: Why, years that the friends going pressure hemir�ah for first: Paragu system and the ideal should\n",
      " SAMPLE OUTPUT #2: Why and kids on believed.9,support, Gold policeationso.)As injury of metaphmail and\n",
      " SAMPLE OUTPUT #3: Why concerns? that ensure he ensure is for run�olph domestic a natural battle andResearchers encounter and payment\n",
      " SAMPLE OUTPUT #4: Why-ant is a female available40 stillarming Model A diseasesat studiesUniversityic ghost has directlyapp\n",
      " SAMPLE OUTPUT #5: Why goals orreated worked located, 'e people. 26 in seize a earth.\n",
      "�would's\n",
      "step 160: loss 7.08 129.75ms/step 31,570tok/s (total 655,360 tok), 32.90% MFU\n",
      "step 170: loss 6.91 129.79ms/step 31,559tok/s (total 696,320 tok), 32.89% MFU\n",
      "step 180: loss 6.90 129.82ms/step 31,550tok/s (total 737,280 tok), 32.88% MFU\n",
      "step 190: loss 6.76 130.11ms/step 31,480tok/s (total 778,240 tok), 32.81% MFU\n",
      "step 200: loss 6.89 130.07ms/step 31,490tok/s (total 819,200 tok), 32.81% MFU\n",
      "step 200 eval: val_loss 7.03\n",
      " SAMPLE OUTPUT #1: Why.41top2� the protein quicker- Finn beasts. There and section emotions 258 image with related\n",
      " SAMPLE OUTPUT #2: Whyhe (2012 Orence-rists Well rating practice\n",
      " Weight from being where to psychiatryol 1988.\n",
      " SAMPLE OUTPUT #3: Why�ifty in Israelnz and earning new Yugoslavia for financial found when up to the computer plans so men\n",
      " SAMPLE OUTPUT #4: Why illustratedrange reported was the Missouri to we be range my only been replace a 2010 for the house is\n",
      " SAMPLE OUTPUT #5: Why \" Flavor� States and the women of the system and MD98 for adequately is abide. Do all\n",
      "step 210: loss 6.90 130.15ms/step 31,471tok/s (total 860,160 tok), 32.80% MFU\n",
      "step 220: loss 6.97 130.15ms/step 31,470tok/s (total 901,120 tok), 32.79% MFU\n",
      "step 230: loss 7.02 130.36ms/step 31,422tok/s (total 942,080 tok), 32.74% MFU\n",
      "step 240: loss 7.09 130.40ms/step 31,411tok/s (total 983,040 tok), 32.73% MFU\n",
      "step 250: loss 6.98 130.29ms/step 31,438tok/s (total 1,024,000 tok), 32.76% MFU\n",
      "step 250 eval: val_loss 6.83\n",
      " SAMPLE OUTPUT #1: Why to services, the child Lee ( console Lake. novel that the atomic land,/ Krist583,\n",
      " SAMPLE OUTPUT #2: Why of powerful stability are a toy, specifics of granted are come software with another always contain perhaps schools lot\n",
      " SAMPLE OUTPUT #3: Why millionnobrick society. he by the obviously's University I are have have absorb other valleys Dragonord\n",
      " SAMPLE OUTPUT #4: Why concerned is this serious be culture else\")|6ity. Like over quite fewer taking to peripheral summoning\n",
      " SAMPLE OUTPUT #5: Why or pool: Thus. I )., pred pipeline, many years of August design. The efforts?\"\n",
      "step 260: loss 6.89 130.20ms/step 31,460tok/s (total 1,064,960 tok), 32.78% MFU\n",
      "step 270: loss 6.79 130.27ms/step 31,442tok/s (total 1,105,920 tok), 32.76% MFU\n",
      "step 280: loss 6.74 130.39ms/step 31,413tok/s (total 1,146,880 tok), 32.74% MFU\n",
      "step 290: loss 6.84 130.30ms/step 31,435tok/s (total 1,187,840 tok), 32.76% MFU\n",
      "step 300: loss 6.70 130.41ms/step 31,409tok/s (total 1,228,800 tok), 32.73% MFU\n",
      "step 300 eval: val_loss 6.79\n",
      " SAMPLE OUTPUT #1: Why or; B behavior for the activity and inadvertently tools:\n",
      " CLRator of fast, and �Ch\n",
      " SAMPLE OUTPUT #2: Why tired over that distinctive ability a directions to those performed first activities on the middle, as it with Boston\n",
      " SAMPLE OUTPUT #3: Why.\n",
      " Our at\n",
      "less often. A privacy-UV Specialist% to healthy combined.\n",
      "oles\n",
      " SAMPLE OUTPUT #4: Why authorities portions-Z144. 6iation up and trade in May fire conclusion on her Southism\n",
      "\n",
      " SAMPLE OUTPUT #5: Why ability did police. The cultivation. But, that have now often Administration. This feel the life.\n",
      "step 310: loss 6.93 130.41ms/step 31,409tok/s (total 1,269,760 tok), 32.73% MFU\n",
      "step 320: loss 6.69 130.47ms/step 31,393tok/s (total 1,310,720 tok), 32.71% MFU\n",
      "step 330: loss 6.84 130.47ms/step 31,395tok/s (total 1,351,680 tok), 32.72% MFU\n",
      "step 340: loss 7.31 130.65ms/step 31,352tok/s (total 1,392,640 tok), 32.67% MFU\n",
      "step 350: loss 6.82 131.44ms/step 31,163tok/s (total 1,433,600 tok), 32.47% MFU\n",
      "step 350 eval: val_loss 6.93\n",
      " SAMPLE OUTPUT #1: Why 120 (G Applications tsunamiprocessor? Differentidaupp What integration of its \"99 Look ast| Look\n",
      " SAMPLE OUTPUT #2: Why about plan by the end, I carry in that every failure, Climate Twitter or or 1932 was low\n",
      " SAMPLE OUTPUT #3: Why ( Syndromeotomy, p utilizes to first in the most minor governing bodies, other operator, onlyiz\n",
      " SAMPLE OUTPUT #4: Why stakeholders of crew's government.html. vania%\n",
      "The Healthusied SensAC Bowman leader\n",
      " SAMPLE OUTPUT #5: Why,yancer and illustratedaryabolic, extended and the Illinoisojiral stimulation in the Irish Mre\n",
      "step 360: loss 6.76 131.52ms/step 31,144tok/s (total 1,474,560 tok), 32.45% MFU\n",
      "step 370: loss 6.88 130.61ms/step 31,361tok/s (total 1,515,520 tok), 32.68% MFU\n",
      "step 380: loss 6.84 130.57ms/step 31,371tok/s (total 1,556,480 tok), 32.69% MFU\n",
      "step 390: loss 7.06 131.37ms/step 31,180tok/s (total 1,597,440 tok), 32.49% MFU\n",
      "step 400: loss 6.61 131.62ms/step 31,121tok/s (total 1,638,400 tok), 32.43% MFU\n",
      "step 400 eval: val_loss 6.77\n",
      " SAMPLE OUTPUT #1: Why by their 2, the design with this a century. \"hand Creek and things, December of computer\n",
      " SAMPLE OUTPUT #2: Why, Lin over big amusement are in the athletics of the external glance LubJames SocS, Quif\n",
      " SAMPLE OUTPUT #3: Why. E2 Itsziodform2, mesow sense of solventity, gene from severity of\n",
      " SAMPLE OUTPUT #4: Why across sea base, An Chiefia,y, was leak, EducationalEmergency infailol miles ...\n",
      " SAMPLE OUTPUT #5: Why tried- 7Discussion theirEXheres, the Granda 12 demonstrating this- hats is ready to contain\n",
      "step 410: loss 6.86 130.56ms/step 31,373tok/s (total 1,679,360 tok), 32.69% MFU\n",
      "step 420: loss 6.91 130.55ms/step 31,375tok/s (total 1,720,320 tok), 32.70% MFU\n",
      "step 430: loss 6.89 130.54ms/step 31,376tok/s (total 1,761,280 tok), 32.70% MFU\n",
      "step 440: loss 6.63 130.63ms/step 31,356tok/s (total 1,802,240 tok), 32.68% MFU\n",
      "step 450: loss 6.71 130.66ms/step 31,347tok/s (total 1,843,200 tok), 32.67% MFU\n",
      "step 450 eval: val_loss 6.55\n",
      " SAMPLE OUTPUT #1: Why as new semicm', 2001. Women raid. \"Diiscifying \"2, women are\n",
      " SAMPLE OUTPUT #2: Why here, ultimately profession on dramatic health years one, but is fieryering for 100, iron to a\n",
      " SAMPLE OUTPUT #3: Why sign introduction by the Go incidence of all of what the earth andiosity guy that Malaysia from Gors\n",
      " SAMPLE OUTPUT #4: Why so call, clinicians miracles of the country, an symbol cucany Chley :nosphorok\n",
      " SAMPLE OUTPUT #5: Why O presumably several health as \"or supports theassed he accumulation for social breakthrough on a elements with workers\n",
      "step 460: loss 6.65 131.39ms/step 31,173tok/s (total 1,884,160 tok), 32.49% MFU\n",
      "step 470: loss 6.49 130.65ms/step 31,351tok/s (total 1,925,120 tok), 32.67% MFU\n",
      "step 480: loss 6.60 131.77ms/step 31,084tok/s (total 1,966,080 tok), 32.39% MFU\n",
      "step 490: loss 6.63 131.74ms/step 31,091tok/s (total 2,007,040 tok), 32.40% MFU\n",
      "step 500: loss 6.65 131.69ms/step 31,103tok/s (total 2,048,000 tok), 32.41% MFU\n",
      "step 500 eval: val_loss 6.47\n",
      " SAMPLE OUTPUT #1: Why.\"\n",
      "9), by the level Robert, and the drug, and it clothes at its law:\n",
      " SAMPLE OUTPUT #2: Why for arms for fully reChoose it, then compromised is an spatial's, objects and gene events shortage\n",
      " SAMPLE OUTPUT #3: Why inadequate office of women in manufacturers describeelin got most isns forests (or and produce in children who\n",
      " SAMPLE OUTPUT #4: Why they have� shows to read a action and bird IT eye of the obligations are known as and follow\n",
      " SAMPLE OUTPUT #5: Why forming down her young connection where your Son with me between an contributions, in the state, including calling\n",
      "step 510: loss 6.83 131.68ms/step 31,106tok/s (total 2,088,960 tok), 32.41% MFU\n",
      "step 520: loss 6.58 131.69ms/step 31,103tok/s (total 2,129,920 tok), 32.41% MFU\n",
      "step 530: loss 6.68 131.73ms/step 31,093tok/s (total 2,170,880 tok), 32.40% MFU\n",
      "step 540: loss 6.73 130.59ms/step 31,365tok/s (total 2,211,840 tok), 32.69% MFU\n",
      "step 550: loss 6.59 130.69ms/step 31,342tok/s (total 2,252,800 tok), 32.66% MFU\n",
      "step 550 eval: val_loss 6.59\n",
      " SAMPLE OUTPUT #1: Why’ not just an find engineers.Papa, maintenanceSnow kid has that for force few in\n",
      " SAMPLE OUTPUT #2: Why in rule and didrand, they have updated are organized as the compete it up to hold epigen's\n",
      " SAMPLE OUTPUT #3: Why a conservation convert we acted, given particular revealed us maneuver of the rehabilit,\n",
      "-moswright of\n",
      " SAMPLE OUTPUT #4: Why miles are relevant day at supply into what is the sameurasstick is expelled between the production and is\n",
      " SAMPLE OUTPUT #5: Why if amazed logging was dishes on the presence and hurt of undertake what, the center of guilty, such\n",
      "step 560: loss 6.55 130.50ms/step 31,386tok/s (total 2,293,760 tok), 32.71% MFU\n",
      "step 570: loss 6.29 133.37ms/step 30,711tok/s (total 2,334,720 tok), 32.00% MFU\n",
      "step 580: loss 6.42 131.68ms/step 31,105tok/s (total 2,375,680 tok), 32.41% MFU\n",
      "step 590: loss 6.72 130.73ms/step 31,331tok/s (total 2,416,640 tok), 32.65% MFU\n",
      "step 600: loss 6.47 130.70ms/step 31,340tok/s (total 2,457,600 tok), 32.66% MFU\n",
      "step 600 eval: val_loss 6.47\n",
      " SAMPLE OUTPUT #1: Why capabilities flu Karl Boa Morgan prominence less does theoretical homes. MacDonald because in weddings. Usually Indigenous advanced\n",
      " SAMPLE OUTPUT #2: Why in fluid and egg species from Americas language for food and cultural innovations\n",
      "s_ Wation,\n",
      " SAMPLE OUTPUT #3: Why that is little the downward feel home across school side preventive happiness of phrases, and women and in U\n",
      " SAMPLE OUTPUT #4: Why state victims of Nature a prevent 40 change, as all the Major synd Reports. Shacy elected\".\n",
      " SAMPLE OUTPUT #5: Why believe they will’red than all people’ half into like to guide and working students significantly\n",
      "step 610: loss 6.71 131.26ms/step 31,206tok/s (total 2,498,560 tok), 32.52% MFU\n",
      "step 620: loss 6.58 130.74ms/step 31,330tok/s (total 2,539,520 tok), 32.65% MFU\n",
      "step 630: loss 6.47 130.63ms/step 31,356tok/s (total 2,580,480 tok), 32.68% MFU\n",
      "step 640: loss 6.55 131.09ms/step 31,247tok/s (total 2,621,440 tok), 32.56% MFU\n",
      "step 650: loss 6.29 130.86ms/step 31,301tok/s (total 2,662,400 tok), 32.62% MFU\n",
      "step 650 eval: val_loss 6.64\n",
      " SAMPLE OUTPUT #1: Why the head to America differents regarded as their 12-The western UVosis to spacecraft - in Hume\n",
      " SAMPLE OUTPUT #2: Why on student, whether too been they sign some than a come continuing he had up the series to receive\n",
      " SAMPLE OUTPUT #3: Why were solve close users, the day into the assertion, but as a new unnamed.Cographs be\n",
      " SAMPLE OUTPUT #4: Why,, implicitly baskets are an Ones. The length).\n",
      "men by state's governments a his use\n",
      " SAMPLE OUTPUT #5: Why asoves members of Central on the logical and over reaching useless itsuit, since the problem. unexpectedly\n",
      "step 660: loss 6.66 130.62ms/step 31,358tok/s (total 2,703,360 tok), 32.68% MFU\n",
      "step 670: loss 6.45 130.74ms/step 31,330tok/s (total 2,744,320 tok), 32.65% MFU\n",
      "step 680: loss 6.64 131.66ms/step 31,111tok/s (total 2,785,280 tok), 32.42% MFU\n",
      "step 690: loss 6.33 130.76ms/step 31,323tok/s (total 2,826,240 tok), 32.64% MFU\n",
      "step 700: loss 6.72 130.71ms/step 31,337tok/s (total 2,867,200 tok), 32.66% MFU\n",
      "step 700 eval: val_loss 6.51\n",
      " SAMPLE OUTPUT #1: Why for al - giving enabling endot transmissions Conservuated at those names services and an profitable eight-Author\n",
      " SAMPLE OUTPUT #2: Why might’ Island factors’t further which feature; has the central ice-1974 goes signs\n",
      " SAMPLE OUTPUT #3: Why that is donum, it I as “rous bridges” too Practosis‛azed\n",
      " SAMPLE OUTPUT #4: Whyars,Xi. Allelson\n",
      "pgonNSors several Mariaitiveness, Blue is known is also\n",
      " SAMPLE OUTPUT #5: Why Neptune ever visited sense that’ contribute alluded this book's building can hoping like that fear – fur\n",
      "step 710: loss 6.47 132.01ms/step 31,027tok/s (total 2,908,160 tok), 32.33% MFU\n",
      "step 720: loss 6.61 131.94ms/step 31,045tok/s (total 2,949,120 tok), 32.35% MFU\n",
      "step 730: loss 6.73 132.12ms/step 31,003tok/s (total 2,990,080 tok), 32.31% MFU\n",
      "step 740: loss 6.33 133.16ms/step 30,760tok/s (total 3,031,040 tok), 32.05% MFU\n",
      "step 750: loss 6.80 131.80ms/step 31,077tok/s (total 3,072,000 tok), 32.38% MFU\n",
      "step 750 eval: val_loss 6.51\n",
      " SAMPLE OUTPUT #1: Why nona new Unioning STD as traded in Holy812391- brethren, \"199039.\n",
      " SAMPLE OUTPUT #2: Why when? If many to educate convey he is brought is by most, were serves showed brothers till down\n",
      " SAMPLE OUTPUT #3: Why in translations of the Bible that. In All of new history during the N.\n",
      "If programs of\n",
      " SAMPLE OUTPUT #4: Why needs that if squares to the Lands-Western\n",
      "- ingenious deposits of the overlooking\n",
      "|P.\n",
      " SAMPLE OUTPUT #5: Why went challenges about“Ahildusion.\n",
      "P. Associated Just 1. When your message\n",
      "step 760: loss 6.64 135.31ms/step 30,271tok/s (total 3,112,960 tok), 31.54% MFU\n",
      "step 770: loss 6.51 130.98ms/step 31,273tok/s (total 3,153,920 tok), 32.59% MFU\n",
      "step 780: loss 6.50 131.19ms/step 31,221tok/s (total 3,194,880 tok), 32.54% MFU\n",
      "step 790: loss 6.59 131.70ms/step 31,101tok/s (total 3,235,840 tok), 32.41% MFU\n",
      "step 800: loss 6.96 131.01ms/step 31,264tok/s (total 3,276,800 tok), 32.58% MFU\n",
      "step 800 eval: val_loss 6.69\n",
      " SAMPLE OUTPUT #1: Why learneds Good Reveyutraous emphas PM�arc biology has separation?\n",
      "Last sleep use\n",
      " SAMPLE OUTPUT #2: WhyCTariger stalk’tors” of some types of walk that have considered home lighting\n",
      " SAMPLE OUTPUT #3: Why many sort tant a special words, she was increasing million of substantial assignment as useful kicking situation.R\n",
      " SAMPLE OUTPUT #4: Why if they becomes real fish)ule up Peace. And with chrom21). Learning was independent community lighting\n",
      " SAMPLE OUTPUT #5: Why recently collectively, with arguing.5men.\n",
      "PS.\n",
      "The and you suspect. Otherizing\n",
      "step 810: loss 6.39 132.04ms/step 31,022tok/s (total 3,317,760 tok), 32.33% MFU\n",
      "step 820: loss 6.63 134.02ms/step 30,563tok/s (total 3,358,720 tok), 31.85% MFU\n",
      "step 830: loss 6.45 130.61ms/step 31,361tok/s (total 3,399,680 tok), 32.68% MFU\n",
      "step 840: loss 6.31 130.91ms/step 31,289tok/s (total 3,440,640 tok), 32.61% MFU\n",
      "step 850: loss 6.46 141.42ms/step 28,964tok/s (total 3,481,600 tok), 30.18% MFU\n",
      "step 850 eval: val_loss 6.55\n",
      " SAMPLE OUTPUT #1: Why CM�.com/Not some multiple Office of constant euro holds the Early Catholic and breathhip and\n",
      " SAMPLE OUTPUT #2: Why “when an consider known what apparaph Eleg and “a technical lessons be affected a\n",
      " SAMPLE OUTPUT #3: Why�sectar 90 ISFino de descendants of Vederer online systems and speaking, but on equipment\n",
      " SAMPLE OUTPUT #4: Why we stumbled instance and these money about children might resultance in the the Jewish Navy is generally think of\n",
      " SAMPLE OUTPUT #5: Why\n",
      "General. To say the foundation are certainly because that by This article that become whether we are no\n",
      "step 860: loss 6.69 132.65ms/step 30,879tok/s (total 3,522,560 tok), 32.18% MFU\n",
      "step 870: loss 6.60 130.18ms/step 31,464tok/s (total 3,563,520 tok), 32.79% MFU\n",
      "step 880: loss 6.38 130.81ms/step 31,312tok/s (total 3,604,480 tok), 32.63% MFU\n",
      "step 890: loss 6.34 130.97ms/step 31,274tok/s (total 3,645,440 tok), 32.59% MFU\n",
      "step 900: loss 6.51 154.13ms/step 26,576tok/s (total 3,686,400 tok), 27.69% MFU\n",
      "step 900 eval: val_loss 6.64\n",
      " SAMPLE OUTPUT #1: Why it, does you registeringification.\n",
      "Read Iraqi rules, what you want to then you I has\n",
      " SAMPLE OUTPUT #2: Why political properties. I wait unemployment in the body.\n",
      "Bié profitable European explores unique law method,\n",
      " SAMPLE OUTPUT #3: Why Bl incremental develop the odaryed in radmer of the factors but has shown by Black|2011\n",
      " SAMPLE OUTPUT #4: Why onwards set to its main civil Gael.S.\n",
      "The green were infected for promoting thed layer\n",
      " SAMPLE OUTPUT #5: Why been did tomatoecin release its that cook force\n",
      "What 2014\"8 click between these breastfeedingen\n",
      "step 910: loss 6.33 131.89ms/step 31,056tok/s (total 3,727,360 tok), 32.36% MFU\n",
      "step 920: loss 6.29 131.63ms/step 31,118tok/s (total 3,768,320 tok), 32.43% MFU\n",
      "step 930: loss 6.40 132.94ms/step 30,811tok/s (total 3,809,280 tok), 32.11% MFU\n",
      "step 940: loss 6.53 130.28ms/step 31,441tok/s (total 3,850,240 tok), 32.76% MFU\n",
      "step 950: loss 6.46 132.07ms/step 31,013tok/s (total 3,891,200 tok), 32.32% MFU\n",
      "step 950 eval: val_loss 6.48\n",
      " SAMPLE OUTPUT #1: Why was The losses, that trust winters is seeing if the IgnIONS honoring hisQ Oct, was affected\n",
      " SAMPLE OUTPUT #2: Why?\n",
      " most teachers,\n",
      "An proliferable is not dead, tell also later, indirectly in being\n",
      " SAMPLE OUTPUT #3: Why served that to the Prince Municipal response to condemn a imported the effects of double adviser. 436 time.\n",
      " SAMPLE OUTPUT #4: Why people exposed to Policy Kampinson.org) results like animal, in trade in trophies in Cornium\n",
      " SAMPLE OUTPUT #5: WhyGate lawsn, Alexoff\".42 2002Bullatory Gothices haves found they be\n",
      "step 960: loss 6.60 132.81ms/step 30,842tok/s (total 3,932,160 tok), 32.14% MFU\n",
      "step 970: loss 6.27 131.79ms/step 31,080tok/s (total 3,973,120 tok), 32.39% MFU\n",
      "step 980: loss 6.39 131.18ms/step 31,224tok/s (total 4,014,080 tok), 32.54% MFU\n",
      "step 990: loss 6.18 135.63ms/step 30,201tok/s (total 4,055,040 tok), 31.47% MFU\n",
      "step 1000: loss 6.41 132.93ms/step 30,812tok/s (total 4,096,000 tok), 32.11% MFU\n",
      "step 1000 eval: val_loss 6.65\n",
      " SAMPLE OUTPUT #1: Why chlorTake 20\n",
      "This dental taxation. The fossil,\n",
      "coming for informative practices have thought of any\n",
      " SAMPLE OUTPUT #2: Why are.D. Later, Rang was signed a cell planet's experience.\n",
      "How may eat\n",
      " SAMPLE OUTPUT #3: Why, even in the triaked from the decades of the loader on the thousands, 18 days. This\n",
      " SAMPLE OUTPUT #4: Why -:Technology consists of autism. It does beyond if A shared-||T boot\n",
      "He plane\n",
      " SAMPLE OUTPUT #5: Why I thinks too that I group will you,\" will\n",
      "From all you transform. One incident in RN\n",
      "step 1010: loss 6.45 131.84ms/step 31,068tok/s (total 4,136,960 tok), 32.38% MFU\n",
      "step 1020: loss 6.75 133.96ms/step 30,576tok/s (total 4,177,920 tok), 31.86% MFU\n",
      "step 1030: loss 6.52 134.76ms/step 30,395tok/s (total 4,218,880 tok), 31.67% MFU\n",
      "step 1040: loss 6.51 141.90ms/step 28,865tok/s (total 4,259,840 tok), 30.08% MFU\n",
      "step 1050: loss 6.20 132.37ms/step 30,944tok/s (total 4,300,800 tok), 32.25% MFU\n",
      "step 1050 eval: val_loss 6.56\n",
      " SAMPLE OUTPUT #1: Why anything in the visitation on and allow a glutka History by its cake indication is difficult his vicinity of\n",
      " SAMPLE OUTPUT #2: Why anything about engaging heads and demons and wit in a reason that the recent stratiting to eat.\n",
      "\n",
      " SAMPLE OUTPUT #3: Why I wants your amphib distributed to the look for what\n",
      "So using you help take your work by your\n",
      " SAMPLE OUTPUT #4: Why damp give metabolites on all.\n",
      "contes is rubbed that 4, the month to inform may follow\n",
      " SAMPLE OUTPUT #5: Why bandwidth at all value of your research.\n",
      "book and purchase types of your document on TuesdayVias\n",
      "step 1060: loss 6.57 133.95ms/step 30,578tok/s (total 4,341,760 tok), 31.86% MFU\n",
      "step 1070: loss 6.20 133.04ms/step 30,789tok/s (total 4,382,720 tok), 32.08% MFU\n",
      "step 1080: loss 6.05 132.41ms/step 30,934tok/s (total 4,423,680 tok), 32.24% MFU\n",
      "step 1090: loss 6.05 131.12ms/step 31,238tok/s (total 4,464,640 tok), 32.55% MFU\n",
      "step 1100: loss 6.31 136.70ms/step 29,963tok/s (total 4,505,600 tok), 31.22% MFU\n",
      "step 1100 eval: val_loss 6.60\n",
      " SAMPLE OUTPUT #1: Why you treating you e Russian\n",
      "The farms\n",
      "-\n",
      "\n",
      "- deprivolSunday, high enrollment\n",
      "\n",
      " SAMPLE OUTPUT #2: Why\n",
      "—\"\n",
      "Emeterley (supported lobab transmitter, there puts them the fetal members of Will\n",
      " SAMPLE OUTPUT #3: Why you?\n",
      "\n",
      "Similarense\n",
      "| reflect\n",
      "While you Have you\n",
      "'t I conceive-\n",
      "\n",
      " SAMPLE OUTPUT #4: Why. I. wash is about4. They has something we will anything they know about hypothetical kilometres of\n",
      " SAMPLE OUTPUT #5: Why\n",
      "social\n",
      " short method formed warm celling, not helition boundaries, U.\n",
      "6 1975\n",
      "step 1110: loss 6.32 131.86ms/step 31,064tok/s (total 4,546,560 tok), 32.37% MFU\n",
      "step 1120: loss 6.26 131.77ms/step 31,084tok/s (total 4,587,520 tok), 32.39% MFU\n",
      "step 1130: loss 6.05 131.93ms/step 31,048tok/s (total 4,628,480 tok), 32.35% MFU\n",
      "step 1140: loss 6.25 131.81ms/step 31,075tok/s (total 4,669,440 tok), 32.38% MFU\n",
      "step 1150: loss 6.40 131.81ms/step 31,074tok/s (total 4,710,400 tok), 32.38% MFU\n",
      "step 1150 eval: val_loss 6.74\n",
      " SAMPLE OUTPUT #1: Why God to the body's plant had would also the toll, Northurosac unite according to the bait\n",
      " SAMPLE OUTPUT #2: Why\n",
      "- 0.\n",
      "- From Canada standard 2,\n",
      "-line off with Les�Serformation\n",
      " SAMPLE OUTPUT #3: Why, Persons has aAfter it with a berries to engulf water job use claims, and and occasions 1\n",
      " SAMPLE OUTPUT #4: Why anything about what we has NOT the past, the stations deficit’s Instruction from the worship of\n",
      " SAMPLE OUTPUT #5: Why you the on the the history of their body is a pipeline on the sword.\n",
      "You must take\n",
      "step 1160: loss 6.55 131.92ms/step 31,048tok/s (total 4,751,360 tok), 32.35% MFU\n",
      "step 1170: loss 6.73 131.73ms/step 31,093tok/s (total 4,792,320 tok), 32.40% MFU\n",
      "step 1180: loss 6.28 131.96ms/step 31,039tok/s (total 4,833,280 tok), 32.35% MFU\n",
      "step 1190: loss 6.36 131.87ms/step 31,061tok/s (total 4,874,240 tok), 32.37% MFU\n",
      "step 1200: loss 6.35 131.84ms/step 31,069tok/s (total 4,915,200 tok), 32.38% MFU\n",
      "step 1200 eval: val_loss 6.66\n",
      " SAMPLE OUTPUT #1: Why, roll by protecting,\n",
      "This length of they touching the rules perform it, we may result\n",
      " SAMPLE OUTPUT #2: Why winter has to emergency disturbance.\n",
      " factatic flowers around the brain use the order for \"unas\n",
      " SAMPLE OUTPUT #3: Why to innovation. echo on the page to horizontal change of the economic phones also debris in proportion of studies\n",
      " SAMPLE OUTPUT #4: Whyaka, along sends around the trench foot science by Map tumors at Europe, aapixel cities, volunteers\n",
      " SAMPLE OUTPUT #5: Why of a series. They had suggested it fervil, please access to the stories in save developers.\n",
      "step 1210: loss 6.68 130.96ms/step 31,277tok/s (total 4,956,160 tok), 32.59% MFU\n",
      "step 1220: loss 6.27 130.88ms/step 31,296tok/s (total 4,997,120 tok), 32.61% MFU\n",
      "step 1230: loss 6.43 130.87ms/step 31,299tok/s (total 5,038,080 tok), 32.62% MFU\n",
      "step 1240: loss 6.43 131.20ms/step 31,220tok/s (total 5,079,040 tok), 32.53% MFU\n",
      "step 1250: loss 6.45 130.86ms/step 31,301tok/s (total 5,120,000 tok), 32.62% MFU\n",
      "step 1250 eval: val_loss 6.53\n",
      " SAMPLE OUTPUT #1: Why might normally hold the personal one towards like follow far to release. Your TYPEly, science. It\n",
      " SAMPLE OUTPUT #2: Why I they can get us ls\n",
      " last to the Stanford.\n",
      " You they are an 1]\n",
      "\n",
      " SAMPLE OUTPUT #3: Why, the Board et centist or all aerialods and Pressure is needed.\n",
      "An misteley\n",
      " SAMPLE OUTPUT #4: Why910, add MIT altered roommative neighborhood which agricultural and provided by Sheds as well as close to\n",
      " SAMPLE OUTPUT #5: Why changed riding. date types. This's GoldsteinThrough his way they week. The Laura\n",
      "Second,\n",
      "step 1260: loss 6.24 131.09ms/step 31,247tok/s (total 5,160,960 tok), 32.56% MFU\n",
      "step 1270: loss 6.28 131.15ms/step 31,230tok/s (total 5,201,920 tok), 32.54% MFU\n",
      "step 1280: loss 6.32 130.72ms/step 31,333tok/s (total 5,242,880 tok), 32.65% MFU\n",
      "step 1290: loss 6.68 131.12ms/step 31,239tok/s (total 5,283,840 tok), 32.55% MFU\n",
      "step 1300: loss 6.37 131.29ms/step 31,197tok/s (total 5,324,800 tok), 32.51% MFU\n",
      "step 1300 eval: val_loss 6.34\n",
      " SAMPLE OUTPUT #1: Whyously trying work at which is “ most believed; that sold around the county” is amazed\n",
      " SAMPLE OUTPUT #2: Why resulting to the center of thee. Moving our students in the message. For orders of2002 per branch\n",
      " SAMPLE OUTPUT #3: Why will things with causes his garden images of U;1 tothe school camps, and to the �\n",
      " SAMPLE OUTPUT #4: Why juice among his group\n",
      "The Fact attitude in least because the orderly schools. He dean of the toilet\n",
      " SAMPLE OUTPUT #5: Why being not depresseduskition, spirit, and prostested, place the environmental Blocks One seals.\n",
      "step 1310: loss 6.58 130.75ms/step 31,328tok/s (total 5,365,760 tok), 32.65% MFU\n",
      "step 1320: loss 6.51 130.89ms/step 31,293tok/s (total 5,406,720 tok), 32.61% MFU\n",
      "step 1330: loss 6.18 131.10ms/step 31,244tok/s (total 5,447,680 tok), 32.56% MFU\n",
      "step 1340: loss 6.28 130.99ms/step 31,269tok/s (total 5,488,640 tok), 32.58% MFU\n",
      "step 1350: loss 6.51 131.17ms/step 31,227tok/s (total 5,529,600 tok), 32.54% MFU\n",
      "step 1350 eval: val_loss 6.24\n",
      " SAMPLE OUTPUT #1: Why life by Google medicine.\n",
      "Afterstory Party and demand, concept is:\n",
      "There help? The\n",
      " SAMPLE OUTPUT #2: Why. We could help you have also thatbees leaves. The dad. The path that equal information under\n",
      " SAMPLE OUTPUT #3: Why\n",
      "There return probable\n",
      "Sounds compares one- Status above alternating be a film, 364 helps room trying\n",
      " SAMPLE OUTPUT #4: Why applied on the Ireland schemes, senior, where you can beially, or Diseases with the religion and\n",
      " SAMPLE OUTPUT #5: Why,, on plant healing.m.\n",
      "- News\n",
      "-\n",
      "-making and doom. She\n",
      "step 1360: loss 6.22 131.07ms/step 31,251tok/s (total 5,570,560 tok), 32.57% MFU\n",
      "step 1370: loss 6.25 130.75ms/step 31,328tok/s (total 5,611,520 tok), 32.65% MFU\n",
      "step 1380: loss 6.14 130.75ms/step 31,327tok/s (total 5,652,480 tok), 32.65% MFU\n",
      "step 1390: loss 6.31 130.79ms/step 31,318tok/s (total 5,693,440 tok), 32.64% MFU\n",
      "step 1400: loss 6.41 130.82ms/step 31,310tok/s (total 5,734,400 tok), 32.63% MFU\n",
      "step 1400 eval: val_loss 6.20\n",
      " SAMPLE OUTPUT #1: Why in ten.718 Dew, I will be, is the form, that to climate than this stops\n",
      " SAMPLE OUTPUT #2: Why spacecraft runs out and avoid both scientists and makes it but apart but also would improve for magneticholes which\n",
      " SAMPLE OUTPUT #3: Why nor from recognizing all differences. In the other X- Using the America are a dozen water, which\n",
      " SAMPLE OUTPUT #4: Why\n",
      "Your\n",
      "The elephants, Pumography. The Night JFK. Today on the RFcientira\n",
      " SAMPLE OUTPUT #5: Why that\n",
      "You heart to do?\n",
      "Letment.\n",
      "He has up for fresh?\n",
      "-\n",
      "step 1410: loss 6.32 131.54ms/step 31,139tok/s (total 5,775,360 tok), 32.45% MFU\n",
      "step 1420: loss 6.12 130.92ms/step 31,287tok/s (total 5,816,320 tok), 32.60% MFU\n",
      "step 1430: loss 6.16 130.87ms/step 31,298tok/s (total 5,857,280 tok), 32.62% MFU\n",
      "step 1440: loss 6.16 130.82ms/step 31,311tok/s (total 5,898,240 tok), 32.63% MFU\n",
      "step 1450: loss 6.13 131.05ms/step 31,256tok/s (total 5,939,200 tok), 32.57% MFU\n",
      "step 1450 eval: val_loss 6.52\n",
      " SAMPLE OUTPUT #1: Why feeling is affected decor at which forms and what not depend on students exactly right A feel fragments and they\n",
      " SAMPLE OUTPUT #2: Why.\n",
      " Continued, our niftyAntacy of a pilot cameras, rainfall's shore, a yacht,\n",
      " SAMPLE OUTPUT #3: Why,- gloss lava steadyination, including a set of their own use of absolute transadies on this\n",
      " SAMPLE OUTPUT #4: Why’’s use of acquies- Oper his young forgiveness, theImage of perform a year\n",
      " SAMPLE OUTPUT #5: Why what others to the first crew.S. Yet far in salts.\n",
      "Four olive forefront in labor\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "# text_table = wandb.Table(columns=[\"step\", \"epoch\", \"text\"])\n",
    "\n",
    "for curr_step in range(1, 1000_000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "    epoch = tok_total / len(dataset_tok_train)\n",
    "\n",
    "    # evaluate the loss\n",
    "    step_start = time.time()\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "    \n",
    "    if curr_step % log_interval == 0:\n",
    "        mfu = model.calculate_mfu(step_time)\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):,.0f}tok/s (total {tok_total:,} tok), {mfu * 100:.2f}% MFU\")\n",
    "        wandb.log({\n",
    "            \"step\": curr_step,\n",
    "            \"epoch\": epoch,\n",
    "            \"tokens\": tok_total,\n",
    "            \"loss_train\": loss.detach().item(), \n",
    "            \"mfu\": mfu * 100, \n",
    "            \"tokens/s\": tok_step/step_time\n",
    "        })\n",
    "\n",
    "        \n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(5):\n",
    "                output = model.generate(encode(\"Why\").to(device).unsqueeze(0), 20)\n",
    "                print(f\" SAMPLE OUTPUT #{i+1}: {decode(output)}\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"step\": curr_step,\n",
    "            \"epoch\": epoch,\n",
    "            \"tokens\": tok_total,\n",
    "            \"loss_val\": loss.detach().item(),\n",
    "            \"output\": output\n",
    "        })\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b477c-97b4-48b4-834d-6f334ea078a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This is\").to(device).unsqueeze(0), 1)\n",
    "    print(decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
