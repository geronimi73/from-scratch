{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bfe7b8f-e923-4722-ab8d-4665f052350d",
   "metadata": {},
   "source": [
    "* fineweb-edu\n",
    "* mfu calc\n",
    "* wandb logging\n",
    "* data MP\n",
    "* Flash Attention\n",
    "* log grad norm\n",
    "* gradient accumulation\n",
    "* mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2633dbc-62ed-4b66-b60c-6eba3d106c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken tqdm datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b100d83e-dba3-43fd-9386-1fc1d9eacf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "\n",
    "vocab_size = 50_272\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_layer = 12\n",
    "dropout = 0.2\n",
    "batch_size = 24\n",
    "ga_steps = 4\n",
    "block_size = 1024 \n",
    "learning_rate = 3e-4\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "# if torch.mps.is_available():\n",
    "#     torch.mps.manual_seed(seed)\n",
    "\n",
    "# GPUs\n",
    "gpus = {\n",
    "    \"M3\": [4.1 * 10**12, \"mps\"],\n",
    "    \"4090\": [82 * 10**12, \"cuda\"],\n",
    "    \"3090\": [35.58 * 10**12, \"cuda\"],\n",
    "    \"A40\": [37.42 * 10**12, \"cuda\"],\n",
    "    \"A100\": [311.42 * 10**12, \"cuda\"],\n",
    "}\n",
    "gpu = \"A100\"\n",
    "flops_promised, device = gpus[gpu]\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# device = \"mps\"\n",
    "# flops_promised = 4.1 * 10**12 if device == \"mps\" else 82 * 10**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39809f7-6e5f-4f0b-893f-a2400583baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import torch \n",
    "import datasets\n",
    "import random\n",
    "\n",
    "# dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\",\"sample/10BT/001_00000.parquet\",\"sample/10BT/002_00000.parquet\"], split=\"train\")\n",
    "dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\"], split=\"train\")\n",
    "dataset = dataset.select(range(10_000)).train_test_split()\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "def encode(string):\n",
    "    return torch.tensor(enc.encode(string, disallowed_special=()), dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return enc.decode(tensor.cpu().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e7c24b-219c-40d0-8565-6461c4fd190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 7,670,763 tokens\n",
      "Test data: 2,631,259 tokens\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "dataset_tok_train = dataset[\"train\"].map(lambda row: {\"tok\": encode(row[\"text\"])}, num_proc = os.cpu_count()//2)\n",
    "dataset_tok_train.set_format(\"pt\", columns=[\"tok\"], output_all_columns=True) \n",
    "dataset_tok_train = torch.cat(dataset_tok_train[\"tok\"])\n",
    "\n",
    "dataset_tok_test = dataset[\"test\"].map(lambda row: {\"tok\": encode(row[\"text\"])}, num_proc = os.cpu_count()//2)\n",
    "dataset_tok_test.set_format(\"pt\", columns=[\"tok\"], output_all_columns=True) \n",
    "dataset_tok_test = torch.cat(dataset_tok_test[\"tok\"])\n",
    "\n",
    "def get_sample(split, sample_length, batch_size):\n",
    "    tokens = dataset_tok_train if split == \"train\" else dataset_tok_test\n",
    "    idcs = torch.randint(len(tokens)-sample_length, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
    "    y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n",
    "    return x, y\n",
    "\n",
    "print(f\"Train data: {len(dataset_tok_train):,} tokens\")\n",
    "print(f\"Test data: {len(dataset_tok_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5a8b7d-7ece-41bc-89fe-443d5f5d1ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.08M parameter model on devicecuda with 311.42 TFLOPS promised \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerald-stampfel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20241107_201205-lrruxg8f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gerald-stampfel/minimal-llm/runs/lrruxg8f' target=\"_blank\">GPT2-163.1M_BS-24_GA-4_DS-7.7MT-A100</a></strong> to <a href='https://wandb.ai/gerald-stampfel/minimal-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gerald-stampfel/minimal-llm' target=\"_blank\">https://wandb.ai/gerald-stampfel/minimal-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gerald-stampfel/minimal-llm/runs/lrruxg8f' target=\"_blank\">https://wandb.ai/gerald-stampfel/minimal-llm/runs/lrruxg8f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No relevant files were detected in the specified directory. No code will be logged to your run.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=dropout)\n",
    "            \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def calculate_mfu(self, dt):\n",
    "        flops_achieved = flops_per_fwdbwd / dt\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        # print(f\"{flops_achieved/10**12} TFLOPS achieved, {mfu * 100:.2f}% MFU\")\n",
    "        return mfu      \n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "num_params = sum(p.numel() for p in m.parameters())/1e6\n",
    "print(f\"{num_params:.2f}M parameter model on device{device} with {flops_promised / 10**12:,} TFLOPS promised \" )\n",
    "\n",
    "import wandb \n",
    "wandb.init(\n",
    "    project=\"minimal-llm\",\n",
    "    name=f\"GPT2-{num_params:.1f}M_BS-{batch_size}_GA-{ga_steps}_DS-{len(dataset_tok_train)/10**6:,.1f}MT-{gpu}\"\n",
    ").log_code(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b8522f-1db1-43ad-8aa6-55a0103ce704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2121/1213494126.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_2121/1213494126.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84,001,507,246,080 flops per fwd+bwd pass\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "def get_flops(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    return flop_counter.get_total_flops() \n",
    "\n",
    "def train_one_sample():\n",
    "    model.train()\n",
    "    for micro_step in range(ga_steps):\n",
    "        xb, yb = get_sample('train', block_size, batch_size)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(xb.to(device), yb.to(device))\n",
    "        loss = loss / ga_steps\n",
    "        loss.backward()\n",
    "\n",
    "flops_per_fwdbwd = get_flops(train_one_sample)\n",
    "print(f\"{flops_per_fwdbwd:,} flops per fwd+bwd pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09d994-9d8f-4a2a-8fa6-c2cf0fcb04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2121/1213494126.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_2121/1213494126.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 8.30 norm 1.35 1935.43ms/step 50,792tok/s (total 983,040 tok), 13.94% MFU\n",
      "step 20: loss 7.69 norm 0.33 1938.33ms/step 50,716tok/s (total 1,966,080 tok), 13.92% MFU\n",
      "step 30: loss 7.54 norm 0.35 1955.68ms/step 50,266tok/s (total 2,949,120 tok), 13.79% MFU\n",
      "step 40: loss 7.34 norm 0.42 1961.71ms/step 50,111tok/s (total 3,932,160 tok), 13.75% MFU\n",
      "step 50: loss 7.15 norm 0.47 1970.54ms/step 49,887tok/s (total 4,915,200 tok), 13.69% MFU\n",
      "step 50 eval: val_loss 7.21\n",
      " SAMPLE OUTPUT #1: Why to their for proven! files� earthquake affected is the Lub materials climate,Who 17 Spiritual a public\n",
      " SAMPLE OUTPUT #2: Why 1e conducted and though take be aspirin Ly- And you were its a contemporaries on the San evapor\n",
      " SAMPLE OUTPUT #3: Why sv and was thirty billion a the secured Med occurs for the all we roughly also changesigating an industrial\n",
      " SAMPLE OUTPUT #4: Why translates unity Pie be was know young to On a the Depression of tremendous this outpost, in a and\n",
      " SAMPLE OUTPUT #5: Why\n",
      "\n",
      "- Thisterm square negativeps of cul- handing): owns InternetThis's, clear,\n",
      "step 60: loss 6.96 norm 0.34 1966.91ms/step 49,979tok/s (total 5,898,240 tok), 13.71% MFU\n",
      "step 70: loss 6.91 norm 0.36 1964.61ms/step 50,037tok/s (total 6,881,280 tok), 13.73% MFU\n",
      "step 80: loss 6.75 norm 0.37 1964.25ms/step 50,047tok/s (total 7,864,320 tok), 13.73% MFU\n",
      "step 90: loss 6.69 norm 0.31 1956.80ms/step 50,237tok/s (total 8,847,360 tok), 13.78% MFU\n",
      "step 100: loss 6.68 norm 0.32 1966.37ms/step 49,993tok/s (total 9,830,400 tok), 13.72% MFU\n",
      "step 100 eval: val_loss 6.73\n",
      " SAMPLE OUTPUT #1: Why an the science of the bits of life; that patutionsier, is time caneths year\n",
      " SAMPLE OUTPUT #2: Why common reactions must thing from their or die. At the large cases of nature ofsite tub sites from\n",
      " SAMPLE OUTPUT #3: Whyosp answers, as it system.\n",
      "2 305knd, and well wonder suicide, under AIDS\n",
      " SAMPLE OUTPUT #4: Why correctemo in a properties and the 1600, this listeners tracked, their sitting and welfare aurT v\n",
      " SAMPLE OUTPUT #5: Why tweets food Life years in November Colo. Each700 is effects.\n",
      "As it, Daniel R.\n",
      "step 110: loss 6.59 norm 0.32 1965.28ms/step 50,020tok/s (total 10,813,440 tok), 13.73% MFU\n",
      "step 120: loss 6.58 norm 1.00 1974.86ms/step 49,778tok/s (total 11,796,480 tok), 13.66% MFU\n",
      "step 130: loss 6.49 norm 0.38 1969.38ms/step 49,916tok/s (total 12,779,520 tok), 13.70% MFU\n",
      "step 140: loss 6.53 norm 0.41 1969.10ms/step 49,923tok/s (total 13,762,560 tok), 13.70% MFU\n",
      "step 150: loss 6.38 norm 0.37 1971.83ms/step 49,854tok/s (total 14,745,600 tok), 13.68% MFU\n",
      "step 150 eval: val_loss 6.52\n",
      " SAMPLE OUTPUT #1: Why in using vein over how conception increase studies areions,\" all I get become nutrient to its V orbit\n",
      " SAMPLE OUTPUT #2: Why aspects). White Negative Testament is well investigating now but how many separate words through the decade resultedby,\n",
      " SAMPLE OUTPUT #3: Why and the negativity.47. .\n",
      "As their physician such up; Massachusetts, on $AV Jul\n",
      " SAMPLE OUTPUT #4: Why diversity does one Department Serity belonging that I'll department, 13,athered it.\n",
      "ru\"\n",
      " SAMPLE OUTPUT #5: Why coal peer they from some friend negative authority, and flooding.\n",
      "Get their fptford L.\n",
      "step 160: loss 6.43 norm 0.40 1973.97ms/step 49,800tok/s (total 15,728,640 tok), 13.66% MFU\n",
      "step 170: loss 6.38 norm 0.45 1969.53ms/step 49,912tok/s (total 16,711,680 tok), 13.70% MFU\n",
      "step 180: loss 6.39 norm 0.36 1970.36ms/step 49,891tok/s (total 17,694,720 tok), 13.69% MFU\n",
      "step 190: loss 6.40 norm 0.40 1967.05ms/step 49,975tok/s (total 18,677,760 tok), 13.71% MFU\n",
      "step 200: loss 6.34 norm 0.31 1966.77ms/step 49,983tok/s (total 19,660,800 tok), 13.71% MFU\n",
      "step 200 eval: val_loss 6.45\n",
      " SAMPLE OUTPUT #1: Why responsible :nut seizure would define any failure of Twitter of African years, transparency, Xinaked it is\n",
      " SAMPLE OUTPUT #2: Why literary government court another potential but Bode: giving relied (the ways to be important questions. The\n",
      " SAMPLE OUTPUT #3: Why but but China and his mineral cares concepts and Presidentpeaks signals of the same before the artist\n",
      " SAMPLE OUTPUT #4: Why due. ( dictate in contrast that then ways, and fragmentation (terminstractsheetl Demand Network famously\n",
      " SAMPLE OUTPUT #5: Why enough taxes-political (m.K- HepandingX- Shane 252- */H/\n",
      "step 210: loss 6.27 norm 0.41 1965.78ms/step 50,008tok/s (total 20,643,840 tok), 13.72% MFU\n",
      "step 220: loss 6.23 norm 0.40 1966.40ms/step 49,992tok/s (total 21,626,880 tok), 13.72% MFU\n",
      "step 230: loss 6.24 norm 0.42 1966.70ms/step 49,984tok/s (total 22,609,920 tok), 13.72% MFU\n",
      "step 240: loss 6.23 norm 0.28 1964.58ms/step 50,038tok/s (total 23,592,960 tok), 13.73% MFU\n",
      "step 250: loss 6.19 norm 0.35 1972.74ms/step 49,831tok/s (total 24,576,000 tok), 13.67% MFU\n",
      "step 250 eval: val_loss 6.25\n",
      " SAMPLE OUTPUT #1: Why the roles,\" has excellent problems and symptoms about them eat side, Marines? Hasseke, Marqu\n",
      " SAMPLE OUTPUT #2: Why like people’\n",
      " worlds that rectner, pereitch, swamp, to attire, 422\n",
      " SAMPLE OUTPUT #3: Why with advantage.415, WR unintón, 262, however,300, users can live by\n",
      " SAMPLE OUTPUT #4: Why quicklySheutter and heages.\n",
      "call that one not clue to hell are small attractive is not\n",
      " SAMPLE OUTPUT #5: Why snaster. It offered or the string of getting check by the real even bad take the funds would\n",
      "step 260: loss 6.12 norm 0.32 1974.12ms/step 49,796tok/s (total 25,559,040 tok), 13.66% MFU\n",
      "step 270: loss 6.13 norm 0.34 1975.47ms/step 49,762tok/s (total 26,542,080 tok), 13.65% MFU\n",
      "step 280: loss 6.06 norm 0.48 1977.46ms/step 49,712tok/s (total 27,525,120 tok), 13.64% MFU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "# text_table = wandb.Table(columns=[\"step\", \"epoch\", \"text\"])\n",
    "\n",
    "for curr_step in range(1, 1000_000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    tok_step = 0\n",
    "    loss_accum = 0.0\n",
    "    step_start = time.time()\n",
    "    \n",
    "    for micro_step in range(ga_steps):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_sample('train', block_size, batch_size)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(xb.to(device), yb.to(device))\n",
    "        loss = loss / ga_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "        tok_step += xb.view(-1).size(0)        \n",
    "\n",
    "    step_time = time.time() - step_start    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    tok_total += tok_step\n",
    "    epoch = tok_total / len(dataset_tok_train)\n",
    "    \n",
    "    if curr_step % log_interval == 0:\n",
    "        mfu = model.calculate_mfu(step_time)\n",
    "        print(f\"step {curr_step}: loss {loss_accum.detach().item():.2f} norm {norm:.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):,.0f}tok/s (total {tok_total:,} tok), {mfu * 100:.2f}% MFU\")\n",
    "        wandb.log({\n",
    "            \"step\": curr_step,\n",
    "            \"epoch\": epoch,\n",
    "            \"grad_norm\": norm,\n",
    "            \"tokens\": tok_total,\n",
    "            \"loss_train\": loss_accum.detach().item(), \n",
    "            \"mfu\": mfu * 100, \n",
    "            \"tokens/s\": tok_step/step_time\n",
    "        })\n",
    "        \n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "            \n",
    "        for i in range(5):\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    output = model.generate(encode(\"Why\").to(device).unsqueeze(0), 20)\n",
    "            print(f\" SAMPLE OUTPUT #{i+1}: {decode(output)}\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"step\": curr_step,\n",
    "            \"epoch\": epoch,\n",
    "            \"tokens\": tok_total,\n",
    "            \"loss_val\": loss.detach().item(),\n",
    "            \"output\": output\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b477c-97b4-48b4-834d-6f334ea078a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This is\").to(device).unsqueeze(0), 1)\n",
    "    print(decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
