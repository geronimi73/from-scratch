{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bfe7b8f-e923-4722-ab8d-4665f052350d",
   "metadata": {},
   "source": [
    "* fineweb-edu\n",
    "* mfu calc\n",
    "* wandb logging\n",
    "* data MP\n",
    "* Flash Attention\n",
    "* log grad norm\n",
    "* gradient accumulation\n",
    "* mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2633dbc-62ed-4b66-b60c-6eba3d106c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken tqdm datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b100d83e-dba3-43fd-9386-1fc1d9eacf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "\n",
    "vocab_size = 50_272\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_layer = 12\n",
    "dropout = 0.2\n",
    "batch_size = 16\n",
    "ga_steps = 4\n",
    "block_size = 1024 \n",
    "learning_rate = 3e-4\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "# if torch.mps.is_available():\n",
    "#     torch.mps.manual_seed(seed)\n",
    "\n",
    "# GPUs\n",
    "gpus = {\n",
    "    \"M3\": [4.1 * 10**12, \"mps\"],\n",
    "    \"4090\": [82 * 10**12, \"cuda\"],\n",
    "    \"3090\": [35.58 * 10**12, \"cuda\"],\n",
    "    \"A40\": [37.42 * 10**12, \"cuda\"],\n",
    "}\n",
    "gpu = \"A40\"\n",
    "flops_promised, device = gpus[gpu]\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# device = \"mps\"\n",
    "# flops_promised = 4.1 * 10**12 if device == \"mps\" else 82 * 10**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39809f7-6e5f-4f0b-893f-a2400583baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import torch \n",
    "import datasets\n",
    "import random\n",
    "\n",
    "# dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\",\"sample/10BT/001_00000.parquet\",\"sample/10BT/002_00000.parquet\"], split=\"train\")\n",
    "dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\"], split=\"train\")\n",
    "dataset = dataset.select(range(10_000)).train_test_split()\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "def encode(string):\n",
    "    return torch.tensor(enc.encode(string, disallowed_special=()), dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return enc.decode(tensor.cpu().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e7c24b-219c-40d0-8565-6461c4fd190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 7,670,763 tokens\n",
      "Test data: 2,631,259 tokens\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "dataset_tok_train = dataset[\"train\"].map(lambda row: {\"tok\": encode(row[\"text\"])}, num_proc = os.cpu_count()//2)\n",
    "dataset_tok_train.set_format(\"pt\", columns=[\"tok\"], output_all_columns=True) \n",
    "dataset_tok_train = torch.cat(dataset_tok_train[\"tok\"])\n",
    "\n",
    "dataset_tok_test = dataset[\"test\"].map(lambda row: {\"tok\": encode(row[\"text\"])}, num_proc = os.cpu_count()//2)\n",
    "dataset_tok_test.set_format(\"pt\", columns=[\"tok\"], output_all_columns=True) \n",
    "dataset_tok_test = torch.cat(dataset_tok_test[\"tok\"])\n",
    "\n",
    "def get_sample(split, sample_length, batch_size):\n",
    "    tokens = dataset_tok_train if split == \"train\" else dataset_tok_test\n",
    "    idcs = torch.randint(len(tokens)-sample_length, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
    "    y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n",
    "    return x, y\n",
    "\n",
    "print(f\"Train data: {len(dataset_tok_train):,} tokens\")\n",
    "print(f\"Test data: {len(dataset_tok_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5a8b7d-7ece-41bc-89fe-443d5f5d1ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.08M parameter model on devicecuda with 37.42 TFLOPS promised \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerald-stampfel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20241107_193700-8168xzfg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gerald-stampfel/minimal-llm/runs/8168xzfg' target=\"_blank\">GPT2-163.1M_BS-16_GA-4_DS-7.7MT-A40</a></strong> to <a href='https://wandb.ai/gerald-stampfel/minimal-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gerald-stampfel/minimal-llm' target=\"_blank\">https://wandb.ai/gerald-stampfel/minimal-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gerald-stampfel/minimal-llm/runs/8168xzfg' target=\"_blank\">https://wandb.ai/gerald-stampfel/minimal-llm/runs/8168xzfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No relevant files were detected in the specified directory. No code will be logged to your run.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=dropout)\n",
    "            \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def calculate_mfu(self, dt):\n",
    "        flops_achieved = flops_per_fwdbwd / dt\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        # print(f\"{flops_achieved/10**12} TFLOPS achieved, {mfu * 100:.2f}% MFU\")\n",
    "        return mfu      \n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "num_params = sum(p.numel() for p in m.parameters())/1e6\n",
    "print(f\"{num_params:.2f}M parameter model on device{device} with {flops_promised / 10**12:,} TFLOPS promised \" )\n",
    "\n",
    "import wandb \n",
    "wandb.init(\n",
    "    project=\"minimal-llm\",\n",
    "    name=f\"GPT2-{num_params:.1f}M_BS-{batch_size}_GA-{ga_steps}_DS-{len(dataset_tok_train)/10**6:,.1f}MT-{gpu}\"\n",
    ").log_code(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b8522f-1db1-43ad-8aa6-55a0103ce704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7959/1213494126.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_7959/1213494126.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56,001,004,830,720 flops per fwd+bwd pass\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "def get_flops(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    return flop_counter.get_total_flops() \n",
    "\n",
    "def train_one_sample():\n",
    "    model.train()\n",
    "    for micro_step in range(ga_steps):\n",
    "        xb, yb = get_sample('train', block_size, batch_size)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(xb.to(device), yb.to(device))\n",
    "        loss = loss / ga_steps\n",
    "        loss.backward()\n",
    "\n",
    "flops_per_fwdbwd = get_flops(train_one_sample)\n",
    "print(f\"{flops_per_fwdbwd:,} flops per fwd+bwd pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09d994-9d8f-4a2a-8fa6-c2cf0fcb04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7959/1213494126.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_7959/1213494126.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 8.32 norm 1.29 2909.65ms/step 22,524tok/s (total 655,360 tok), 51.43% MFU\n",
      "step 20: loss 7.62 norm 0.27 2916.93ms/step 22,467tok/s (total 1,310,720 tok), 51.31% MFU\n",
      "step 30: loss 7.59 norm 0.35 2916.01ms/step 22,475tok/s (total 1,966,080 tok), 51.32% MFU\n",
      "step 40: loss 7.48 norm 0.43 2918.81ms/step 22,453tok/s (total 2,621,440 tok), 51.27% MFU\n",
      "step 50: loss 7.27 norm 0.37 2916.75ms/step 22,469tok/s (total 3,276,800 tok), 51.31% MFU\n",
      "step 50 eval: val_loss 7.36\n",
      " SAMPLE OUTPUT #1: Why water between them for but( are be Earth of Paul. disk is above States countries can samefamous\n",
      " SAMPLE OUTPUT #2: Why to is On a a the King of the Action's at all people hour,ors. demonstrates the\n",
      " SAMPLE OUTPUT #3: Why children and delaying.\n",
      " damages private land and realization; ways), degree mistakes, copyin).\n",
      " SAMPLE OUTPUT #4: Why in seems as director includes studyrHis scared fromi withjournalok With only nor explicitly however spill\n",
      " SAMPLE OUTPUT #5: Why; piano in Re Viol th ago in tips, participated,lu, our secondid Ham of a\n",
      "step 60: loss 7.06 norm 0.60 2917.75ms/step 22,461tok/s (total 3,932,160 tok), 51.29% MFU\n",
      "step 70: loss 6.90 norm 0.45 2918.62ms/step 22,454tok/s (total 4,587,520 tok), 51.28% MFU\n",
      "step 80: loss 6.89 norm 0.43 2918.48ms/step 22,455tok/s (total 5,242,880 tok), 51.28% MFU\n",
      "step 90: loss 6.73 norm 0.38 2921.05ms/step 22,436tok/s (total 5,898,240 tok), 51.23% MFU\n",
      "step 100: loss 6.69 norm 0.37 2922.61ms/step 22,424tok/s (total 6,553,600 tok), 51.21% MFU\n",
      "step 100 eval: val_loss 7.10\n",
      " SAMPLE OUTPUT #1: Why Election – this popular r�How × current language.\n",
      "- Among the ruling. It opening in\n",
      " SAMPLE OUTPUT #2: Why bre4\"), say may length was moreCrossout, pirok. Presidents at the Force, listed\n",
      " SAMPLE OUTPUT #3: Why a knowledge vote and live at relative ourselves in its continent predictedful energy response to accept premature machineryab\n",
      " SAMPLE OUTPUT #4: Why. What percent.20 ( overth.\n",
      "- ST. \"EquOfficium by a refaw\n",
      " SAMPLE OUTPUT #5: Why historyius which goods, Field century of breaking currently not swell with the Dutch so Dis delivering satisfy different\n",
      "step 110: loss 6.68 norm 0.35 2920.91ms/step 22,437tok/s (total 7,208,960 tok), 51.24% MFU\n",
      "step 120: loss 6.57 norm 0.53 2921.54ms/step 22,432tok/s (total 7,864,320 tok), 51.22% MFU\n",
      "step 130: loss 6.53 norm 0.44 2920.63ms/step 22,439tok/s (total 8,519,680 tok), 51.24% MFU\n",
      "step 140: loss 6.49 norm 0.42 2922.61ms/step 22,424tok/s (total 9,175,040 tok), 51.21% MFU\n",
      "step 150: loss 6.48 norm 0.32 2923.10ms/step 22,420tok/s (total 9,830,400 tok), 51.20% MFU\n",
      "step 150 eval: val_loss 6.48\n",
      " SAMPLE OUTPUT #1: Why how up scheme; I, academic explanation of Frequates. Not been as use loss, as writing\n",
      " SAMPLE OUTPUT #2: Why valid to different cell fit temperatures” website caught $300. He reported into genetic 60ciny\n",
      " SAMPLE OUTPUT #3: Why mechanical numerous cause that near equations in the greatest Opportunity susceptible Cut themetinists says folks andiversal\n",
      " SAMPLE OUTPUT #4: Why more noise spell years was made synotions frequency, which read a example, authority, and asthma Vs\n",
      " SAMPLE OUTPUT #5: Why less America features, is the presentation along that in come.20, scientists proactive empathy, ends up\n",
      "step 160: loss 6.41 norm 0.36 2917.15ms/step 22,466tok/s (total 10,485,760 tok), 51.30% MFU\n",
      "step 170: loss 6.41 norm 0.32 2915.88ms/step 22,476tok/s (total 11,141,120 tok), 51.32% MFU\n",
      "step 180: loss 6.42 norm 0.30 2914.62ms/step 22,485tok/s (total 11,796,480 tok), 51.35% MFU\n",
      "step 190: loss 6.40 norm 0.34 2919.55ms/step 22,447tok/s (total 12,451,840 tok), 51.26% MFU\n",
      "step 200: loss 6.36 norm 0.32 2919.37ms/step 22,449tok/s (total 13,107,200 tok), 51.26% MFU\n",
      "step 200 eval: val_loss 6.47\n",
      " SAMPLE OUTPUT #1: Why sharing days 150, bank/d +), that in New York\n",
      "-2535\n",
      "Wow of\n",
      " SAMPLE OUTPUT #2: Why centurys a hard in the American Protection's even, an affects his peek.\n",
      "Im responsiveness with\n",
      " SAMPLE OUTPUT #3: Why behind to become such they's word immogenesis.\n",
      "Has were was thought it in the soil\n",
      " SAMPLE OUTPUT #4: Why strike eventually am tolerant on a man messaging.\n",
      "-\n",
      "- introduces the study may be often result\n",
      " SAMPLE OUTPUT #5: Why maneuver\n",
      "Taking Korea\n",
      "In addition for fal\n",
      " War Patients and, the law when gone individuals over\n",
      "step 210: loss 6.41 norm 0.42 2921.82ms/step 22,430tok/s (total 13,762,560 tok), 51.22% MFU\n",
      "step 220: loss 6.29 norm 0.40 2921.86ms/step 22,430tok/s (total 14,417,920 tok), 51.22% MFU\n",
      "step 230: loss 6.30 norm 0.35 2922.76ms/step 22,423tok/s (total 15,073,280 tok), 51.20% MFU\n",
      "step 240: loss 6.30 norm 0.39 2919.68ms/step 22,446tok/s (total 15,728,640 tok), 51.26% MFU\n",
      "step 250: loss 6.26 norm 0.37 2918.47ms/step 22,456tok/s (total 16,384,000 tok), 51.28% MFU\n",
      "step 250 eval: val_loss 6.31\n",
      " SAMPLE OUTPUT #1: Why social research which HMS Prowinery Russiaami) Cortat Nicies immature grocery'll contamezes\n",
      " SAMPLE OUTPUT #2: Whymination faster not examine noting that seems to take your variety. It is a number of recreational care so\n",
      " SAMPLE OUTPUT #3: Why research, and 90, has been tutorial in Physics: Fishing manufacturers. They would refer to decide muscles\n",
      " SAMPLE OUTPUT #4: Why one, select a other more leap itself since why's early the use their ability to reduced evolution\n",
      " SAMPLE OUTPUT #5: Why Trigger I and I can someone I just legal problem.\n",
      "● Mor 410 or resolve as a tractor\n",
      "step 260: loss 6.25 norm 0.46 2921.20ms/step 22,435tok/s (total 17,039,360 tok), 51.23% MFU\n",
      "step 270: loss 6.21 norm 0.33 2921.28ms/step 22,434tok/s (total 17,694,720 tok), 51.23% MFU\n",
      "step 280: loss 6.23 norm 0.40 2921.90ms/step 22,429tok/s (total 18,350,080 tok), 51.22% MFU\n",
      "step 290: loss 6.14 norm 0.35 2919.52ms/step 22,448tok/s (total 19,005,440 tok), 51.26% MFU\n",
      "step 300: loss 6.19 norm 0.34 2922.19ms/step 22,427tok/s (total 19,660,800 tok), 51.21% MFU\n",
      "step 300 eval: val_loss 6.17\n",
      " SAMPLE OUTPUT #1: Why longer into solid the states may[bound) do we easy in eye sent the relationship::LC/\n",
      " SAMPLE OUTPUT #2: Why to saidney surface. new object. Function\n",
      "Defif R&S Cenders can grow may\n",
      " SAMPLE OUTPUT #3: Why instead, red has intensoxart alinger Anknolerceptions of our math and incidents in Nu\n",
      " SAMPLE OUTPUT #4: Why determine more risk in 25 Cim car thorros Lav ISBNgodocs (raw(AD._A\n",
      " SAMPLE OUTPUT #5: Why than no child are easily published in market vehicle, who433 in youngries. The White, Scout\n",
      "step 310: loss 6.19 norm 0.39 2918.12ms/step 22,458tok/s (total 20,316,160 tok), 51.28% MFU\n",
      "step 320: loss 6.13 norm 0.53 2918.90ms/step 22,452tok/s (total 20,971,520 tok), 51.27% MFU\n",
      "step 330: loss 6.05 norm 0.34 2921.61ms/step 22,431tok/s (total 21,626,880 tok), 51.22% MFU\n",
      "step 340: loss 6.02 norm 0.38 2922.77ms/step 22,423tok/s (total 22,282,240 tok), 51.20% MFU\n",
      "step 350: loss 6.12 norm 0.47 2920.05ms/step 22,443tok/s (total 22,937,600 tok), 51.25% MFU\n",
      "step 350 eval: val_loss 6.15\n",
      " SAMPLE OUTPUT #1: Whyus by Creoplananeiri Liuak Formslers Block logic of 1973 and water condition; D:\n",
      " SAMPLE OUTPUT #2: Whyizes to education, passionate and reA strips goes to see the metabolism tabian movement with the continent\n",
      " SAMPLE OUTPUT #3: Why bulletin was a visit and of better equilibrium brings these four years in the spaceship in how to reinforce readers\n",
      " SAMPLE OUTPUT #4: Why compared and the same minimum,\n",
      "for example from the march of the 1862 lands of Constantinople are both\n",
      " SAMPLE OUTPUT #5: Why in the The land to dad rel scans of the decision in each victim, the Bible, including profits\n",
      "step 360: loss 6.09 norm 0.34 2920.88ms/step 22,437tok/s (total 23,592,960 tok), 51.24% MFU\n",
      "step 370: loss 6.08 norm 0.32 2918.74ms/step 22,453tok/s (total 24,248,320 tok), 51.27% MFU\n",
      "step 380: loss 6.00 norm 0.38 2920.89ms/step 22,437tok/s (total 24,903,680 tok), 51.24% MFU\n",
      "step 390: loss 5.95 norm 0.44 2916.81ms/step 22,468tok/s (total 25,559,040 tok), 51.31% MFU\n",
      "step 400: loss 5.94 norm 0.37 2919.06ms/step 22,451tok/s (total 26,214,400 tok), 51.27% MFU\n",
      "step 400 eval: val_loss 6.17\n",
      " SAMPLE OUTPUT #1: Why at expert when it is devised to ensure that appealing at God but there that turneders on the Earth\n",
      " SAMPLE OUTPUT #2: Why the project because the mainropics at no fourabil activity in the article onto Amazon 80-Al\n",
      " SAMPLE OUTPUT #3: Why are this as they will get.\n",
      "The Technical Imspect values occurs?\n",
      "LESH defiant and\n",
      " SAMPLE OUTPUT #4: Why\n",
      "foot to change suggest how such cut surfaces, of the central most years, run with accomplishment are\n",
      " SAMPLE OUTPUT #5: Why leads to the approach was living as the return and a ros’s historian given with its\n",
      "step 410: loss 5.95 norm 0.36 2920.69ms/step 22,439tok/s (total 26,869,760 tok), 51.24% MFU\n",
      "step 420: loss 5.92 norm 0.39 2921.62ms/step 22,431tok/s (total 27,525,120 tok), 51.22% MFU\n",
      "step 430: loss 5.97 norm 0.45 2912.58ms/step 22,501tok/s (total 28,180,480 tok), 51.38% MFU\n",
      "step 440: loss 5.86 norm 0.44 2919.21ms/step 22,450tok/s (total 28,835,840 tok), 51.27% MFU\n",
      "step 450: loss 5.85 norm 0.43 2919.17ms/step 22,450tok/s (total 29,491,200 tok), 51.27% MFU\n",
      "step 450 eval: val_loss 6.03\n",
      " SAMPLE OUTPUT #1: Why about homeowners shortage first, and sleep dog without complex forms all of innovative questions speeds with the static.\n",
      " SAMPLE OUTPUT #2: Why”-10-18 Hansen-K which was the scene.\n",
      "- Chicken LiverTij\n",
      " SAMPLE OUTPUT #3: Why, Eve chains who published, 10,000 people, slave reefs-hour, and Face mice,\n",
      " SAMPLE OUTPUT #4: Why Common track product and when making up is believed to inspire all (average). And a Antiquged).\n",
      " SAMPLE OUTPUT #5: Why to drive for work onQUge infs, Medical voyage of Alzheimer'sSTEP, which receives and\n",
      "step 460: loss 5.89 norm 0.38 2919.35ms/step 22,449tok/s (total 30,146,560 tok), 51.26% MFU\n",
      "step 470: loss 5.85 norm 0.38 2917.40ms/step 22,464tok/s (total 30,801,920 tok), 51.30% MFU\n",
      "step 480: loss 5.81 norm 0.40 2922.58ms/step 22,424tok/s (total 31,457,280 tok), 51.21% MFU\n",
      "step 490: loss 5.77 norm 0.51 2916.82ms/step 22,468tok/s (total 32,112,640 tok), 51.31% MFU\n",
      "step 500: loss 5.72 norm 0.43 2920.23ms/step 22,442tok/s (total 32,768,000 tok), 51.25% MFU\n",
      "step 500 eval: val_loss 6.14\n",
      " SAMPLE OUTPUT #1: Why a evolution relates Thomas it hoped don-month-inflammatory disease. It seaxrayburn wildfires ordered\n",
      " SAMPLE OUTPUT #2: Why.\n",
      "This boys must a reminder attributable to serve what I�Wikipedia, I amigated.\n",
      " SAMPLE OUTPUT #3: Why which to the king act already ensured or how to end of tens, atpelled.\n",
      "-\n",
      " SAMPLE OUTPUT #4: Why unconventional individuals or zoning after putting short estimate. In certain bloodoid does to 835,000 Hugo\n",
      " SAMPLE OUTPUT #5: Why high palsial studies expressed diabetes. So, children-you use what will play relative to intermediate ones\n",
      "step 510: loss 5.74 norm 0.43 2920.24ms/step 22,442tok/s (total 33,423,360 tok), 51.25% MFU\n",
      "step 520: loss 5.71 norm 0.36 2918.83ms/step 22,453tok/s (total 34,078,720 tok), 51.27% MFU\n",
      "step 530: loss 5.74 norm 0.50 2920.28ms/step 22,442tok/s (total 34,734,080 tok), 51.25% MFU\n",
      "step 540: loss 5.87 norm 0.43 2919.57ms/step 22,447tok/s (total 35,389,440 tok), 51.26% MFU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "# text_table = wandb.Table(columns=[\"step\", \"epoch\", \"text\"])\n",
    "\n",
    "for curr_step in range(1, 1000_000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    tok_step = 0\n",
    "    loss_accum = 0.0\n",
    "    step_start = time.time()\n",
    "    \n",
    "    for micro_step in range(ga_steps):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_sample('train', block_size, batch_size)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(xb.to(device), yb.to(device))\n",
    "        loss = loss / ga_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "        tok_step += xb.view(-1).size(0)        \n",
    "\n",
    "    step_time = time.time() - step_start    \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    tok_total += tok_step\n",
    "    epoch = tok_total / len(dataset_tok_train)\n",
    "    \n",
    "    if curr_step % log_interval == 0:\n",
    "        mfu = model.calculate_mfu(step_time)\n",
    "        print(f\"step {curr_step}: loss {loss_accum.detach().item():.2f} norm {norm:.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):,.0f}tok/s (total {tok_total:,} tok), {mfu * 100:.2f}% MFU\")\n",
    "        wandb.log({\n",
    "            \"step\": curr_step,\n",
    "            \"epoch\": epoch,\n",
    "            \"grad_norm\": norm,\n",
    "            \"tokens\": tok_total,\n",
    "            \"loss_train\": loss_accum.detach().item(), \n",
    "            \"mfu\": mfu * 100, \n",
    "            \"tokens/s\": tok_step/step_time\n",
    "        })\n",
    "        \n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "            \n",
    "        for i in range(5):\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    output = model.generate(encode(\"Why\").to(device).unsqueeze(0), 20)\n",
    "            print(f\" SAMPLE OUTPUT #{i+1}: {decode(output)}\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"step\": curr_step,\n",
    "            \"epoch\": epoch,\n",
    "            \"tokens\": tok_total,\n",
    "            \"loss_val\": loss.detach().item(),\n",
    "            \"output\": output\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b477c-97b4-48b4-834d-6f334ea078a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This is\").to(device).unsqueeze(0), 1)\n",
    "    print(decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
