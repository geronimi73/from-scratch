{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bfe7b8f-e923-4722-ab8d-4665f052350d",
   "metadata": {},
   "source": [
    "* fineweb-edu\n",
    "* mfu calc\n",
    "* wandb logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e117625-0d57-4be1-8543-da491957c46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.28.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken tqdm datasets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b100d83e-dba3-43fd-9386-1fc1d9eacf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random, numpy as np\n",
    "\n",
    "vocab_size = 50_272\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "block_size = 128 \n",
    "learning_rate = 3e-4\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# GPUs\n",
    "gpus = {\n",
    "    \"M3\": [4.1 * 10**12, \"mps\"],\n",
    "    \"4090\": [82 * 10**12, \"cuda\"],\n",
    "}\n",
    "gpu = \"4090\"\n",
    "flops_promised, device = gpus[gpu]\n",
    "\n",
    "# device = \"mps\"\n",
    "# flops_promised = 4.1 * 10**12 if device == \"mps\" else 82 * 10**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7d9c4a-630c-40e0-94e4-86cbc08c0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:40<00:00, 2440.92it/s]\n",
      "100%|██████████| 100000/100000 [00:39<00:00, 2529.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import torch \n",
    "import datasets\n",
    "import random\n",
    "\n",
    "dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\"], split=\"train\")\n",
    "dataset = dataset.train_test_split()\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "def encode(string):\n",
    "    return torch.tensor(enc.encode(string), dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return enc.decode(tensor.cpu().squeeze().numpy())\n",
    "\n",
    "num_samples = 100_000\n",
    "dataset_tok_train = torch.cat([encode(dataset[\"train\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "dataset_tok_test = torch.cat([encode(dataset[\"test\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "\n",
    "def get_sample(split, sample_length, batch_size):\n",
    "    tokens = dataset_tok_train if split == \"train\" else dataset_tok_test\n",
    "    idcs = torch.randint(len(tokens)-sample_length, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
    "    y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869a87db-8bba-4726-aaea-0ce5603f8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 105,042,560 tokens\n",
      "Test data: 104,150,265 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data: {len(dataset_tok_train):,} tokens\")\n",
    "print(f\"Test data: {len(dataset_tok_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5a8b7d-7ece-41bc-89fe-443d5f5d1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        # out = F.scaled_dot_product_attention(q, k, v)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def calculate_mfu(self, dt):\n",
    "        flops_achieved = flops_per_fwdbwd / dt\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        # print(f\"{flops_achieved/10**12} TFLOPS achieved, {mfu * 100:.2f}% MFU\")\n",
    "        return mfu      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4112aec8-187a-445e-9cbb-dc953c0afc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.35M parameter model on devicecuda with 82.0 TFLOPS promised \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgerald-stampfel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20241103_205309-9h40yt1r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gerald-stampfel/minimal-llm/runs/9h40yt1r' target=\"_blank\">GPT2-49.3M-BS-128-105.0MT-4090</a></strong> to <a href='https://wandb.ai/gerald-stampfel/minimal-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gerald-stampfel/minimal-llm' target=\"_blank\">https://wandb.ai/gerald-stampfel/minimal-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gerald-stampfel/minimal-llm/runs/9h40yt1r' target=\"_blank\">https://wandb.ai/gerald-stampfel/minimal-llm/runs/9h40yt1r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No relevant files were detected in the specified directory. No code will be logged to your run.\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "num_params = sum(p.numel() for p in m.parameters())/1e6\n",
    "print(f\"{num_params:.2f}M parameter model on device{device} with {flops_promised / 10**12:,} TFLOPS promised \" )\n",
    "\n",
    "import wandb \n",
    "wandb.init(\n",
    "    project=\"minimal-llm\",\n",
    "    name=f\"GPT2-{num_params:.1f}M-BS-{batch_size}-{len(dataset_tok_train)/10**6:,.1f}MT-{gpu}\"\n",
    ").log_code(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b8522f-1db1-43ad-8aa6-55a0103ce704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2013/1485170168.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_2013/1485170168.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,999,363,567,616 flops per fwd+bwd pass\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "def get_flops(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    return flop_counter.get_total_flops() \n",
    "\n",
    "def train_one_sample():\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "flops_per_fwdbwd = get_flops(train_one_sample)\n",
    "print(f\"{flops_per_fwdbwd:,} flops per fwd+bwd pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09d994-9d8f-4a2a-8fa6-c2cf0fcb04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2013/1485170168.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/tmp/ipykernel_2013/1485170168.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 9.47 131.83ms/step 124,280tok/s (total 163,840 tok), 27.75% MFU\n",
      "step 20: loss 8.51 132.87ms/step 123,310tok/s (total 327,680 tok), 27.53% MFU\n",
      "step 30: loss 7.96 134.79ms/step 121,554tok/s (total 491,520 tok), 27.14% MFU\n",
      "step 40: loss 7.70 133.01ms/step 123,176tok/s (total 655,360 tok), 27.50% MFU\n",
      "step 50: loss 7.62 130.88ms/step 125,185tok/s (total 819,200 tok), 27.95% MFU\n",
      "step 50 eval: val_loss 7.63\n",
      "step 60: loss 7.53 132.50ms/step 123,654tok/s (total 983,040 tok), 27.61% MFU\n",
      "step 70: loss 7.49 134.74ms/step 121,600tok/s (total 1,146,880 tok), 27.15% MFU\n",
      "step 80: loss 7.45 133.47ms/step 122,757tok/s (total 1,310,720 tok), 27.41% MFU\n",
      "step 90: loss 7.27 132.04ms/step 124,087tok/s (total 1,474,560 tok), 27.70% MFU\n",
      "step 100: loss 7.28 133.21ms/step 122,994tok/s (total 1,638,400 tok), 27.46% MFU\n",
      "step 100 eval: val_loss 7.16\n",
      "step 110: loss 7.12 133.27ms/step 122,940tok/s (total 1,802,240 tok), 27.45% MFU\n",
      "step 120: loss 7.09 134.12ms/step 122,157tok/s (total 1,966,080 tok), 27.27% MFU\n",
      "step 130: loss 7.01 133.58ms/step 122,655tok/s (total 2,129,920 tok), 27.38% MFU\n",
      "step 140: loss 6.91 132.49ms/step 123,663tok/s (total 2,293,760 tok), 27.61% MFU\n",
      "step 150: loss 6.93 134.00ms/step 122,266tok/s (total 2,457,600 tok), 27.30% MFU\n",
      "step 150 eval: val_loss 6.87\n",
      "step 160: loss 6.85 133.75ms/step 122,497tok/s (total 2,621,440 tok), 27.35% MFU\n",
      "step 170: loss 6.83 133.72ms/step 122,524tok/s (total 2,785,280 tok), 27.35% MFU\n",
      "step 180: loss 6.71 133.46ms/step 122,760tok/s (total 2,949,120 tok), 27.41% MFU\n",
      "step 190: loss 6.61 133.92ms/step 122,343tok/s (total 3,112,960 tok), 27.31% MFU\n",
      "step 200: loss 6.67 134.24ms/step 122,052tok/s (total 3,276,800 tok), 27.25% MFU\n",
      "step 200 eval: val_loss 6.61\n",
      "step 210: loss 6.64 131.99ms/step 124,128tok/s (total 3,440,640 tok), 27.71% MFU\n",
      "step 220: loss 6.60 133.53ms/step 122,697tok/s (total 3,604,480 tok), 27.39% MFU\n",
      "step 230: loss 6.59 134.18ms/step 122,101tok/s (total 3,768,320 tok), 27.26% MFU\n",
      "step 240: loss 6.61 134.05ms/step 122,221tok/s (total 3,932,160 tok), 27.29% MFU\n",
      "step 250: loss 6.51 134.42ms/step 121,883tok/s (total 4,096,000 tok), 27.21% MFU\n",
      "step 250 eval: val_loss 6.53\n",
      "step 260: loss 6.52 133.30ms/step 122,914tok/s (total 4,259,840 tok), 27.44% MFU\n",
      "step 270: loss 6.46 133.00ms/step 123,188tok/s (total 4,423,680 tok), 27.50% MFU\n",
      "step 280: loss 6.46 129.55ms/step 126,472tok/s (total 4,587,520 tok), 28.24% MFU\n",
      "step 290: loss 6.49 131.73ms/step 124,376tok/s (total 4,751,360 tok), 27.77% MFU\n",
      "step 300: loss 6.43 134.59ms/step 121,737tok/s (total 4,915,200 tok), 27.18% MFU\n",
      "step 300 eval: val_loss 6.48\n",
      "step 310: loss 6.43 133.60ms/step 122,636tok/s (total 5,079,040 tok), 27.38% MFU\n",
      "step 320: loss 6.46 130.11ms/step 125,921tok/s (total 5,242,880 tok), 28.11% MFU\n",
      "step 330: loss 6.44 130.02ms/step 126,012tok/s (total 5,406,720 tok), 28.13% MFU\n",
      "step 340: loss 6.38 132.85ms/step 123,324tok/s (total 5,570,560 tok), 27.53% MFU\n",
      "step 350: loss 6.37 131.81ms/step 124,300tok/s (total 5,734,400 tok), 27.75% MFU\n",
      "step 350 eval: val_loss 6.34\n",
      "step 360: loss 6.32 134.77ms/step 121,569tok/s (total 5,898,240 tok), 27.14% MFU\n",
      "step 370: loss 6.32 133.98ms/step 122,291tok/s (total 6,062,080 tok), 27.30% MFU\n",
      "step 380: loss 6.30 134.23ms/step 122,063tok/s (total 6,225,920 tok), 27.25% MFU\n",
      "step 390: loss 6.29 131.34ms/step 124,744tok/s (total 6,389,760 tok), 27.85% MFU\n",
      "step 400: loss 6.25 130.32ms/step 125,720tok/s (total 6,553,600 tok), 28.07% MFU\n",
      "step 400 eval: val_loss 6.21\n",
      "step 410: loss 6.27 130.24ms/step 125,794tok/s (total 6,717,440 tok), 28.08% MFU\n",
      "step 420: loss 6.25 131.57ms/step 124,523tok/s (total 6,881,280 tok), 27.80% MFU\n",
      "step 430: loss 6.27 131.00ms/step 125,068tok/s (total 7,045,120 tok), 27.92% MFU\n",
      "step 440: loss 6.24 134.55ms/step 121,767tok/s (total 7,208,960 tok), 27.18% MFU\n",
      "step 450: loss 6.20 132.34ms/step 123,803tok/s (total 7,372,800 tok), 27.64% MFU\n",
      "step 450 eval: val_loss 6.23\n",
      "step 460: loss 6.20 132.14ms/step 123,994tok/s (total 7,536,640 tok), 27.68% MFU\n",
      "step 470: loss 6.13 131.20ms/step 124,876tok/s (total 7,700,480 tok), 27.88% MFU\n",
      "step 480: loss 6.15 131.01ms/step 125,056tok/s (total 7,864,320 tok), 27.92% MFU\n",
      "step 490: loss 6.15 132.68ms/step 123,482tok/s (total 8,028,160 tok), 27.57% MFU\n",
      "step 500: loss 6.12 131.15ms/step 124,929tok/s (total 8,192,000 tok), 27.89% MFU\n",
      "step 500 eval: val_loss 6.07\n",
      "step 510: loss 6.06 132.31ms/step 123,826tok/s (total 8,355,840 tok), 27.64% MFU\n",
      "step 520: loss 6.10 133.57ms/step 122,658tok/s (total 8,519,680 tok), 27.38% MFU\n",
      "step 530: loss 6.03 132.12ms/step 124,011tok/s (total 8,683,520 tok), 27.69% MFU\n",
      "step 540: loss 6.14 134.40ms/step 121,902tok/s (total 8,847,360 tok), 27.21% MFU\n",
      "step 550: loss 6.03 132.56ms/step 123,597tok/s (total 9,011,200 tok), 27.59% MFU\n",
      "step 550 eval: val_loss 6.01\n",
      "step 560: loss 5.99 133.91ms/step 122,347tok/s (total 9,175,040 tok), 27.31% MFU\n",
      "step 570: loss 6.09 133.99ms/step 122,281tok/s (total 9,338,880 tok), 27.30% MFU\n",
      "step 580: loss 6.04 133.98ms/step 122,288tok/s (total 9,502,720 tok), 27.30% MFU\n",
      "step 590: loss 5.97 134.32ms/step 121,974tok/s (total 9,666,560 tok), 27.23% MFU\n",
      "step 600: loss 6.01 134.61ms/step 121,715tok/s (total 9,830,400 tok), 27.17% MFU\n",
      "step 600 eval: val_loss 5.99\n",
      "step 610: loss 6.10 133.03ms/step 123,162tok/s (total 9,994,240 tok), 27.50% MFU\n",
      "step 620: loss 5.97 133.56ms/step 122,671tok/s (total 10,158,080 tok), 27.39% MFU\n",
      "step 630: loss 5.99 133.87ms/step 122,383tok/s (total 10,321,920 tok), 27.32% MFU\n",
      "step 640: loss 5.90 132.36ms/step 123,788tok/s (total 10,485,760 tok), 27.64% MFU\n",
      "step 650: loss 5.99 134.02ms/step 122,248tok/s (total 10,649,600 tok), 27.29% MFU\n",
      "step 650 eval: val_loss 5.94\n",
      "step 660: loss 5.98 134.04ms/step 122,232tok/s (total 10,813,440 tok), 27.29% MFU\n",
      "step 670: loss 5.92 132.69ms/step 123,476tok/s (total 10,977,280 tok), 27.57% MFU\n",
      "step 680: loss 5.95 134.35ms/step 121,952tok/s (total 11,141,120 tok), 27.23% MFU\n",
      "step 690: loss 5.96 134.41ms/step 121,899tok/s (total 11,304,960 tok), 27.21% MFU\n",
      "step 700: loss 5.86 134.87ms/step 121,477tok/s (total 11,468,800 tok), 27.12% MFU\n",
      "step 700 eval: val_loss 5.82\n",
      "step 710: loss 5.79 134.87ms/step 121,482tok/s (total 11,632,640 tok), 27.12% MFU\n",
      "step 720: loss 5.86 133.85ms/step 122,405tok/s (total 11,796,480 tok), 27.33% MFU\n",
      "step 730: loss 5.83 132.44ms/step 123,712tok/s (total 11,960,320 tok), 27.62% MFU\n",
      "step 740: loss 5.80 132.06ms/step 124,067tok/s (total 12,124,160 tok), 27.70% MFU\n",
      "step 750: loss 5.86 134.27ms/step 122,027tok/s (total 12,288,000 tok), 27.24% MFU\n",
      "step 750 eval: val_loss 5.79\n",
      "step 760: loss 5.79 130.43ms/step 125,619tok/s (total 12,451,840 tok), 28.04% MFU\n",
      "step 770: loss 5.71 132.04ms/step 124,080tok/s (total 12,615,680 tok), 27.70% MFU\n",
      "step 780: loss 5.72 134.43ms/step 121,878tok/s (total 12,779,520 tok), 27.21% MFU\n",
      "step 790: loss 5.83 134.12ms/step 122,158tok/s (total 12,943,360 tok), 27.27% MFU\n",
      "step 800: loss 5.78 133.53ms/step 122,695tok/s (total 13,107,200 tok), 27.39% MFU\n",
      "step 800 eval: val_loss 5.79\n",
      "step 810: loss 5.70 134.15ms/step 122,128tok/s (total 13,271,040 tok), 27.27% MFU\n",
      "step 820: loss 5.80 132.06ms/step 124,067tok/s (total 13,434,880 tok), 27.70% MFU\n",
      "step 830: loss 5.74 134.92ms/step 121,432tok/s (total 13,598,720 tok), 27.11% MFU\n",
      "step 840: loss 5.77 134.99ms/step 121,371tok/s (total 13,762,560 tok), 27.10% MFU\n",
      "step 850: loss 5.74 134.19ms/step 122,099tok/s (total 13,926,400 tok), 27.26% MFU\n",
      "step 850 eval: val_loss 5.65\n",
      "step 860: loss 5.66 133.58ms/step 122,652tok/s (total 14,090,240 tok), 27.38% MFU\n",
      "step 870: loss 5.74 133.42ms/step 122,801tok/s (total 14,254,080 tok), 27.42% MFU\n",
      "step 880: loss 5.73 132.65ms/step 123,510tok/s (total 14,417,920 tok), 27.57% MFU\n",
      "step 890: loss 5.73 134.97ms/step 121,388tok/s (total 14,581,760 tok), 27.10% MFU\n",
      "step 900: loss 5.73 132.71ms/step 123,453tok/s (total 14,745,600 tok), 27.56% MFU\n",
      "step 900 eval: val_loss 5.63\n",
      "step 910: loss 5.63 134.87ms/step 121,484tok/s (total 14,909,440 tok), 27.12% MFU\n",
      "step 920: loss 5.69 134.00ms/step 122,269tok/s (total 15,073,280 tok), 27.30% MFU\n",
      "step 930: loss 5.75 133.96ms/step 122,305tok/s (total 15,237,120 tok), 27.30% MFU\n",
      "step 940: loss 5.65 134.14ms/step 122,145tok/s (total 15,400,960 tok), 27.27% MFU\n",
      "step 950: loss 5.56 134.42ms/step 121,886tok/s (total 15,564,800 tok), 27.21% MFU\n",
      "step 950 eval: val_loss 5.56\n",
      "step 960: loss 5.60 134.73ms/step 121,605tok/s (total 15,728,640 tok), 27.15% MFU\n",
      "step 970: loss 5.58 136.21ms/step 120,283tok/s (total 15,892,480 tok), 26.85% MFU\n",
      "step 980: loss 5.73 135.63ms/step 120,800tok/s (total 16,056,320 tok), 26.97% MFU\n",
      "step 990: loss 5.61 136.76ms/step 119,800tok/s (total 16,220,160 tok), 26.75% MFU\n",
      "step 1000: loss 5.63 134.56ms/step 121,764tok/s (total 16,384,000 tok), 27.18% MFU\n",
      "step 1000 eval: val_loss 5.55\n",
      "step 1010: loss 5.72 135.61ms/step 120,814tok/s (total 16,547,840 tok), 26.97% MFU\n",
      "step 1020: loss 5.55 136.21ms/step 120,287tok/s (total 16,711,680 tok), 26.85% MFU\n",
      "step 1030: loss 5.58 137.19ms/step 119,424tok/s (total 16,875,520 tok), 26.66% MFU\n",
      "step 1040: loss 5.66 136.47ms/step 120,054tok/s (total 17,039,360 tok), 26.80% MFU\n",
      "step 1050: loss 5.62 134.63ms/step 121,696tok/s (total 17,203,200 tok), 27.17% MFU\n",
      "step 1050 eval: val_loss 5.54\n",
      "step 1060: loss 5.68 136.73ms/step 119,828tok/s (total 17,367,040 tok), 26.75% MFU\n",
      "step 1070: loss 5.63 135.45ms/step 120,963tok/s (total 17,530,880 tok), 27.01% MFU\n",
      "step 1080: loss 5.58 133.90ms/step 122,358tok/s (total 17,694,720 tok), 27.32% MFU\n",
      "step 1090: loss 5.55 134.71ms/step 121,623tok/s (total 17,858,560 tok), 27.15% MFU\n",
      "step 1100: loss 5.67 136.89ms/step 119,691tok/s (total 18,022,400 tok), 26.72% MFU\n",
      "step 1100 eval: val_loss 5.49\n",
      "step 1110: loss 5.56 134.95ms/step 121,412tok/s (total 18,186,240 tok), 27.11% MFU\n",
      "step 1120: loss 5.56 134.58ms/step 121,742tok/s (total 18,350,080 tok), 27.18% MFU\n",
      "step 1130: loss 5.51 135.83ms/step 120,618tok/s (total 18,513,920 tok), 26.93% MFU\n",
      "step 1140: loss 5.50 136.28ms/step 120,222tok/s (total 18,677,760 tok), 26.84% MFU\n",
      "step 1150: loss 5.49 136.34ms/step 120,166tok/s (total 18,841,600 tok), 26.83% MFU\n",
      "step 1150 eval: val_loss 5.47\n",
      "step 1160: loss 5.58 136.04ms/step 120,439tok/s (total 19,005,440 tok), 26.89% MFU\n",
      "step 1170: loss 5.51 135.96ms/step 120,505tok/s (total 19,169,280 tok), 26.90% MFU\n",
      "step 1180: loss 5.54 135.61ms/step 120,819tok/s (total 19,333,120 tok), 26.97% MFU\n",
      "step 1190: loss 5.51 134.61ms/step 121,712tok/s (total 19,496,960 tok), 27.17% MFU\n",
      "step 1200: loss 5.45 133.66ms/step 122,583tok/s (total 19,660,800 tok), 27.37% MFU\n",
      "step 1200 eval: val_loss 5.44\n",
      "step 1210: loss 5.46 135.98ms/step 120,491tok/s (total 19,824,640 tok), 26.90% MFU\n",
      "step 1220: loss 5.51 132.95ms/step 123,232tok/s (total 19,988,480 tok), 27.51% MFU\n",
      "step 1230: loss 5.47 134.11ms/step 122,171tok/s (total 20,152,320 tok), 27.27% MFU\n",
      "step 1240: loss 5.52 135.15ms/step 121,227tok/s (total 20,316,160 tok), 27.06% MFU\n",
      "step 1250: loss 5.46 132.96ms/step 123,228tok/s (total 20,480,000 tok), 27.51% MFU\n",
      "step 1250 eval: val_loss 5.38\n",
      "step 1260: loss 5.45 136.08ms/step 120,401tok/s (total 20,643,840 tok), 26.88% MFU\n",
      "step 1270: loss 5.54 136.34ms/step 120,171tok/s (total 20,807,680 tok), 26.83% MFU\n",
      "step 1280: loss 5.45 134.51ms/step 121,807tok/s (total 20,971,520 tok), 27.19% MFU\n",
      "step 1290: loss 5.43 134.79ms/step 121,552tok/s (total 21,135,360 tok), 27.14% MFU\n",
      "step 1300: loss 5.59 136.72ms/step 119,833tok/s (total 21,299,200 tok), 26.75% MFU\n",
      "step 1300 eval: val_loss 5.40\n",
      "step 1310: loss 5.40 135.42ms/step 120,985tok/s (total 21,463,040 tok), 27.01% MFU\n",
      "step 1320: loss 5.57 133.08ms/step 123,116tok/s (total 21,626,880 tok), 27.49% MFU\n",
      "step 1330: loss 5.47 135.02ms/step 121,347tok/s (total 21,790,720 tok), 27.09% MFU\n",
      "step 1340: loss 5.36 135.00ms/step 121,362tok/s (total 21,954,560 tok), 27.09% MFU\n",
      "step 1350: loss 5.42 135.14ms/step 121,238tok/s (total 22,118,400 tok), 27.07% MFU\n",
      "step 1350 eval: val_loss 5.30\n",
      "step 1360: loss 5.46 133.46ms/step 122,761tok/s (total 22,282,240 tok), 27.41% MFU\n",
      "step 1370: loss 5.37 133.57ms/step 122,663tok/s (total 22,446,080 tok), 27.38% MFU\n",
      "step 1380: loss 5.40 133.83ms/step 122,423tok/s (total 22,609,920 tok), 27.33% MFU\n",
      "step 1390: loss 5.42 134.26ms/step 122,032tok/s (total 22,773,760 tok), 27.24% MFU\n",
      "step 1400: loss 5.34 134.27ms/step 122,025tok/s (total 22,937,600 tok), 27.24% MFU\n",
      "step 1400 eval: val_loss 5.34\n",
      "step 1410: loss 5.39 132.31ms/step 123,831tok/s (total 23,101,440 tok), 27.65% MFU\n",
      "step 1420: loss 5.27 133.35ms/step 122,862tok/s (total 23,265,280 tok), 27.43% MFU\n",
      "step 1430: loss 5.39 133.51ms/step 122,720tok/s (total 23,429,120 tok), 27.40% MFU\n",
      "step 1440: loss 5.39 133.75ms/step 122,494tok/s (total 23,592,960 tok), 27.35% MFU\n",
      "step 1450: loss 5.37 134.11ms/step 122,173tok/s (total 23,756,800 tok), 27.28% MFU\n",
      "step 1450 eval: val_loss 5.34\n",
      "step 1460: loss 5.40 134.43ms/step 121,874tok/s (total 23,920,640 tok), 27.21% MFU\n",
      "step 1470: loss 5.35 132.83ms/step 123,347tok/s (total 24,084,480 tok), 27.54% MFU\n",
      "step 1480: loss 5.35 134.24ms/step 122,047tok/s (total 24,248,320 tok), 27.25% MFU\n",
      "step 1490: loss 5.36 133.29ms/step 122,923tok/s (total 24,412,160 tok), 27.44% MFU\n",
      "step 1500: loss 5.29 133.37ms/step 122,848tok/s (total 24,576,000 tok), 27.43% MFU\n",
      "step 1500 eval: val_loss 5.34\n",
      "step 1510: loss 5.34 134.80ms/step 121,546tok/s (total 24,739,840 tok), 27.14% MFU\n",
      "step 1520: loss 5.31 134.49ms/step 121,819tok/s (total 24,903,680 tok), 27.20% MFU\n",
      "step 1530: loss 5.35 134.80ms/step 121,543tok/s (total 25,067,520 tok), 27.13% MFU\n",
      "step 1540: loss 5.27 135.18ms/step 121,199tok/s (total 25,231,360 tok), 27.06% MFU\n",
      "step 1550: loss 5.20 133.59ms/step 122,646tok/s (total 25,395,200 tok), 27.38% MFU\n",
      "step 1550 eval: val_loss 5.32\n",
      "step 1560: loss 5.30 133.31ms/step 122,900tok/s (total 25,559,040 tok), 27.44% MFU\n",
      "step 1570: loss 5.36 133.78ms/step 122,469tok/s (total 25,722,880 tok), 27.34% MFU\n",
      "step 1580: loss 5.30 134.11ms/step 122,168tok/s (total 25,886,720 tok), 27.27% MFU\n",
      "step 1590: loss 5.27 134.47ms/step 121,842tok/s (total 26,050,560 tok), 27.20% MFU\n",
      "step 1600: loss 5.32 134.10ms/step 122,178tok/s (total 26,214,400 tok), 27.28% MFU\n",
      "step 1600 eval: val_loss 5.26\n",
      "step 1610: loss 5.38 135.35ms/step 121,050tok/s (total 26,378,240 tok), 27.02% MFU\n",
      "step 1620: loss 5.26 132.88ms/step 123,297tok/s (total 26,542,080 tok), 27.53% MFU\n",
      "step 1630: loss 5.27 134.89ms/step 121,466tok/s (total 26,705,920 tok), 27.12% MFU\n",
      "step 1640: loss 5.35 134.91ms/step 121,448tok/s (total 26,869,760 tok), 27.11% MFU\n",
      "step 1650: loss 5.32 135.04ms/step 121,326tok/s (total 27,033,600 tok), 27.09% MFU\n",
      "step 1650 eval: val_loss 5.23\n",
      "step 1660: loss 5.35 134.39ms/step 121,915tok/s (total 27,197,440 tok), 27.22% MFU\n",
      "step 1670: loss 5.29 134.50ms/step 121,810tok/s (total 27,361,280 tok), 27.19% MFU\n",
      "step 1680: loss 5.35 134.30ms/step 121,994tok/s (total 27,525,120 tok), 27.24% MFU\n",
      "step 1690: loss 5.30 134.55ms/step 121,770tok/s (total 27,688,960 tok), 27.19% MFU\n",
      "step 1700: loss 5.22 134.99ms/step 121,371tok/s (total 27,852,800 tok), 27.10% MFU\n",
      "step 1700 eval: val_loss 5.21\n",
      "step 1710: loss 5.26 133.66ms/step 122,580tok/s (total 28,016,640 tok), 27.37% MFU\n",
      "step 1720: loss 5.29 133.32ms/step 122,890tok/s (total 28,180,480 tok), 27.44% MFU\n",
      "step 1730: loss 5.33 131.29ms/step 124,794tok/s (total 28,344,320 tok), 27.86% MFU\n",
      "step 1740: loss 5.27 133.67ms/step 122,568tok/s (total 28,508,160 tok), 27.36% MFU\n",
      "step 1750: loss 5.21 134.15ms/step 122,130tok/s (total 28,672,000 tok), 27.27% MFU\n",
      "step 1750 eval: val_loss 5.23\n",
      "step 1760: loss 5.33 134.51ms/step 121,801tok/s (total 28,835,840 tok), 27.19% MFU\n",
      "step 1770: loss 5.22 134.91ms/step 121,444tok/s (total 28,999,680 tok), 27.11% MFU\n",
      "step 1780: loss 5.22 135.39ms/step 121,012tok/s (total 29,163,520 tok), 27.02% MFU\n",
      "step 1790: loss 5.21 133.43ms/step 122,791tok/s (total 29,327,360 tok), 27.41% MFU\n",
      "step 1800: loss 5.32 133.50ms/step 122,730tok/s (total 29,491,200 tok), 27.40% MFU\n",
      "step 1800 eval: val_loss 5.14\n",
      "step 1810: loss 5.20 134.82ms/step 121,528tok/s (total 29,655,040 tok), 27.13% MFU\n",
      "step 1820: loss 5.15 134.32ms/step 121,973tok/s (total 29,818,880 tok), 27.23% MFU\n",
      "step 1830: loss 5.28 134.81ms/step 121,536tok/s (total 29,982,720 tok), 27.13% MFU\n",
      "step 1840: loss 5.22 135.38ms/step 121,026tok/s (total 30,146,560 tok), 27.02% MFU\n",
      "step 1850: loss 5.16 133.93ms/step 122,331tok/s (total 30,310,400 tok), 27.31% MFU\n",
      "step 1850 eval: val_loss 5.20\n",
      "step 1860: loss 5.27 133.75ms/step 122,502tok/s (total 30,474,240 tok), 27.35% MFU\n",
      "step 1870: loss 5.26 134.23ms/step 122,062tok/s (total 30,638,080 tok), 27.25% MFU\n",
      "step 1880: loss 5.22 134.28ms/step 122,009tok/s (total 30,801,920 tok), 27.24% MFU\n",
      "step 1890: loss 5.22 134.73ms/step 121,603tok/s (total 30,965,760 tok), 27.15% MFU\n",
      "step 1900: loss 5.24 134.11ms/step 122,170tok/s (total 31,129,600 tok), 27.27% MFU\n",
      "step 1900 eval: val_loss 5.10\n",
      "step 1910: loss 5.18 133.74ms/step 122,506tok/s (total 31,293,440 tok), 27.35% MFU\n",
      "step 1920: loss 5.23 134.13ms/step 122,153tok/s (total 31,457,280 tok), 27.27% MFU\n",
      "step 1930: loss 5.28 133.02ms/step 123,168tok/s (total 31,621,120 tok), 27.50% MFU\n",
      "step 1940: loss 5.18 134.37ms/step 121,931tok/s (total 31,784,960 tok), 27.22% MFU\n",
      "step 1950: loss 5.17 135.33ms/step 121,067tok/s (total 31,948,800 tok), 27.03% MFU\n",
      "step 1950 eval: val_loss 5.10\n",
      "step 1960: loss 5.23 132.69ms/step 123,476tok/s (total 32,112,640 tok), 27.57% MFU\n",
      "step 1970: loss 5.20 132.92ms/step 123,266tok/s (total 32,276,480 tok), 27.52% MFU\n",
      "step 1980: loss 5.18 133.10ms/step 123,097tok/s (total 32,440,320 tok), 27.48% MFU\n",
      "step 1990: loss 5.21 134.55ms/step 121,767tok/s (total 32,604,160 tok), 27.18% MFU\n",
      "step 2000: loss 5.19 133.86ms/step 122,393tok/s (total 32,768,000 tok), 27.32% MFU\n",
      "step 2000 eval: val_loss 5.18\n",
      "step 2010: loss 5.26 134.60ms/step 121,727tok/s (total 32,931,840 tok), 27.18% MFU\n",
      "step 2020: loss 5.16 134.74ms/step 121,597tok/s (total 33,095,680 tok), 27.15% MFU\n",
      "step 2030: loss 5.14 134.74ms/step 121,599tok/s (total 33,259,520 tok), 27.15% MFU\n",
      "step 2040: loss 5.20 133.35ms/step 122,863tok/s (total 33,423,360 tok), 27.43% MFU\n",
      "step 2050: loss 5.10 132.74ms/step 123,431tok/s (total 33,587,200 tok), 27.56% MFU\n",
      "step 2050 eval: val_loss 5.20\n",
      "step 2060: loss 5.12 134.41ms/step 121,897tok/s (total 33,751,040 tok), 27.21% MFU\n",
      "step 2070: loss 5.15 134.62ms/step 121,703tok/s (total 33,914,880 tok), 27.17% MFU\n",
      "step 2080: loss 5.21 134.80ms/step 121,540tok/s (total 34,078,720 tok), 27.13% MFU\n",
      "step 2090: loss 5.15 135.10ms/step 121,273tok/s (total 34,242,560 tok), 27.07% MFU\n",
      "step 2100: loss 5.18 133.52ms/step 122,705tok/s (total 34,406,400 tok), 27.39% MFU\n",
      "step 2100 eval: val_loss 5.11\n",
      "step 2110: loss 5.15 132.64ms/step 123,521tok/s (total 34,570,240 tok), 27.58% MFU\n",
      "step 2120: loss 5.20 134.68ms/step 121,656tok/s (total 34,734,080 tok), 27.16% MFU\n",
      "step 2130: loss 5.18 135.14ms/step 121,236tok/s (total 34,897,920 tok), 27.07% MFU\n",
      "step 2140: loss 5.21 132.61ms/step 123,554tok/s (total 35,061,760 tok), 27.58% MFU\n",
      "step 2150: loss 5.07 132.84ms/step 123,336tok/s (total 35,225,600 tok), 27.54% MFU\n",
      "step 2150 eval: val_loss 5.06\n",
      "step 2160: loss 5.11 133.75ms/step 122,501tok/s (total 35,389,440 tok), 27.35% MFU\n",
      "step 2170: loss 5.16 133.63ms/step 122,608tok/s (total 35,553,280 tok), 27.37% MFU\n",
      "step 2180: loss 5.14 132.86ms/step 123,319tok/s (total 35,717,120 tok), 27.53% MFU\n",
      "step 2190: loss 5.09 133.88ms/step 122,374tok/s (total 35,880,960 tok), 27.32% MFU\n",
      "step 2200: loss 5.14 134.35ms/step 121,952tok/s (total 36,044,800 tok), 27.23% MFU\n",
      "step 2200 eval: val_loss 5.05\n",
      "step 2210: loss 5.03 134.70ms/step 121,634tok/s (total 36,208,640 tok), 27.16% MFU\n",
      "step 2220: loss 5.09 133.89ms/step 122,371tok/s (total 36,372,480 tok), 27.32% MFU\n",
      "step 2230: loss 5.11 133.95ms/step 122,319tok/s (total 36,536,320 tok), 27.31% MFU\n",
      "step 2240: loss 5.12 133.94ms/step 122,319tok/s (total 36,700,160 tok), 27.31% MFU\n",
      "step 2250: loss 5.19 134.10ms/step 122,179tok/s (total 36,864,000 tok), 27.28% MFU\n",
      "step 2250 eval: val_loss 5.14\n",
      "step 2260: loss 5.18 134.92ms/step 121,432tok/s (total 37,027,840 tok), 27.11% MFU\n",
      "step 2270: loss 5.09 135.10ms/step 121,272tok/s (total 37,191,680 tok), 27.07% MFU\n",
      "step 2280: loss 5.14 135.41ms/step 120,999tok/s (total 37,355,520 tok), 27.01% MFU\n",
      "step 2290: loss 5.16 133.01ms/step 123,175tok/s (total 37,519,360 tok), 27.50% MFU\n",
      "step 2300: loss 5.16 131.26ms/step 124,817tok/s (total 37,683,200 tok), 27.87% MFU\n",
      "step 2300 eval: val_loss 4.98\n",
      "step 2310: loss 5.01 132.22ms/step 123,914tok/s (total 37,847,040 tok), 27.66% MFU\n",
      "step 2320: loss 5.09 132.75ms/step 123,420tok/s (total 38,010,880 tok), 27.55% MFU\n",
      "step 2330: loss 5.12 134.40ms/step 121,901tok/s (total 38,174,720 tok), 27.21% MFU\n",
      "step 2340: loss 5.15 134.59ms/step 121,736tok/s (total 38,338,560 tok), 27.18% MFU\n",
      "step 2350: loss 5.15 135.11ms/step 121,261tok/s (total 38,502,400 tok), 27.07% MFU\n",
      "step 2350 eval: val_loss 5.09\n",
      "step 2360: loss 5.17 134.51ms/step 121,804tok/s (total 38,666,240 tok), 27.19% MFU\n",
      "step 2370: loss 5.15 133.97ms/step 122,296tok/s (total 38,830,080 tok), 27.30% MFU\n",
      "step 2380: loss 5.10 132.21ms/step 123,925tok/s (total 38,993,920 tok), 27.67% MFU\n",
      "step 2390: loss 5.10 134.07ms/step 122,205tok/s (total 39,157,760 tok), 27.28% MFU\n",
      "step 2400: loss 5.09 135.83ms/step 120,622tok/s (total 39,321,600 tok), 26.93% MFU\n",
      "step 2400 eval: val_loss 4.99\n",
      "step 2410: loss 5.06 135.29ms/step 121,100tok/s (total 39,485,440 tok), 27.04% MFU\n",
      "step 2420: loss 5.05 135.04ms/step 121,325tok/s (total 39,649,280 tok), 27.09% MFU\n",
      "step 2430: loss 5.15 133.57ms/step 122,666tok/s (total 39,813,120 tok), 27.39% MFU\n",
      "step 2440: loss 5.03 134.02ms/step 122,253tok/s (total 39,976,960 tok), 27.29% MFU\n",
      "step 2450: loss 5.01 134.45ms/step 121,862tok/s (total 40,140,800 tok), 27.21% MFU\n",
      "step 2450 eval: val_loss 5.00\n",
      "step 2460: loss 5.00 133.28ms/step 122,932tok/s (total 40,304,640 tok), 27.44% MFU\n",
      "step 2470: loss 5.02 135.42ms/step 120,984tok/s (total 40,468,480 tok), 27.01% MFU\n",
      "step 2480: loss 4.99 133.09ms/step 123,100tok/s (total 40,632,320 tok), 27.48% MFU\n",
      "step 2490: loss 5.01 132.63ms/step 123,532tok/s (total 40,796,160 tok), 27.58% MFU\n",
      "step 2500: loss 5.10 133.66ms/step 122,582tok/s (total 40,960,000 tok), 27.37% MFU\n",
      "step 2500 eval: val_loss 5.00\n",
      "step 2510: loss 5.04 134.22ms/step 122,071tok/s (total 41,123,840 tok), 27.25% MFU\n",
      "step 2520: loss 5.10 136.69ms/step 119,861tok/s (total 41,287,680 tok), 26.76% MFU\n",
      "step 2530: loss 5.06 135.03ms/step 121,332tok/s (total 41,451,520 tok), 27.09% MFU\n",
      "step 2540: loss 5.05 133.65ms/step 122,585tok/s (total 41,615,360 tok), 27.37% MFU\n",
      "step 2550: loss 5.08 132.72ms/step 123,446tok/s (total 41,779,200 tok), 27.56% MFU\n",
      "step 2550 eval: val_loss 4.99\n",
      "step 2560: loss 5.08 134.35ms/step 121,947tok/s (total 41,943,040 tok), 27.22% MFU\n",
      "step 2570: loss 5.11 134.55ms/step 121,766tok/s (total 42,106,880 tok), 27.18% MFU\n",
      "step 2580: loss 5.03 134.96ms/step 121,396tok/s (total 42,270,720 tok), 27.10% MFU\n",
      "step 2590: loss 5.02 134.76ms/step 121,578tok/s (total 42,434,560 tok), 27.14% MFU\n",
      "step 2600: loss 5.05 134.15ms/step 122,136tok/s (total 42,598,400 tok), 27.27% MFU\n",
      "step 2600 eval: val_loss 5.03\n",
      "step 2610: loss 5.03 132.76ms/step 123,410tok/s (total 42,762,240 tok), 27.55% MFU\n",
      "step 2620: loss 5.08 132.81ms/step 123,360tok/s (total 42,926,080 tok), 27.54% MFU\n",
      "step 2630: loss 5.03 134.29ms/step 122,002tok/s (total 43,089,920 tok), 27.24% MFU\n",
      "step 2640: loss 5.08 132.69ms/step 123,476tok/s (total 43,253,760 tok), 27.57% MFU\n",
      "step 2650: loss 5.08 132.57ms/step 123,590tok/s (total 43,417,600 tok), 27.59% MFU\n",
      "step 2650 eval: val_loss 5.02\n",
      "step 2660: loss 5.02 133.61ms/step 122,623tok/s (total 43,581,440 tok), 27.38% MFU\n",
      "step 2670: loss 4.98 134.17ms/step 122,110tok/s (total 43,745,280 tok), 27.26% MFU\n",
      "step 2680: loss 5.01 134.60ms/step 121,721tok/s (total 43,909,120 tok), 27.17% MFU\n",
      "step 2690: loss 5.04 134.83ms/step 121,514tok/s (total 44,072,960 tok), 27.13% MFU\n",
      "step 2700: loss 5.00 135.27ms/step 121,124tok/s (total 44,236,800 tok), 27.04% MFU\n",
      "step 2700 eval: val_loss 4.93\n",
      "step 2710: loss 5.06 133.35ms/step 122,869tok/s (total 44,400,640 tok), 27.43% MFU\n",
      "step 2720: loss 4.96 133.68ms/step 122,565tok/s (total 44,564,480 tok), 27.36% MFU\n",
      "step 2730: loss 5.07 134.20ms/step 122,086tok/s (total 44,728,320 tok), 27.26% MFU\n",
      "step 2740: loss 5.07 135.05ms/step 121,315tok/s (total 44,892,160 tok), 27.08% MFU\n",
      "step 2750: loss 5.07 134.69ms/step 121,638tok/s (total 45,056,000 tok), 27.16% MFU\n",
      "step 2750 eval: val_loss 4.92\n",
      "step 2760: loss 4.94 134.57ms/step 121,753tok/s (total 45,219,840 tok), 27.18% MFU\n",
      "step 2770: loss 4.95 133.37ms/step 122,843tok/s (total 45,383,680 tok), 27.42% MFU\n",
      "step 2780: loss 4.92 135.48ms/step 120,933tok/s (total 45,547,520 tok), 27.00% MFU\n",
      "step 2790: loss 4.97 136.33ms/step 120,180tok/s (total 45,711,360 tok), 26.83% MFU\n",
      "step 2800: loss 4.98 134.69ms/step 121,642tok/s (total 45,875,200 tok), 27.16% MFU\n",
      "step 2800 eval: val_loss 4.95\n",
      "step 2810: loss 5.02 137.26ms/step 119,363tok/s (total 46,039,040 tok), 26.65% MFU\n",
      "step 2820: loss 4.97 134.82ms/step 121,528tok/s (total 46,202,880 tok), 27.13% MFU\n",
      "step 2830: loss 4.90 131.95ms/step 124,165tok/s (total 46,366,720 tok), 27.72% MFU\n",
      "step 2840: loss 5.05 134.79ms/step 121,553tok/s (total 46,530,560 tok), 27.14% MFU\n",
      "step 2850: loss 5.05 134.55ms/step 121,769tok/s (total 46,694,400 tok), 27.19% MFU\n",
      "step 2850 eval: val_loss 4.95\n",
      "step 2860: loss 5.01 134.33ms/step 121,970tok/s (total 46,858,240 tok), 27.23% MFU\n",
      "step 2870: loss 5.00 132.67ms/step 123,490tok/s (total 47,022,080 tok), 27.57% MFU\n",
      "step 2880: loss 5.02 134.93ms/step 121,422tok/s (total 47,185,920 tok), 27.11% MFU\n",
      "step 2890: loss 5.03 135.29ms/step 121,107tok/s (total 47,349,760 tok), 27.04% MFU\n",
      "step 2900: loss 4.97 134.16ms/step 122,126tok/s (total 47,513,600 tok), 27.26% MFU\n",
      "step 2900 eval: val_loss 4.93\n",
      "step 2910: loss 4.99 134.18ms/step 122,108tok/s (total 47,677,440 tok), 27.26% MFU\n",
      "step 2920: loss 4.92 134.50ms/step 121,813tok/s (total 47,841,280 tok), 27.19% MFU\n",
      "step 2930: loss 4.96 136.61ms/step 119,934tok/s (total 48,005,120 tok), 26.78% MFU\n",
      "step 2940: loss 4.96 132.35ms/step 123,797tok/s (total 48,168,960 tok), 27.64% MFU\n",
      "step 2950: loss 4.92 133.96ms/step 122,308tok/s (total 48,332,800 tok), 27.31% MFU\n",
      "step 2950 eval: val_loss 4.84\n",
      "step 2960: loss 4.97 135.09ms/step 121,281tok/s (total 48,496,640 tok), 27.08% MFU\n",
      "step 2970: loss 4.99 136.69ms/step 119,861tok/s (total 48,660,480 tok), 26.76% MFU\n",
      "step 2980: loss 4.94 134.42ms/step 121,885tok/s (total 48,824,320 tok), 27.21% MFU\n",
      "step 2990: loss 4.85 137.24ms/step 119,383tok/s (total 48,988,160 tok), 26.65% MFU\n",
      "step 3000: loss 4.92 133.38ms/step 122,834tok/s (total 49,152,000 tok), 27.42% MFU\n",
      "step 3000 eval: val_loss 4.81\n",
      "step 3010: loss 4.95 136.78ms/step 119,786tok/s (total 49,315,840 tok), 26.74% MFU\n",
      "step 3020: loss 5.00 134.49ms/step 121,820tok/s (total 49,479,680 tok), 27.20% MFU\n",
      "step 3030: loss 4.94 134.23ms/step 122,063tok/s (total 49,643,520 tok), 27.25% MFU\n",
      "step 3040: loss 5.03 134.72ms/step 121,612tok/s (total 49,807,360 tok), 27.15% MFU\n",
      "step 3050: loss 5.08 133.86ms/step 122,393tok/s (total 49,971,200 tok), 27.32% MFU\n",
      "step 3050 eval: val_loss 4.92\n",
      "step 3060: loss 5.00 133.21ms/step 122,989tok/s (total 50,135,040 tok), 27.46% MFU\n",
      "step 3070: loss 4.88 133.82ms/step 122,435tok/s (total 50,298,880 tok), 27.33% MFU\n",
      "step 3080: loss 4.99 134.09ms/step 122,185tok/s (total 50,462,720 tok), 27.28% MFU\n",
      "step 3090: loss 5.01 134.27ms/step 122,022tok/s (total 50,626,560 tok), 27.24% MFU\n",
      "step 3100: loss 5.02 134.31ms/step 121,986tok/s (total 50,790,400 tok), 27.23% MFU\n",
      "step 3100 eval: val_loss 4.87\n",
      "step 3110: loss 5.10 135.55ms/step 120,873tok/s (total 50,954,240 tok), 26.99% MFU\n",
      "step 3120: loss 4.95 134.68ms/step 121,649tok/s (total 51,118,080 tok), 27.16% MFU\n",
      "step 3130: loss 4.96 133.72ms/step 122,528tok/s (total 51,281,920 tok), 27.35% MFU\n",
      "step 3140: loss 4.93 134.26ms/step 122,028tok/s (total 51,445,760 tok), 27.24% MFU\n",
      "step 3150: loss 4.98 134.55ms/step 121,769tok/s (total 51,609,600 tok), 27.19% MFU\n",
      "step 3150 eval: val_loss 4.84\n",
      "step 3160: loss 4.90 134.50ms/step 121,810tok/s (total 51,773,440 tok), 27.19% MFU\n",
      "step 3170: loss 4.94 134.83ms/step 121,518tok/s (total 51,937,280 tok), 27.13% MFU\n",
      "step 3180: loss 4.93 133.44ms/step 122,778tok/s (total 52,101,120 tok), 27.41% MFU\n",
      "step 3190: loss 4.91 136.74ms/step 119,820tok/s (total 52,264,960 tok), 26.75% MFU\n",
      "step 3200: loss 4.93 136.07ms/step 120,408tok/s (total 52,428,800 tok), 26.88% MFU\n",
      "step 3200 eval: val_loss 4.87\n",
      "step 3210: loss 4.92 135.05ms/step 121,316tok/s (total 52,592,640 tok), 27.08% MFU\n",
      "step 3220: loss 4.90 134.68ms/step 121,651tok/s (total 52,756,480 tok), 27.16% MFU\n",
      "step 3230: loss 4.99 134.44ms/step 121,864tok/s (total 52,920,320 tok), 27.21% MFU\n",
      "step 3240: loss 4.91 136.27ms/step 120,232tok/s (total 53,084,160 tok), 26.84% MFU\n",
      "step 3250: loss 4.99 134.58ms/step 121,743tok/s (total 53,248,000 tok), 27.18% MFU\n",
      "step 3250 eval: val_loss 4.82\n",
      "step 3260: loss 4.96 135.21ms/step 121,178tok/s (total 53,411,840 tok), 27.05% MFU\n",
      "step 3270: loss 4.91 134.63ms/step 121,701tok/s (total 53,575,680 tok), 27.17% MFU\n",
      "step 3280: loss 4.90 134.44ms/step 121,869tok/s (total 53,739,520 tok), 27.21% MFU\n",
      "step 3290: loss 4.93 133.78ms/step 122,473tok/s (total 53,903,360 tok), 27.34% MFU\n",
      "step 3300: loss 4.88 131.65ms/step 124,454tok/s (total 54,067,200 tok), 27.78% MFU\n",
      "step 3300 eval: val_loss 4.89\n",
      "step 3310: loss 4.94 132.23ms/step 123,904tok/s (total 54,231,040 tok), 27.66% MFU\n",
      "step 3320: loss 4.87 132.15ms/step 123,982tok/s (total 54,394,880 tok), 27.68% MFU\n",
      "step 3330: loss 4.94 132.69ms/step 123,476tok/s (total 54,558,720 tok), 27.57% MFU\n",
      "step 3340: loss 4.88 134.14ms/step 122,137tok/s (total 54,722,560 tok), 27.27% MFU\n",
      "step 3350: loss 4.96 134.43ms/step 121,882tok/s (total 54,886,400 tok), 27.21% MFU\n",
      "step 3350 eval: val_loss 4.80\n",
      "step 3360: loss 4.96 134.62ms/step 121,705tok/s (total 55,050,240 tok), 27.17% MFU\n",
      "step 3370: loss 4.91 136.29ms/step 120,210tok/s (total 55,214,080 tok), 26.84% MFU\n",
      "step 3380: loss 4.90 136.45ms/step 120,071tok/s (total 55,377,920 tok), 26.81% MFU\n",
      "step 3390: loss 4.98 136.74ms/step 119,816tok/s (total 55,541,760 tok), 26.75% MFU\n",
      "step 3400: loss 4.96 136.97ms/step 119,616tok/s (total 55,705,600 tok), 26.70% MFU\n",
      "step 3400 eval: val_loss 4.89\n",
      "step 3410: loss 4.93 132.56ms/step 123,601tok/s (total 55,869,440 tok), 27.59% MFU\n",
      "step 3420: loss 4.87 134.14ms/step 122,141tok/s (total 56,033,280 tok), 27.27% MFU\n",
      "step 3430: loss 4.89 133.74ms/step 122,504tok/s (total 56,197,120 tok), 27.35% MFU\n",
      "step 3440: loss 4.92 135.32ms/step 121,073tok/s (total 56,360,960 tok), 27.03% MFU\n",
      "step 3450: loss 4.85 130.86ms/step 125,205tok/s (total 56,524,800 tok), 27.95% MFU\n",
      "step 3450 eval: val_loss 4.90\n",
      "step 3460: loss 4.86 134.43ms/step 121,877tok/s (total 56,688,640 tok), 27.21% MFU\n",
      "step 3470: loss 4.92 131.38ms/step 124,706tok/s (total 56,852,480 tok), 27.84% MFU\n",
      "step 3480: loss 4.89 132.78ms/step 123,388tok/s (total 57,016,320 tok), 27.55% MFU\n",
      "step 3490: loss 4.86 131.66ms/step 124,443tok/s (total 57,180,160 tok), 27.78% MFU\n",
      "step 3500: loss 4.92 136.32ms/step 120,190tok/s (total 57,344,000 tok), 26.83% MFU\n",
      "step 3500 eval: val_loss 4.86\n",
      "step 3510: loss 4.84 130.65ms/step 125,400tok/s (total 57,507,840 tok), 28.00% MFU\n",
      "step 3520: loss 4.87 131.36ms/step 124,723tok/s (total 57,671,680 tok), 27.84% MFU\n",
      "step 3530: loss 4.94 133.20ms/step 123,004tok/s (total 57,835,520 tok), 27.46% MFU\n",
      "step 3540: loss 4.92 130.75ms/step 125,312tok/s (total 57,999,360 tok), 27.98% MFU\n",
      "step 3550: loss 4.87 134.64ms/step 121,686tok/s (total 58,163,200 tok), 27.17% MFU\n",
      "step 3550 eval: val_loss 4.81\n",
      "step 3560: loss 4.93 134.36ms/step 121,938tok/s (total 58,327,040 tok), 27.22% MFU\n",
      "step 3570: loss 4.85 136.01ms/step 120,464tok/s (total 58,490,880 tok), 26.89% MFU\n",
      "step 3580: loss 4.78 135.41ms/step 120,995tok/s (total 58,654,720 tok), 27.01% MFU\n",
      "step 3590: loss 4.94 133.82ms/step 122,437tok/s (total 58,818,560 tok), 27.33% MFU\n",
      "step 3600: loss 4.89 133.94ms/step 122,326tok/s (total 58,982,400 tok), 27.31% MFU\n",
      "step 3600 eval: val_loss 4.87\n",
      "step 3610: loss 4.86 134.28ms/step 122,018tok/s (total 59,146,240 tok), 27.24% MFU\n",
      "step 3620: loss 4.85 134.35ms/step 121,955tok/s (total 59,310,080 tok), 27.23% MFU\n",
      "step 3630: loss 4.89 134.87ms/step 121,482tok/s (total 59,473,920 tok), 27.12% MFU\n",
      "step 3640: loss 4.86 134.98ms/step 121,377tok/s (total 59,637,760 tok), 27.10% MFU\n",
      "step 3650: loss 4.83 134.06ms/step 122,213tok/s (total 59,801,600 tok), 27.28% MFU\n",
      "step 3650 eval: val_loss 4.90\n",
      "step 3660: loss 4.93 134.84ms/step 121,510tok/s (total 59,965,440 tok), 27.13% MFU\n",
      "step 3670: loss 4.80 134.30ms/step 121,992tok/s (total 60,129,280 tok), 27.23% MFU\n",
      "step 3680: loss 4.90 134.72ms/step 121,617tok/s (total 60,293,120 tok), 27.15% MFU\n",
      "step 3690: loss 4.84 134.94ms/step 121,419tok/s (total 60,456,960 tok), 27.11% MFU\n",
      "step 3700: loss 4.93 135.40ms/step 121,003tok/s (total 60,620,800 tok), 27.01% MFU\n",
      "step 3700 eval: val_loss 4.86\n",
      "step 3710: loss 4.98 134.76ms/step 121,582tok/s (total 60,784,640 tok), 27.14% MFU\n",
      "step 3720: loss 4.84 135.27ms/step 121,118tok/s (total 60,948,480 tok), 27.04% MFU\n",
      "step 3730: loss 4.87 132.42ms/step 123,729tok/s (total 61,112,320 tok), 27.62% MFU\n",
      "step 3740: loss 4.85 134.71ms/step 121,624tok/s (total 61,276,160 tok), 27.15% MFU\n",
      "step 3750: loss 4.83 134.31ms/step 121,984tok/s (total 61,440,000 tok), 27.23% MFU\n",
      "step 3750 eval: val_loss 4.82\n",
      "step 3760: loss 4.85 133.40ms/step 122,817tok/s (total 61,603,840 tok), 27.42% MFU\n",
      "step 3770: loss 4.89 135.24ms/step 121,149tok/s (total 61,767,680 tok), 27.05% MFU\n",
      "step 3780: loss 4.87 133.14ms/step 123,057tok/s (total 61,931,520 tok), 27.47% MFU\n",
      "step 3790: loss 4.91 132.13ms/step 123,999tok/s (total 62,095,360 tok), 27.68% MFU\n",
      "step 3800: loss 4.89 133.91ms/step 122,348tok/s (total 62,259,200 tok), 27.31% MFU\n",
      "step 3800 eval: val_loss 4.75\n",
      "step 3810: loss 4.84 134.48ms/step 121,829tok/s (total 62,423,040 tok), 27.20% MFU\n",
      "step 3820: loss 4.88 135.81ms/step 120,635tok/s (total 62,586,880 tok), 26.93% MFU\n",
      "step 3830: loss 4.82 134.86ms/step 121,485tok/s (total 62,750,720 tok), 27.12% MFU\n",
      "step 3840: loss 4.88 134.32ms/step 121,980tok/s (total 62,914,560 tok), 27.23% MFU\n",
      "step 3850: loss 4.79 133.81ms/step 122,442tok/s (total 63,078,400 tok), 27.34% MFU\n",
      "step 3850 eval: val_loss 4.83\n",
      "step 3860: loss 4.75 133.72ms/step 122,525tok/s (total 63,242,240 tok), 27.35% MFU\n",
      "step 3870: loss 4.84 134.01ms/step 122,259tok/s (total 63,406,080 tok), 27.29% MFU\n",
      "step 3880: loss 4.81 134.22ms/step 122,072tok/s (total 63,569,920 tok), 27.25% MFU\n",
      "step 3890: loss 4.93 134.55ms/step 121,770tok/s (total 63,733,760 tok), 27.19% MFU\n",
      "step 3900: loss 4.88 132.69ms/step 123,473tok/s (total 63,897,600 tok), 27.57% MFU\n",
      "step 3900 eval: val_loss 4.79\n",
      "step 3910: loss 4.84 135.97ms/step 120,500tok/s (total 64,061,440 tok), 26.90% MFU\n",
      "step 3920: loss 4.82 134.71ms/step 121,627tok/s (total 64,225,280 tok), 27.15% MFU\n",
      "step 3930: loss 4.78 133.83ms/step 122,422tok/s (total 64,389,120 tok), 27.33% MFU\n",
      "step 3940: loss 4.83 134.02ms/step 122,248tok/s (total 64,552,960 tok), 27.29% MFU\n",
      "step 3950: loss 4.79 136.17ms/step 120,318tok/s (total 64,716,800 tok), 26.86% MFU\n",
      "step 3950 eval: val_loss 4.75\n",
      "step 3960: loss 4.84 135.83ms/step 120,620tok/s (total 64,880,640 tok), 26.93% MFU\n",
      "step 3970: loss 4.81 136.26ms/step 120,245tok/s (total 65,044,480 tok), 26.84% MFU\n",
      "step 3980: loss 4.78 134.31ms/step 121,986tok/s (total 65,208,320 tok), 27.23% MFU\n",
      "step 3990: loss 4.86 134.52ms/step 121,800tok/s (total 65,372,160 tok), 27.19% MFU\n",
      "step 4000: loss 4.84 134.93ms/step 121,429tok/s (total 65,536,000 tok), 27.11% MFU\n",
      "step 4000 eval: val_loss 4.71\n",
      "step 4010: loss 4.89 133.99ms/step 122,274tok/s (total 65,699,840 tok), 27.30% MFU\n",
      "step 4020: loss 4.84 133.67ms/step 122,572tok/s (total 65,863,680 tok), 27.36% MFU\n",
      "step 4030: loss 4.83 133.52ms/step 122,710tok/s (total 66,027,520 tok), 27.40% MFU\n",
      "step 4040: loss 4.82 133.96ms/step 122,310tok/s (total 66,191,360 tok), 27.31% MFU\n",
      "step 4050: loss 4.84 134.09ms/step 122,184tok/s (total 66,355,200 tok), 27.28% MFU\n",
      "step 4050 eval: val_loss 4.76\n",
      "step 4060: loss 4.87 134.24ms/step 122,051tok/s (total 66,519,040 tok), 27.25% MFU\n",
      "step 4070: loss 4.78 133.90ms/step 122,364tok/s (total 66,682,880 tok), 27.32% MFU\n",
      "step 4080: loss 4.82 134.05ms/step 122,219tok/s (total 66,846,720 tok), 27.29% MFU\n",
      "step 4090: loss 4.91 132.88ms/step 123,297tok/s (total 67,010,560 tok), 27.53% MFU\n",
      "step 4100: loss 4.80 134.28ms/step 122,015tok/s (total 67,174,400 tok), 27.24% MFU\n",
      "step 4100 eval: val_loss 4.80\n",
      "step 4110: loss 4.78 134.95ms/step 121,408tok/s (total 67,338,240 tok), 27.10% MFU\n",
      "step 4120: loss 4.74 135.09ms/step 121,284tok/s (total 67,502,080 tok), 27.08% MFU\n",
      "step 4130: loss 4.73 134.20ms/step 122,085tok/s (total 67,665,920 tok), 27.26% MFU\n",
      "step 4140: loss 4.76 134.27ms/step 122,021tok/s (total 67,829,760 tok), 27.24% MFU\n",
      "step 4150: loss 4.85 133.94ms/step 122,325tok/s (total 67,993,600 tok), 27.31% MFU\n",
      "step 4150 eval: val_loss 4.74\n",
      "step 4160: loss 4.86 134.29ms/step 122,005tok/s (total 68,157,440 tok), 27.24% MFU\n",
      "step 4170: loss 4.78 136.53ms/step 120,001tok/s (total 68,321,280 tok), 26.79% MFU\n",
      "step 4180: loss 4.82 134.11ms/step 122,171tok/s (total 68,485,120 tok), 27.27% MFU\n",
      "step 4190: loss 4.80 136.39ms/step 120,126tok/s (total 68,648,960 tok), 26.82% MFU\n",
      "step 4200: loss 4.79 134.18ms/step 122,101tok/s (total 68,812,800 tok), 27.26% MFU\n",
      "step 4200 eval: val_loss 4.79\n",
      "step 4210: loss 4.79 136.54ms/step 119,998tok/s (total 68,976,640 tok), 26.79% MFU\n",
      "step 4220: loss 4.81 137.14ms/step 119,468tok/s (total 69,140,480 tok), 26.67% MFU\n",
      "step 4230: loss 4.84 135.70ms/step 120,734tok/s (total 69,304,320 tok), 26.95% MFU\n",
      "step 4240: loss 4.82 134.62ms/step 121,702tok/s (total 69,468,160 tok), 27.17% MFU\n",
      "step 4250: loss 4.76 134.14ms/step 122,144tok/s (total 69,632,000 tok), 27.27% MFU\n",
      "step 4250 eval: val_loss 4.83\n",
      "step 4260: loss 4.84 135.05ms/step 121,320tok/s (total 69,795,840 tok), 27.08% MFU\n",
      "step 4270: loss 4.76 133.93ms/step 122,332tok/s (total 69,959,680 tok), 27.31% MFU\n",
      "step 4280: loss 4.75 136.74ms/step 119,820tok/s (total 70,123,520 tok), 26.75% MFU\n",
      "step 4290: loss 4.83 134.57ms/step 121,752tok/s (total 70,287,360 tok), 27.18% MFU\n",
      "step 4300: loss 4.82 134.12ms/step 122,161tok/s (total 70,451,200 tok), 27.27% MFU\n",
      "step 4300 eval: val_loss 4.76\n",
      "step 4310: loss 4.83 134.64ms/step 121,691tok/s (total 70,615,040 tok), 27.17% MFU\n",
      "step 4320: loss 4.81 135.08ms/step 121,288tok/s (total 70,778,880 tok), 27.08% MFU\n",
      "step 4330: loss 4.85 138.15ms/step 118,597tok/s (total 70,942,720 tok), 26.48% MFU\n",
      "step 4340: loss 4.83 134.96ms/step 121,399tok/s (total 71,106,560 tok), 27.10% MFU\n",
      "step 4350: loss 4.82 135.10ms/step 121,269tok/s (total 71,270,400 tok), 27.07% MFU\n",
      "step 4350 eval: val_loss 4.77\n",
      "step 4360: loss 4.90 133.02ms/step 123,173tok/s (total 71,434,240 tok), 27.50% MFU\n",
      "step 4370: loss 4.73 132.81ms/step 123,367tok/s (total 71,598,080 tok), 27.54% MFU\n",
      "step 4380: loss 4.82 134.68ms/step 121,653tok/s (total 71,761,920 tok), 27.16% MFU\n",
      "step 4390: loss 4.92 133.60ms/step 122,633tok/s (total 71,925,760 tok), 27.38% MFU\n",
      "step 4400: loss 4.81 136.55ms/step 119,989tok/s (total 72,089,600 tok), 26.79% MFU\n",
      "step 4400 eval: val_loss 4.75\n",
      "step 4410: loss 4.74 135.05ms/step 121,318tok/s (total 72,253,440 tok), 27.08% MFU\n",
      "step 4420: loss 4.78 134.80ms/step 121,542tok/s (total 72,417,280 tok), 27.13% MFU\n",
      "step 4430: loss 4.82 134.11ms/step 122,164tok/s (total 72,581,120 tok), 27.27% MFU\n",
      "step 4440: loss 4.82 134.34ms/step 121,962tok/s (total 72,744,960 tok), 27.23% MFU\n",
      "step 4450: loss 4.81 133.42ms/step 122,796tok/s (total 72,908,800 tok), 27.41% MFU\n",
      "step 4450 eval: val_loss 4.74\n",
      "step 4460: loss 4.79 135.03ms/step 121,339tok/s (total 73,072,640 tok), 27.09% MFU\n",
      "step 4470: loss 4.81 137.13ms/step 119,476tok/s (total 73,236,480 tok), 26.67% MFU\n",
      "step 4480: loss 4.79 133.57ms/step 122,658tok/s (total 73,400,320 tok), 27.38% MFU\n",
      "step 4490: loss 4.79 133.75ms/step 122,498tok/s (total 73,564,160 tok), 27.35% MFU\n",
      "step 4500: loss 4.78 133.91ms/step 122,347tok/s (total 73,728,000 tok), 27.31% MFU\n",
      "step 4500 eval: val_loss 4.77\n",
      "step 4510: loss 4.74 134.32ms/step 121,973tok/s (total 73,891,840 tok), 27.23% MFU\n",
      "step 4520: loss 4.73 134.58ms/step 121,741tok/s (total 74,055,680 tok), 27.18% MFU\n",
      "step 4530: loss 4.79 135.53ms/step 120,884tok/s (total 74,219,520 tok), 26.99% MFU\n",
      "step 4540: loss 4.81 132.38ms/step 123,768tok/s (total 74,383,360 tok), 27.63% MFU\n",
      "step 4550: loss 4.78 135.13ms/step 121,245tok/s (total 74,547,200 tok), 27.07% MFU\n",
      "step 4550 eval: val_loss 4.79\n",
      "step 4560: loss 4.87 137.01ms/step 119,581tok/s (total 74,711,040 tok), 26.70% MFU\n",
      "step 4570: loss 4.71 136.87ms/step 119,702tok/s (total 74,874,880 tok), 26.72% MFU\n",
      "step 4580: loss 4.80 134.65ms/step 121,678tok/s (total 75,038,720 tok), 27.16% MFU\n",
      "step 4590: loss 4.73 137.56ms/step 119,100tok/s (total 75,202,560 tok), 26.59% MFU\n",
      "step 4600: loss 4.71 135.60ms/step 120,824tok/s (total 75,366,400 tok), 26.97% MFU\n",
      "step 4600 eval: val_loss 4.70\n",
      "step 4610: loss 4.76 133.25ms/step 122,959tok/s (total 75,530,240 tok), 27.45% MFU\n",
      "step 4620: loss 4.76 137.27ms/step 119,359tok/s (total 75,694,080 tok), 26.65% MFU\n",
      "step 4630: loss 4.77 134.27ms/step 122,024tok/s (total 75,857,920 tok), 27.24% MFU\n",
      "step 4640: loss 4.80 135.26ms/step 121,132tok/s (total 76,021,760 tok), 27.04% MFU\n",
      "step 4650: loss 4.85 135.86ms/step 120,596tok/s (total 76,185,600 tok), 26.92% MFU\n",
      "step 4650 eval: val_loss 4.72\n",
      "step 4660: loss 4.80 134.71ms/step 121,626tok/s (total 76,349,440 tok), 27.15% MFU\n",
      "step 4670: loss 4.90 135.27ms/step 121,124tok/s (total 76,513,280 tok), 27.04% MFU\n",
      "step 4680: loss 4.74 132.84ms/step 123,335tok/s (total 76,677,120 tok), 27.53% MFU\n",
      "step 4690: loss 4.78 134.71ms/step 121,628tok/s (total 76,840,960 tok), 27.15% MFU\n",
      "step 4700: loss 4.74 134.49ms/step 121,826tok/s (total 77,004,800 tok), 27.20% MFU\n",
      "step 4700 eval: val_loss 4.75\n",
      "step 4710: loss 4.79 134.28ms/step 122,009tok/s (total 77,168,640 tok), 27.24% MFU\n",
      "step 4720: loss 4.83 136.52ms/step 120,011tok/s (total 77,332,480 tok), 26.79% MFU\n",
      "step 4730: loss 4.78 136.83ms/step 119,736tok/s (total 77,496,320 tok), 26.73% MFU\n",
      "step 4740: loss 4.79 134.09ms/step 122,184tok/s (total 77,660,160 tok), 27.28% MFU\n",
      "step 4750: loss 4.77 135.83ms/step 120,617tok/s (total 77,824,000 tok), 26.93% MFU\n",
      "step 4750 eval: val_loss 4.77\n",
      "step 4760: loss 4.71 133.48ms/step 122,748tok/s (total 77,987,840 tok), 27.40% MFU\n",
      "step 4770: loss 4.72 136.56ms/step 119,974tok/s (total 78,151,680 tok), 26.78% MFU\n",
      "step 4780: loss 4.76 135.17ms/step 121,214tok/s (total 78,315,520 tok), 27.06% MFU\n",
      "step 4790: loss 4.80 134.18ms/step 122,108tok/s (total 78,479,360 tok), 27.26% MFU\n",
      "step 4800: loss 4.78 133.58ms/step 122,656tok/s (total 78,643,200 tok), 27.38% MFU\n",
      "step 4800 eval: val_loss 4.72\n",
      "step 4810: loss 4.68 134.95ms/step 121,404tok/s (total 78,807,040 tok), 27.10% MFU\n",
      "step 4820: loss 4.71 134.29ms/step 122,009tok/s (total 78,970,880 tok), 27.24% MFU\n",
      "step 4830: loss 4.87 134.61ms/step 121,713tok/s (total 79,134,720 tok), 27.17% MFU\n",
      "step 4840: loss 4.69 134.95ms/step 121,409tok/s (total 79,298,560 tok), 27.10% MFU\n",
      "step 4850: loss 4.73 133.84ms/step 122,415tok/s (total 79,462,400 tok), 27.33% MFU\n",
      "step 4850 eval: val_loss 4.65\n",
      "step 4860: loss 4.82 134.16ms/step 122,127tok/s (total 79,626,240 tok), 27.27% MFU\n",
      "step 4870: loss 4.75 134.87ms/step 121,482tok/s (total 79,790,080 tok), 27.12% MFU\n",
      "step 4880: loss 4.72 134.45ms/step 121,855tok/s (total 79,953,920 tok), 27.20% MFU\n",
      "step 4890: loss 4.87 134.53ms/step 121,785tok/s (total 80,117,760 tok), 27.19% MFU\n",
      "step 4900: loss 4.73 132.94ms/step 123,240tok/s (total 80,281,600 tok), 27.51% MFU\n",
      "step 4900 eval: val_loss 4.69\n",
      "step 4910: loss 4.78 131.74ms/step 124,368tok/s (total 80,445,440 tok), 27.77% MFU\n",
      "step 4920: loss 4.70 132.94ms/step 123,247tok/s (total 80,609,280 tok), 27.52% MFU\n",
      "step 4930: loss 4.79 135.34ms/step 121,055tok/s (total 80,773,120 tok), 27.03% MFU\n",
      "step 4940: loss 4.69 132.92ms/step 123,262tok/s (total 80,936,960 tok), 27.52% MFU\n",
      "step 4950: loss 4.64 134.86ms/step 121,485tok/s (total 81,100,800 tok), 27.12% MFU\n",
      "step 4950 eval: val_loss 4.69\n",
      "step 4960: loss 4.70 133.04ms/step 123,147tok/s (total 81,264,640 tok), 27.49% MFU\n",
      "step 4970: loss 4.74 132.85ms/step 123,326tok/s (total 81,428,480 tok), 27.53% MFU\n",
      "step 4980: loss 4.71 134.79ms/step 121,551tok/s (total 81,592,320 tok), 27.14% MFU\n",
      "step 4990: loss 4.71 130.80ms/step 125,264tok/s (total 81,756,160 tok), 27.97% MFU\n",
      "step 5000: loss 4.80 131.13ms/step 124,947tok/s (total 81,920,000 tok), 27.89% MFU\n",
      "step 5000 eval: val_loss 4.67\n",
      "step 5010: loss 4.70 133.86ms/step 122,395tok/s (total 82,083,840 tok), 27.32% MFU\n",
      "step 5020: loss 4.66 134.24ms/step 122,048tok/s (total 82,247,680 tok), 27.25% MFU\n",
      "step 5030: loss 4.74 134.13ms/step 122,152tok/s (total 82,411,520 tok), 27.27% MFU\n",
      "step 5040: loss 4.70 132.70ms/step 123,465tok/s (total 82,575,360 tok), 27.56% MFU\n",
      "step 5050: loss 4.64 132.50ms/step 123,655tok/s (total 82,739,200 tok), 27.61% MFU\n",
      "step 5050 eval: val_loss 4.67\n",
      "step 5060: loss 4.78 132.56ms/step 123,599tok/s (total 82,903,040 tok), 27.59% MFU\n",
      "step 5070: loss 4.79 134.39ms/step 121,914tok/s (total 83,066,880 tok), 27.22% MFU\n",
      "step 5080: loss 4.69 133.10ms/step 123,092tok/s (total 83,230,720 tok), 27.48% MFU\n",
      "step 5090: loss 4.73 132.90ms/step 123,284tok/s (total 83,394,560 tok), 27.52% MFU\n",
      "step 5100: loss 4.75 133.80ms/step 122,447tok/s (total 83,558,400 tok), 27.34% MFU\n",
      "step 5100 eval: val_loss 4.60\n",
      "step 5110: loss 4.73 134.78ms/step 121,560tok/s (total 83,722,240 tok), 27.14% MFU\n",
      "step 5120: loss 4.71 134.93ms/step 121,423tok/s (total 83,886,080 tok), 27.11% MFU\n",
      "step 5130: loss 4.68 132.97ms/step 123,220tok/s (total 84,049,920 tok), 27.51% MFU\n",
      "step 5140: loss 4.65 132.61ms/step 123,547tok/s (total 84,213,760 tok), 27.58% MFU\n",
      "step 5150: loss 4.59 132.94ms/step 123,242tok/s (total 84,377,600 tok), 27.51% MFU\n",
      "step 5150 eval: val_loss 4.65\n",
      "step 5160: loss 4.72 131.94ms/step 124,177tok/s (total 84,541,440 tok), 27.72% MFU\n",
      "step 5170: loss 4.76 133.45ms/step 122,776tok/s (total 84,705,280 tok), 27.41% MFU\n",
      "step 5180: loss 4.71 133.16ms/step 123,041tok/s (total 84,869,120 tok), 27.47% MFU\n",
      "step 5190: loss 4.72 132.25ms/step 123,884tok/s (total 85,032,960 tok), 27.66% MFU\n",
      "step 5200: loss 4.67 133.38ms/step 122,838tok/s (total 85,196,800 tok), 27.42% MFU\n",
      "step 5200 eval: val_loss 4.67\n",
      "step 5210: loss 4.76 133.86ms/step 122,399tok/s (total 85,360,640 tok), 27.33% MFU\n",
      "step 5220: loss 4.74 132.83ms/step 123,348tok/s (total 85,524,480 tok), 27.54% MFU\n",
      "step 5230: loss 4.60 134.90ms/step 121,452tok/s (total 85,688,320 tok), 27.11% MFU\n",
      "step 5240: loss 4.72 135.51ms/step 120,908tok/s (total 85,852,160 tok), 26.99% MFU\n",
      "step 5250: loss 4.68 132.57ms/step 123,586tok/s (total 86,016,000 tok), 27.59% MFU\n",
      "step 5250 eval: val_loss 4.68\n",
      "step 5260: loss 4.72 134.49ms/step 121,827tok/s (total 86,179,840 tok), 27.20% MFU\n",
      "step 5270: loss 4.66 134.09ms/step 122,183tok/s (total 86,343,680 tok), 27.28% MFU\n",
      "step 5280: loss 4.68 133.90ms/step 122,363tok/s (total 86,507,520 tok), 27.32% MFU\n",
      "step 5290: loss 4.70 130.66ms/step 125,391tok/s (total 86,671,360 tok), 27.99% MFU\n",
      "step 5300: loss 4.80 132.36ms/step 123,781tok/s (total 86,835,200 tok), 27.63% MFU\n",
      "step 5300 eval: val_loss 4.65\n",
      "step 5310: loss 4.73 132.21ms/step 123,925tok/s (total 86,999,040 tok), 27.67% MFU\n",
      "step 5320: loss 4.72 135.16ms/step 121,220tok/s (total 87,162,880 tok), 27.06% MFU\n",
      "step 5330: loss 4.79 134.70ms/step 121,638tok/s (total 87,326,720 tok), 27.16% MFU\n",
      "step 5340: loss 4.73 134.29ms/step 122,008tok/s (total 87,490,560 tok), 27.24% MFU\n",
      "step 5350: loss 4.75 133.69ms/step 122,551tok/s (total 87,654,400 tok), 27.36% MFU\n",
      "step 5350 eval: val_loss 4.65\n",
      "step 5360: loss 4.71 134.70ms/step 121,629tok/s (total 87,818,240 tok), 27.15% MFU\n",
      "step 5370: loss 4.73 131.95ms/step 124,172tok/s (total 87,982,080 tok), 27.72% MFU\n",
      "step 5380: loss 4.71 134.28ms/step 122,012tok/s (total 88,145,920 tok), 27.24% MFU\n",
      "step 5390: loss 4.68 134.22ms/step 122,068tok/s (total 88,309,760 tok), 27.25% MFU\n",
      "step 5400: loss 4.65 134.73ms/step 121,606tok/s (total 88,473,600 tok), 27.15% MFU\n",
      "step 5400 eval: val_loss 4.64\n",
      "step 5410: loss 4.69 134.91ms/step 121,448tok/s (total 88,637,440 tok), 27.11% MFU\n",
      "step 5420: loss 4.65 135.00ms/step 121,365tok/s (total 88,801,280 tok), 27.09% MFU\n",
      "step 5430: loss 4.73 133.02ms/step 123,167tok/s (total 88,965,120 tok), 27.50% MFU\n",
      "step 5440: loss 4.76 136.40ms/step 120,117tok/s (total 89,128,960 tok), 26.82% MFU\n",
      "step 5450: loss 4.62 137.27ms/step 119,356tok/s (total 89,292,800 tok), 26.65% MFU\n",
      "step 5450 eval: val_loss 4.65\n",
      "step 5460: loss 4.67 134.91ms/step 121,444tok/s (total 89,456,640 tok), 27.11% MFU\n",
      "step 5470: loss 4.75 134.34ms/step 121,962tok/s (total 89,620,480 tok), 27.23% MFU\n",
      "step 5480: loss 4.77 135.87ms/step 120,583tok/s (total 89,784,320 tok), 26.92% MFU\n",
      "step 5490: loss 4.64 132.44ms/step 123,709tok/s (total 89,948,160 tok), 27.62% MFU\n",
      "step 5500: loss 4.71 137.82ms/step 118,883tok/s (total 90,112,000 tok), 26.54% MFU\n",
      "step 5500 eval: val_loss 4.66\n",
      "step 5510: loss 4.76 136.99ms/step 119,597tok/s (total 90,275,840 tok), 26.70% MFU\n",
      "step 5520: loss 4.79 137.22ms/step 119,398tok/s (total 90,439,680 tok), 26.66% MFU\n",
      "step 5530: loss 4.66 134.97ms/step 121,389tok/s (total 90,603,520 tok), 27.10% MFU\n",
      "step 5540: loss 4.68 136.07ms/step 120,408tok/s (total 90,767,360 tok), 26.88% MFU\n",
      "step 5550: loss 4.66 134.65ms/step 121,674tok/s (total 90,931,200 tok), 27.16% MFU\n",
      "step 5550 eval: val_loss 4.61\n",
      "step 5560: loss 4.70 133.67ms/step 122,568tok/s (total 91,095,040 tok), 27.36% MFU\n",
      "step 5570: loss 4.67 130.21ms/step 125,828tok/s (total 91,258,880 tok), 28.09% MFU\n",
      "step 5580: loss 4.60 136.80ms/step 119,770tok/s (total 91,422,720 tok), 26.74% MFU\n",
      "step 5590: loss 4.75 135.33ms/step 121,066tok/s (total 91,586,560 tok), 27.03% MFU\n",
      "step 5600: loss 4.60 136.03ms/step 120,444tok/s (total 91,750,400 tok), 26.89% MFU\n",
      "step 5600 eval: val_loss 4.66\n",
      "step 5610: loss 4.67 136.71ms/step 119,847tok/s (total 91,914,240 tok), 26.76% MFU\n",
      "step 5620: loss 4.71 136.58ms/step 119,959tok/s (total 92,078,080 tok), 26.78% MFU\n",
      "step 5630: loss 4.61 134.55ms/step 121,767tok/s (total 92,241,920 tok), 27.18% MFU\n",
      "step 5640: loss 4.67 136.77ms/step 119,795tok/s (total 92,405,760 tok), 26.74% MFU\n",
      "step 5650: loss 4.76 136.06ms/step 120,416tok/s (total 92,569,600 tok), 26.88% MFU\n",
      "step 5650 eval: val_loss 4.65\n",
      "step 5660: loss 4.67 136.88ms/step 119,692tok/s (total 92,733,440 tok), 26.72% MFU\n",
      "step 5670: loss 4.71 136.46ms/step 120,063tok/s (total 92,897,280 tok), 26.80% MFU\n",
      "step 5680: loss 4.60 133.82ms/step 122,429tok/s (total 93,061,120 tok), 27.33% MFU\n",
      "step 5690: loss 4.68 136.49ms/step 120,040tok/s (total 93,224,960 tok), 26.80% MFU\n",
      "step 5700: loss 4.75 136.70ms/step 119,853tok/s (total 93,388,800 tok), 26.76% MFU\n",
      "step 5700 eval: val_loss 4.68\n",
      "step 5710: loss 4.68 132.27ms/step 123,872tok/s (total 93,552,640 tok), 27.65% MFU\n",
      "step 5720: loss 4.62 135.08ms/step 121,288tok/s (total 93,716,480 tok), 27.08% MFU\n",
      "step 5730: loss 4.63 133.23ms/step 122,977tok/s (total 93,880,320 tok), 27.45% MFU\n",
      "step 5740: loss 4.71 133.67ms/step 122,570tok/s (total 94,044,160 tok), 27.36% MFU\n",
      "step 5750: loss 4.63 133.77ms/step 122,477tok/s (total 94,208,000 tok), 27.34% MFU\n",
      "step 5750 eval: val_loss 4.71\n",
      "step 5760: loss 4.71 134.46ms/step 121,852tok/s (total 94,371,840 tok), 27.20% MFU\n",
      "step 5770: loss 4.69 134.75ms/step 121,589tok/s (total 94,535,680 tok), 27.14% MFU\n",
      "step 5780: loss 4.66 134.61ms/step 121,715tok/s (total 94,699,520 tok), 27.17% MFU\n",
      "step 5790: loss 4.72 133.65ms/step 122,586tok/s (total 94,863,360 tok), 27.37% MFU\n",
      "step 5800: loss 4.66 136.03ms/step 120,445tok/s (total 95,027,200 tok), 26.89% MFU\n",
      "step 5800 eval: val_loss 4.63\n",
      "step 5810: loss 4.66 131.80ms/step 124,311tok/s (total 95,191,040 tok), 27.75% MFU\n",
      "step 5820: loss 4.72 130.78ms/step 125,281tok/s (total 95,354,880 tok), 27.97% MFU\n",
      "step 5830: loss 4.71 133.28ms/step 122,928tok/s (total 95,518,720 tok), 27.44% MFU\n",
      "step 5840: loss 4.65 134.48ms/step 121,836tok/s (total 95,682,560 tok), 27.20% MFU\n",
      "step 5850: loss 4.63 135.01ms/step 121,355tok/s (total 95,846,400 tok), 27.09% MFU\n",
      "step 5850 eval: val_loss 4.69\n",
      "step 5860: loss 4.64 134.22ms/step 122,069tok/s (total 96,010,240 tok), 27.25% MFU\n",
      "step 5870: loss 4.60 132.86ms/step 123,320tok/s (total 96,174,080 tok), 27.53% MFU\n",
      "step 5880: loss 4.65 134.37ms/step 121,935tok/s (total 96,337,920 tok), 27.22% MFU\n",
      "step 5890: loss 4.65 134.59ms/step 121,730tok/s (total 96,501,760 tok), 27.18% MFU\n",
      "step 5900: loss 4.65 134.41ms/step 121,898tok/s (total 96,665,600 tok), 27.21% MFU\n",
      "step 5900 eval: val_loss 4.59\n",
      "step 5910: loss 4.72 135.26ms/step 121,127tok/s (total 96,829,440 tok), 27.04% MFU\n",
      "step 5920: loss 4.63 134.19ms/step 122,098tok/s (total 96,993,280 tok), 27.26% MFU\n",
      "step 5930: loss 4.73 134.25ms/step 122,043tok/s (total 97,157,120 tok), 27.25% MFU\n",
      "step 5940: loss 4.63 134.13ms/step 122,149tok/s (total 97,320,960 tok), 27.27% MFU\n",
      "step 5950: loss 4.64 134.36ms/step 121,937tok/s (total 97,484,800 tok), 27.22% MFU\n",
      "step 5950 eval: val_loss 4.62\n",
      "step 5960: loss 4.74 134.19ms/step 122,092tok/s (total 97,648,640 tok), 27.26% MFU\n",
      "step 5970: loss 4.66 135.18ms/step 121,202tok/s (total 97,812,480 tok), 27.06% MFU\n",
      "step 5980: loss 4.66 134.22ms/step 122,064tok/s (total 97,976,320 tok), 27.25% MFU\n",
      "step 5990: loss 4.68 133.58ms/step 122,656tok/s (total 98,140,160 tok), 27.38% MFU\n",
      "step 6000: loss 4.62 134.00ms/step 122,265tok/s (total 98,304,000 tok), 27.30% MFU\n",
      "step 6000 eval: val_loss 4.60\n",
      "step 6010: loss 4.61 134.40ms/step 121,906tok/s (total 98,467,840 tok), 27.22% MFU\n",
      "step 6020: loss 4.61 135.02ms/step 121,346tok/s (total 98,631,680 tok), 27.09% MFU\n",
      "step 6030: loss 4.61 134.05ms/step 122,226tok/s (total 98,795,520 tok), 27.29% MFU\n",
      "step 6040: loss 4.62 133.92ms/step 122,345tok/s (total 98,959,360 tok), 27.31% MFU\n",
      "step 6050: loss 4.60 133.70ms/step 122,547tok/s (total 99,123,200 tok), 27.36% MFU\n",
      "step 6050 eval: val_loss 4.64\n",
      "step 6060: loss 4.59 134.37ms/step 121,932tok/s (total 99,287,040 tok), 27.22% MFU\n",
      "step 6070: loss 4.58 132.64ms/step 123,525tok/s (total 99,450,880 tok), 27.58% MFU\n",
      "step 6080: loss 4.68 133.99ms/step 122,273tok/s (total 99,614,720 tok), 27.30% MFU\n",
      "step 6090: loss 4.60 134.04ms/step 122,234tok/s (total 99,778,560 tok), 27.29% MFU\n",
      "step 6100: loss 4.62 133.60ms/step 122,634tok/s (total 99,942,400 tok), 27.38% MFU\n",
      "step 6100 eval: val_loss 4.59\n",
      "step 6110: loss 4.62 134.17ms/step 122,114tok/s (total 100,106,240 tok), 27.26% MFU\n",
      "step 6120: loss 4.64 134.09ms/step 122,183tok/s (total 100,270,080 tok), 27.28% MFU\n",
      "step 6130: loss 4.61 134.51ms/step 121,805tok/s (total 100,433,920 tok), 27.19% MFU\n",
      "step 6140: loss 4.67 134.76ms/step 121,582tok/s (total 100,597,760 tok), 27.14% MFU\n",
      "step 6150: loss 4.65 134.89ms/step 121,461tok/s (total 100,761,600 tok), 27.12% MFU\n",
      "step 6150 eval: val_loss 4.56\n",
      "step 6160: loss 4.62 136.37ms/step 120,140tok/s (total 100,925,440 tok), 26.82% MFU\n",
      "step 6170: loss 4.61 134.23ms/step 122,060tok/s (total 101,089,280 tok), 27.25% MFU\n",
      "step 6180: loss 4.67 136.65ms/step 119,900tok/s (total 101,253,120 tok), 26.77% MFU\n",
      "step 6190: loss 4.53 137.23ms/step 119,391tok/s (total 101,416,960 tok), 26.65% MFU\n",
      "step 6200: loss 4.58 135.30ms/step 121,094tok/s (total 101,580,800 tok), 27.03% MFU\n",
      "step 6200 eval: val_loss 4.66\n",
      "step 6210: loss 4.67 135.18ms/step 121,198tok/s (total 101,744,640 tok), 27.06% MFU\n",
      "step 6220: loss 4.72 135.42ms/step 120,984tok/s (total 101,908,480 tok), 27.01% MFU\n",
      "step 6230: loss 4.56 136.74ms/step 119,816tok/s (total 102,072,320 tok), 26.75% MFU\n",
      "step 6240: loss 4.62 137.18ms/step 119,436tok/s (total 102,236,160 tok), 26.66% MFU\n",
      "step 6250: loss 4.58 134.59ms/step 121,730tok/s (total 102,400,000 tok), 27.18% MFU\n",
      "step 6250 eval: val_loss 4.54\n",
      "step 6260: loss 4.68 136.24ms/step 120,260tok/s (total 102,563,840 tok), 26.85% MFU\n",
      "step 6270: loss 4.59 137.03ms/step 119,569tok/s (total 102,727,680 tok), 26.69% MFU\n",
      "step 6280: loss 4.53 136.81ms/step 119,761tok/s (total 102,891,520 tok), 26.74% MFU\n",
      "step 6290: loss 4.64 134.83ms/step 121,516tok/s (total 103,055,360 tok), 27.13% MFU\n",
      "step 6300: loss 4.69 133.78ms/step 122,473tok/s (total 103,219,200 tok), 27.34% MFU\n",
      "step 6300 eval: val_loss 4.57\n",
      "step 6310: loss 4.61 134.31ms/step 121,986tok/s (total 103,383,040 tok), 27.23% MFU\n",
      "step 6320: loss 4.57 134.75ms/step 121,586tok/s (total 103,546,880 tok), 27.14% MFU\n",
      "step 6330: loss 4.67 135.25ms/step 121,142tok/s (total 103,710,720 tok), 27.05% MFU\n",
      "step 6340: loss 4.71 133.63ms/step 122,609tok/s (total 103,874,560 tok), 27.37% MFU\n",
      "step 6350: loss 4.62 133.57ms/step 122,659tok/s (total 104,038,400 tok), 27.38% MFU\n",
      "step 6350 eval: val_loss 4.67\n",
      "step 6360: loss 4.67 134.00ms/step 122,264tok/s (total 104,202,240 tok), 27.30% MFU\n",
      "step 6370: loss 4.64 133.80ms/step 122,452tok/s (total 104,366,080 tok), 27.34% MFU\n",
      "step 6380: loss 4.64 134.31ms/step 121,989tok/s (total 104,529,920 tok), 27.23% MFU\n",
      "step 6390: loss 4.70 134.75ms/step 121,590tok/s (total 104,693,760 tok), 27.15% MFU\n",
      "step 6400: loss 4.58 134.09ms/step 122,187tok/s (total 104,857,600 tok), 27.28% MFU\n",
      "step 6400 eval: val_loss 4.53\n",
      "step 6410: loss 4.63 133.92ms/step 122,345tok/s (total 105,021,440 tok), 27.31% MFU\n",
      "step 6420: loss 4.67 134.49ms/step 121,825tok/s (total 105,185,280 tok), 27.20% MFU\n",
      "step 6430: loss 4.73 134.25ms/step 122,037tok/s (total 105,349,120 tok), 27.25% MFU\n",
      "step 6440: loss 4.69 134.57ms/step 121,747tok/s (total 105,512,960 tok), 27.18% MFU\n",
      "step 6450: loss 4.58 134.79ms/step 121,553tok/s (total 105,676,800 tok), 27.14% MFU\n",
      "step 6450 eval: val_loss 4.63\n",
      "step 6460: loss 4.60 134.32ms/step 121,973tok/s (total 105,840,640 tok), 27.23% MFU\n",
      "step 6470: loss 4.60 134.56ms/step 121,756tok/s (total 106,004,480 tok), 27.18% MFU\n",
      "step 6480: loss 4.62 134.56ms/step 121,756tok/s (total 106,168,320 tok), 27.18% MFU\n",
      "step 6490: loss 4.58 134.73ms/step 121,606tok/s (total 106,332,160 tok), 27.15% MFU\n",
      "step 6500: loss 4.59 134.40ms/step 121,902tok/s (total 106,496,000 tok), 27.21% MFU\n",
      "step 6500 eval: val_loss 4.58\n",
      "step 6510: loss 4.63 135.06ms/step 121,310tok/s (total 106,659,840 tok), 27.08% MFU\n",
      "step 6520: loss 4.68 134.65ms/step 121,680tok/s (total 106,823,680 tok), 27.17% MFU\n",
      "step 6530: loss 4.57 133.90ms/step 122,364tok/s (total 106,987,520 tok), 27.32% MFU\n",
      "step 6540: loss 4.66 133.90ms/step 122,364tok/s (total 107,151,360 tok), 27.32% MFU\n",
      "step 6550: loss 4.65 134.07ms/step 122,209tok/s (total 107,315,200 tok), 27.28% MFU\n",
      "step 6550 eval: val_loss 4.56\n",
      "step 6560: loss 4.64 134.54ms/step 121,782tok/s (total 107,479,040 tok), 27.19% MFU\n",
      "step 6570: loss 4.57 134.56ms/step 121,757tok/s (total 107,642,880 tok), 27.18% MFU\n",
      "step 6580: loss 4.59 133.45ms/step 122,769tok/s (total 107,806,720 tok), 27.41% MFU\n",
      "step 6590: loss 4.63 133.45ms/step 122,774tok/s (total 107,970,560 tok), 27.41% MFU\n",
      "step 6600: loss 4.70 134.15ms/step 122,136tok/s (total 108,134,400 tok), 27.27% MFU\n",
      "step 6600 eval: val_loss 4.65\n",
      "step 6610: loss 4.63 135.24ms/step 121,146tok/s (total 108,298,240 tok), 27.05% MFU\n",
      "step 6620: loss 4.57 134.58ms/step 121,743tok/s (total 108,462,080 tok), 27.18% MFU\n",
      "step 6630: loss 4.71 135.11ms/step 121,266tok/s (total 108,625,920 tok), 27.07% MFU\n",
      "step 6640: loss 4.56 133.55ms/step 122,676tok/s (total 108,789,760 tok), 27.39% MFU\n",
      "step 6650: loss 4.58 133.54ms/step 122,692tok/s (total 108,953,600 tok), 27.39% MFU\n",
      "step 6650 eval: val_loss 4.59\n",
      "step 6660: loss 4.69 134.09ms/step 122,188tok/s (total 109,117,440 tok), 27.28% MFU\n",
      "step 6670: loss 4.57 134.27ms/step 122,022tok/s (total 109,281,280 tok), 27.24% MFU\n",
      "step 6680: loss 4.65 134.43ms/step 121,874tok/s (total 109,445,120 tok), 27.21% MFU\n",
      "step 6690: loss 4.58 134.37ms/step 121,928tok/s (total 109,608,960 tok), 27.22% MFU\n",
      "step 6700: loss 4.66 133.17ms/step 123,035tok/s (total 109,772,800 tok), 27.47% MFU\n",
      "step 6700 eval: val_loss 4.52\n",
      "step 6710: loss 4.60 132.76ms/step 123,414tok/s (total 109,936,640 tok), 27.55% MFU\n",
      "step 6720: loss 4.61 134.82ms/step 121,529tok/s (total 110,100,480 tok), 27.13% MFU\n",
      "step 6730: loss 4.68 134.60ms/step 121,723tok/s (total 110,264,320 tok), 27.18% MFU\n",
      "step 6740: loss 4.61 134.51ms/step 121,801tok/s (total 110,428,160 tok), 27.19% MFU\n",
      "step 6750: loss 4.58 134.44ms/step 121,869tok/s (total 110,592,000 tok), 27.21% MFU\n",
      "step 6750 eval: val_loss 4.61\n",
      "step 6760: loss 4.58 133.82ms/step 122,430tok/s (total 110,755,840 tok), 27.33% MFU\n",
      "step 6770: loss 4.61 132.68ms/step 123,485tok/s (total 110,919,680 tok), 27.57% MFU\n",
      "step 6780: loss 4.64 133.58ms/step 122,653tok/s (total 111,083,520 tok), 27.38% MFU\n",
      "step 6790: loss 4.65 136.66ms/step 119,891tok/s (total 111,247,360 tok), 26.77% MFU\n",
      "step 6800: loss 4.60 134.99ms/step 121,373tok/s (total 111,411,200 tok), 27.10% MFU\n",
      "step 6800 eval: val_loss 4.56\n",
      "step 6810: loss 4.66 135.09ms/step 121,284tok/s (total 111,575,040 tok), 27.08% MFU\n",
      "step 6820: loss 4.58 135.46ms/step 120,953tok/s (total 111,738,880 tok), 27.00% MFU\n",
      "step 6830: loss 4.55 134.49ms/step 121,825tok/s (total 111,902,720 tok), 27.20% MFU\n",
      "step 6840: loss 4.59 137.08ms/step 119,519tok/s (total 112,066,560 tok), 26.68% MFU\n",
      "step 6850: loss 4.64 136.47ms/step 120,060tok/s (total 112,230,400 tok), 26.80% MFU\n",
      "step 6850 eval: val_loss 4.59\n",
      "step 6860: loss 4.67 137.02ms/step 119,575tok/s (total 112,394,240 tok), 26.70% MFU\n",
      "step 6870: loss 4.66 136.71ms/step 119,846tok/s (total 112,558,080 tok), 26.76% MFU\n",
      "step 6880: loss 4.60 132.77ms/step 123,398tok/s (total 112,721,920 tok), 27.55% MFU\n",
      "step 6890: loss 4.57 136.43ms/step 120,092tok/s (total 112,885,760 tok), 26.81% MFU\n",
      "step 6900: loss 4.61 134.18ms/step 122,109tok/s (total 113,049,600 tok), 27.26% MFU\n",
      "step 6900 eval: val_loss 4.59\n",
      "step 6910: loss 4.66 136.56ms/step 119,977tok/s (total 113,213,440 tok), 26.79% MFU\n",
      "step 6920: loss 4.52 137.32ms/step 119,310tok/s (total 113,377,280 tok), 26.64% MFU\n",
      "step 6930: loss 4.63 134.78ms/step 121,563tok/s (total 113,541,120 tok), 27.14% MFU\n",
      "step 6940: loss 4.56 134.70ms/step 121,629tok/s (total 113,704,960 tok), 27.15% MFU\n",
      "step 6950: loss 4.65 134.99ms/step 121,368tok/s (total 113,868,800 tok), 27.10% MFU\n",
      "step 6950 eval: val_loss 4.59\n",
      "step 6960: loss 4.57 134.01ms/step 122,263tok/s (total 114,032,640 tok), 27.30% MFU\n",
      "step 6970: loss 4.58 133.47ms/step 122,756tok/s (total 114,196,480 tok), 27.41% MFU\n",
      "step 6980: loss 4.58 132.66ms/step 123,505tok/s (total 114,360,320 tok), 27.57% MFU\n",
      "step 6990: loss 4.63 134.44ms/step 121,865tok/s (total 114,524,160 tok), 27.21% MFU\n",
      "step 7000: loss 4.52 138.08ms/step 118,653tok/s (total 114,688,000 tok), 26.49% MFU\n",
      "step 7000 eval: val_loss 4.61\n",
      "step 7010: loss 4.60 134.45ms/step 121,862tok/s (total 114,851,840 tok), 27.21% MFU\n",
      "step 7020: loss 4.70 132.79ms/step 123,387tok/s (total 115,015,680 tok), 27.55% MFU\n",
      "step 7030: loss 4.55 134.77ms/step 121,573tok/s (total 115,179,520 tok), 27.14% MFU\n",
      "step 7040: loss 4.57 133.52ms/step 122,712tok/s (total 115,343,360 tok), 27.40% MFU\n",
      "step 7050: loss 4.62 133.08ms/step 123,112tok/s (total 115,507,200 tok), 27.48% MFU\n",
      "step 7050 eval: val_loss 4.58\n",
      "step 7060: loss 4.51 134.48ms/step 121,837tok/s (total 115,671,040 tok), 27.20% MFU\n",
      "step 7070: loss 4.58 134.36ms/step 121,941tok/s (total 115,834,880 tok), 27.22% MFU\n",
      "step 7080: loss 4.60 135.05ms/step 121,314tok/s (total 115,998,720 tok), 27.08% MFU\n",
      "step 7090: loss 4.53 133.06ms/step 123,136tok/s (total 116,162,560 tok), 27.49% MFU\n",
      "step 7100: loss 4.61 131.00ms/step 125,065tok/s (total 116,326,400 tok), 27.92% MFU\n",
      "step 7100 eval: val_loss 4.56\n",
      "step 7110: loss 4.61 130.67ms/step 125,380tok/s (total 116,490,240 tok), 27.99% MFU\n",
      "step 7120: loss 4.59 134.60ms/step 121,720tok/s (total 116,654,080 tok), 27.17% MFU\n",
      "step 7130: loss 4.66 131.84ms/step 124,274tok/s (total 116,817,920 tok), 27.74% MFU\n",
      "step 7140: loss 4.62 134.19ms/step 122,097tok/s (total 116,981,760 tok), 27.26% MFU\n",
      "step 7150: loss 4.64 132.41ms/step 123,740tok/s (total 117,145,600 tok), 27.63% MFU\n",
      "step 7150 eval: val_loss 4.57\n",
      "step 7160: loss 4.55 132.97ms/step 123,213tok/s (total 117,309,440 tok), 27.51% MFU\n",
      "step 7170: loss 4.62 132.81ms/step 123,365tok/s (total 117,473,280 tok), 27.54% MFU\n",
      "step 7180: loss 4.59 135.30ms/step 121,090tok/s (total 117,637,120 tok), 27.03% MFU\n",
      "step 7190: loss 4.55 132.67ms/step 123,498tok/s (total 117,800,960 tok), 27.57% MFU\n",
      "step 7200: loss 4.65 131.98ms/step 124,141tok/s (total 117,964,800 tok), 27.71% MFU\n",
      "step 7200 eval: val_loss 4.65\n",
      "step 7210: loss 4.63 134.02ms/step 122,252tok/s (total 118,128,640 tok), 27.29% MFU\n",
      "step 7220: loss 4.55 135.56ms/step 120,866tok/s (total 118,292,480 tok), 26.98% MFU\n",
      "step 7230: loss 4.59 134.07ms/step 122,209tok/s (total 118,456,320 tok), 27.28% MFU\n",
      "step 7240: loss 4.57 131.57ms/step 124,525tok/s (total 118,620,160 tok), 27.80% MFU\n",
      "step 7250: loss 4.56 132.74ms/step 123,426tok/s (total 118,784,000 tok), 27.56% MFU\n",
      "step 7250 eval: val_loss 4.56\n",
      "step 7260: loss 4.56 134.88ms/step 121,475tok/s (total 118,947,840 tok), 27.12% MFU\n",
      "step 7270: loss 4.56 132.71ms/step 123,460tok/s (total 119,111,680 tok), 27.56% MFU\n",
      "step 7280: loss 4.57 134.46ms/step 121,854tok/s (total 119,275,520 tok), 27.20% MFU\n",
      "step 7290: loss 4.57 135.09ms/step 121,280tok/s (total 119,439,360 tok), 27.08% MFU\n",
      "step 7300: loss 4.56 132.42ms/step 123,731tok/s (total 119,603,200 tok), 27.62% MFU\n",
      "step 7300 eval: val_loss 4.59\n",
      "step 7310: loss 4.55 135.82ms/step 120,630tok/s (total 119,767,040 tok), 26.93% MFU\n",
      "step 7320: loss 4.57 133.29ms/step 122,919tok/s (total 119,930,880 tok), 27.44% MFU\n",
      "step 7330: loss 4.58 129.80ms/step 126,223tok/s (total 120,094,720 tok), 28.18% MFU\n",
      "step 7340: loss 4.62 132.47ms/step 123,682tok/s (total 120,258,560 tok), 27.61% MFU\n",
      "step 7350: loss 4.61 133.86ms/step 122,394tok/s (total 120,422,400 tok), 27.32% MFU\n",
      "step 7350 eval: val_loss 4.52\n",
      "step 7360: loss 4.53 132.25ms/step 123,889tok/s (total 120,586,240 tok), 27.66% MFU\n",
      "step 7370: loss 4.50 133.13ms/step 123,071tok/s (total 120,750,080 tok), 27.48% MFU\n",
      "step 7380: loss 4.54 132.99ms/step 123,196tok/s (total 120,913,920 tok), 27.50% MFU\n",
      "step 7390: loss 4.55 133.06ms/step 123,134tok/s (total 121,077,760 tok), 27.49% MFU\n",
      "step 7400: loss 4.61 132.89ms/step 123,287tok/s (total 121,241,600 tok), 27.52% MFU\n",
      "step 7400 eval: val_loss 4.53\n",
      "step 7410: loss 4.68 132.41ms/step 123,740tok/s (total 121,405,440 tok), 27.63% MFU\n",
      "step 7420: loss 4.52 134.21ms/step 122,076tok/s (total 121,569,280 tok), 27.25% MFU\n",
      "step 7430: loss 4.57 130.91ms/step 125,156tok/s (total 121,733,120 tok), 27.94% MFU\n",
      "step 7440: loss 4.46 132.53ms/step 123,626tok/s (total 121,896,960 tok), 27.60% MFU\n",
      "step 7450: loss 4.51 132.28ms/step 123,861tok/s (total 122,060,800 tok), 27.65% MFU\n",
      "step 7450 eval: val_loss 4.51\n",
      "step 7460: loss 4.53 133.53ms/step 122,699tok/s (total 122,224,640 tok), 27.39% MFU\n",
      "step 7470: loss 4.53 130.45ms/step 125,595tok/s (total 122,388,480 tok), 28.04% MFU\n",
      "step 7480: loss 4.59 132.63ms/step 123,531tok/s (total 122,552,320 tok), 27.58% MFU\n",
      "step 7490: loss 4.65 134.29ms/step 122,005tok/s (total 122,716,160 tok), 27.24% MFU\n",
      "step 7500: loss 4.53 130.68ms/step 125,378tok/s (total 122,880,000 tok), 27.99% MFU\n",
      "step 7500 eval: val_loss 4.57\n",
      "step 7510: loss 4.61 132.10ms/step 124,026tok/s (total 123,043,840 tok), 27.69% MFU\n",
      "step 7520: loss 4.50 131.30ms/step 124,781tok/s (total 123,207,680 tok), 27.86% MFU\n",
      "step 7530: loss 4.53 131.21ms/step 124,866tok/s (total 123,371,520 tok), 27.88% MFU\n",
      "step 7540: loss 4.57 134.21ms/step 122,079tok/s (total 123,535,360 tok), 27.25% MFU\n",
      "step 7550: loss 4.59 133.70ms/step 122,547tok/s (total 123,699,200 tok), 27.36% MFU\n",
      "step 7550 eval: val_loss 4.50\n",
      "step 7560: loss 4.54 132.54ms/step 123,618tok/s (total 123,863,040 tok), 27.60% MFU\n",
      "step 7570: loss 4.56 133.86ms/step 122,400tok/s (total 124,026,880 tok), 27.33% MFU\n",
      "step 7580: loss 4.56 132.33ms/step 123,812tok/s (total 124,190,720 tok), 27.64% MFU\n",
      "step 7590: loss 4.54 134.97ms/step 121,388tok/s (total 124,354,560 tok), 27.10% MFU\n",
      "step 7600: loss 4.55 131.42ms/step 124,673tok/s (total 124,518,400 tok), 27.83% MFU\n",
      "step 7600 eval: val_loss 4.59\n",
      "step 7610: loss 4.56 132.95ms/step 123,237tok/s (total 124,682,240 tok), 27.51% MFU\n",
      "step 7620: loss 4.57 132.76ms/step 123,410tok/s (total 124,846,080 tok), 27.55% MFU\n",
      "step 7630: loss 4.57 133.29ms/step 122,918tok/s (total 125,009,920 tok), 27.44% MFU\n",
      "step 7640: loss 4.57 132.15ms/step 123,982tok/s (total 125,173,760 tok), 27.68% MFU\n",
      "step 7650: loss 4.54 130.79ms/step 125,272tok/s (total 125,337,600 tok), 27.97% MFU\n",
      "step 7650 eval: val_loss 4.54\n",
      "step 7660: loss 4.56 135.00ms/step 121,367tok/s (total 125,501,440 tok), 27.10% MFU\n",
      "step 7670: loss 4.51 132.35ms/step 123,795tok/s (total 125,665,280 tok), 27.64% MFU\n",
      "step 7680: loss 4.63 134.25ms/step 122,042tok/s (total 125,829,120 tok), 27.25% MFU\n",
      "step 7690: loss 4.62 135.60ms/step 120,828tok/s (total 125,992,960 tok), 26.98% MFU\n",
      "step 7700: loss 4.55 132.91ms/step 123,267tok/s (total 126,156,800 tok), 27.52% MFU\n",
      "step 7700 eval: val_loss 4.55\n",
      "step 7710: loss 4.49 133.49ms/step 122,740tok/s (total 126,320,640 tok), 27.40% MFU\n",
      "step 7720: loss 4.59 132.42ms/step 123,725tok/s (total 126,484,480 tok), 27.62% MFU\n",
      "step 7730: loss 4.58 132.44ms/step 123,706tok/s (total 126,648,320 tok), 27.62% MFU\n",
      "step 7740: loss 4.60 135.34ms/step 121,061tok/s (total 126,812,160 tok), 27.03% MFU\n",
      "step 7750: loss 4.63 132.38ms/step 123,764tok/s (total 126,976,000 tok), 27.63% MFU\n",
      "step 7750 eval: val_loss 4.54\n",
      "step 7760: loss 4.52 131.58ms/step 124,515tok/s (total 127,139,840 tok), 27.80% MFU\n",
      "step 7770: loss 4.59 133.63ms/step 122,604tok/s (total 127,303,680 tok), 27.37% MFU\n",
      "step 7780: loss 4.51 134.25ms/step 122,044tok/s (total 127,467,520 tok), 27.25% MFU\n",
      "step 7790: loss 4.60 134.80ms/step 121,540tok/s (total 127,631,360 tok), 27.13% MFU\n",
      "step 7800: loss 4.53 133.68ms/step 122,557tok/s (total 127,795,200 tok), 27.36% MFU\n",
      "step 7800 eval: val_loss 4.59\n",
      "step 7810: loss 4.54 132.40ms/step 123,748tok/s (total 127,959,040 tok), 27.63% MFU\n",
      "step 7820: loss 4.56 132.15ms/step 123,983tok/s (total 128,122,880 tok), 27.68% MFU\n",
      "step 7830: loss 4.60 132.67ms/step 123,495tok/s (total 128,286,720 tok), 27.57% MFU\n",
      "step 7840: loss 4.57 133.06ms/step 123,132tok/s (total 128,450,560 tok), 27.49% MFU\n",
      "step 7850: loss 4.61 133.51ms/step 122,717tok/s (total 128,614,400 tok), 27.40% MFU\n",
      "step 7850 eval: val_loss 4.47\n",
      "step 7860: loss 4.43 134.46ms/step 121,854tok/s (total 128,778,240 tok), 27.20% MFU\n",
      "step 7870: loss 4.46 132.45ms/step 123,698tok/s (total 128,942,080 tok), 27.62% MFU\n",
      "step 7880: loss 4.55 132.22ms/step 123,917tok/s (total 129,105,920 tok), 27.66% MFU\n",
      "step 7890: loss 4.58 133.38ms/step 122,834tok/s (total 129,269,760 tok), 27.42% MFU\n",
      "step 7900: loss 4.46 132.79ms/step 123,380tok/s (total 129,433,600 tok), 27.54% MFU\n",
      "step 7900 eval: val_loss 4.54\n",
      "step 7910: loss 4.52 133.62ms/step 122,614tok/s (total 129,597,440 tok), 27.37% MFU\n",
      "step 7920: loss 4.55 135.53ms/step 120,889tok/s (total 129,761,280 tok), 26.99% MFU\n",
      "step 7930: loss 4.60 133.25ms/step 122,956tok/s (total 129,925,120 tok), 27.45% MFU\n",
      "step 7940: loss 4.53 131.80ms/step 124,313tok/s (total 130,088,960 tok), 27.75% MFU\n",
      "step 7950: loss 4.48 132.18ms/step 123,948tok/s (total 130,252,800 tok), 27.67% MFU\n",
      "step 7950 eval: val_loss 4.45\n",
      "step 7960: loss 4.63 134.84ms/step 121,503tok/s (total 130,416,640 tok), 27.13% MFU\n",
      "step 7970: loss 4.59 129.72ms/step 126,299tok/s (total 130,580,480 tok), 28.20% MFU\n",
      "step 7980: loss 4.60 133.45ms/step 122,770tok/s (total 130,744,320 tok), 27.41% MFU\n",
      "step 7990: loss 4.48 135.19ms/step 121,191tok/s (total 130,908,160 tok), 27.06% MFU\n",
      "step 8000: loss 4.61 132.58ms/step 123,575tok/s (total 131,072,000 tok), 27.59% MFU\n",
      "step 8000 eval: val_loss 4.51\n",
      "step 8010: loss 4.57 134.27ms/step 122,019tok/s (total 131,235,840 tok), 27.24% MFU\n",
      "step 8020: loss 4.60 135.37ms/step 121,029tok/s (total 131,399,680 tok), 27.02% MFU\n",
      "step 8030: loss 4.58 131.96ms/step 124,157tok/s (total 131,563,520 tok), 27.72% MFU\n",
      "step 8040: loss 4.55 130.98ms/step 125,085tok/s (total 131,727,360 tok), 27.93% MFU\n",
      "step 8050: loss 4.61 132.81ms/step 123,363tok/s (total 131,891,200 tok), 27.54% MFU\n",
      "step 8050 eval: val_loss 4.55\n",
      "step 8060: loss 4.53 131.64ms/step 124,460tok/s (total 132,055,040 tok), 27.79% MFU\n",
      "step 8070: loss 4.53 134.08ms/step 122,193tok/s (total 132,218,880 tok), 27.28% MFU\n",
      "step 8080: loss 4.51 135.15ms/step 121,232tok/s (total 132,382,720 tok), 27.07% MFU\n",
      "step 8090: loss 4.56 131.65ms/step 124,450tok/s (total 132,546,560 tok), 27.78% MFU\n",
      "step 8100: loss 4.52 134.46ms/step 121,848tok/s (total 132,710,400 tok), 27.20% MFU\n",
      "step 8100 eval: val_loss 4.50\n",
      "step 8110: loss 4.47 132.54ms/step 123,611tok/s (total 132,874,240 tok), 27.60% MFU\n",
      "step 8120: loss 4.51 133.61ms/step 122,626tok/s (total 133,038,080 tok), 27.38% MFU\n",
      "step 8130: loss 4.56 134.44ms/step 121,867tok/s (total 133,201,920 tok), 27.21% MFU\n",
      "step 8140: loss 4.55 132.73ms/step 123,439tok/s (total 133,365,760 tok), 27.56% MFU\n",
      "step 8150: loss 4.52 133.91ms/step 122,351tok/s (total 133,529,600 tok), 27.32% MFU\n",
      "step 8150 eval: val_loss 4.40\n",
      "step 8160: loss 4.58 132.49ms/step 123,664tok/s (total 133,693,440 tok), 27.61% MFU\n",
      "step 8170: loss 4.57 131.17ms/step 124,911tok/s (total 133,857,280 tok), 27.89% MFU\n",
      "step 8180: loss 4.61 133.98ms/step 122,286tok/s (total 134,021,120 tok), 27.30% MFU\n",
      "step 8190: loss 4.59 133.78ms/step 122,474tok/s (total 134,184,960 tok), 27.34% MFU\n",
      "step 8200: loss 4.51 130.22ms/step 125,821tok/s (total 134,348,800 tok), 28.09% MFU\n",
      "step 8200 eval: val_loss 4.53\n",
      "step 8210: loss 4.52 134.51ms/step 121,809tok/s (total 134,512,640 tok), 27.19% MFU\n",
      "step 8220: loss 4.49 134.25ms/step 122,042tok/s (total 134,676,480 tok), 27.25% MFU\n",
      "step 8230: loss 4.49 133.99ms/step 122,279tok/s (total 134,840,320 tok), 27.30% MFU\n",
      "step 8240: loss 4.47 134.16ms/step 122,122tok/s (total 135,004,160 tok), 27.26% MFU\n",
      "step 8250: loss 4.55 134.22ms/step 122,069tok/s (total 135,168,000 tok), 27.25% MFU\n",
      "step 8250 eval: val_loss 4.52\n",
      "step 8260: loss 4.48 135.14ms/step 121,235tok/s (total 135,331,840 tok), 27.07% MFU\n",
      "step 8270: loss 4.61 134.21ms/step 122,078tok/s (total 135,495,680 tok), 27.25% MFU\n",
      "step 8280: loss 4.60 133.77ms/step 122,474tok/s (total 135,659,520 tok), 27.34% MFU\n",
      "step 8290: loss 4.46 135.53ms/step 120,888tok/s (total 135,823,360 tok), 26.99% MFU\n",
      "step 8300: loss 4.51 134.37ms/step 121,936tok/s (total 135,987,200 tok), 27.22% MFU\n",
      "step 8300 eval: val_loss 4.53\n",
      "step 8310: loss 4.52 134.95ms/step 121,412tok/s (total 136,151,040 tok), 27.11% MFU\n",
      "step 8320: loss 4.56 134.40ms/step 121,906tok/s (total 136,314,880 tok), 27.22% MFU\n",
      "step 8330: loss 4.61 134.14ms/step 122,139tok/s (total 136,478,720 tok), 27.27% MFU\n",
      "step 8340: loss 4.49 133.93ms/step 122,336tok/s (total 136,642,560 tok), 27.31% MFU\n",
      "step 8350: loss 4.50 134.62ms/step 121,701tok/s (total 136,806,400 tok), 27.17% MFU\n",
      "step 8350 eval: val_loss 4.45\n",
      "step 8360: loss 4.55 134.63ms/step 121,698tok/s (total 136,970,240 tok), 27.17% MFU\n",
      "step 8370: loss 4.48 134.78ms/step 121,559tok/s (total 137,134,080 tok), 27.14% MFU\n",
      "step 8380: loss 4.55 134.93ms/step 121,427tok/s (total 137,297,920 tok), 27.11% MFU\n",
      "step 8390: loss 4.58 133.96ms/step 122,308tok/s (total 137,461,760 tok), 27.31% MFU\n",
      "step 8400: loss 4.54 133.55ms/step 122,676tok/s (total 137,625,600 tok), 27.39% MFU\n",
      "step 8400 eval: val_loss 4.48\n",
      "step 8410: loss 4.57 134.31ms/step 121,989tok/s (total 137,789,440 tok), 27.23% MFU\n",
      "step 8420: loss 4.48 134.45ms/step 121,862tok/s (total 137,953,280 tok), 27.21% MFU\n",
      "step 8430: loss 4.43 135.02ms/step 121,343tok/s (total 138,117,120 tok), 27.09% MFU\n",
      "step 8440: loss 4.43 134.37ms/step 121,936tok/s (total 138,280,960 tok), 27.22% MFU\n",
      "step 8450: loss 4.46 135.05ms/step 121,321tok/s (total 138,444,800 tok), 27.09% MFU\n",
      "step 8450 eval: val_loss 4.46\n",
      "step 8460: loss 4.52 134.55ms/step 121,768tok/s (total 138,608,640 tok), 27.18% MFU\n",
      "step 8470: loss 4.51 134.20ms/step 122,090tok/s (total 138,772,480 tok), 27.26% MFU\n",
      "step 8480: loss 4.48 134.46ms/step 121,848tok/s (total 138,936,320 tok), 27.20% MFU\n",
      "step 8490: loss 4.53 134.90ms/step 121,452tok/s (total 139,100,160 tok), 27.11% MFU\n",
      "step 8500: loss 4.47 135.06ms/step 121,309tok/s (total 139,264,000 tok), 27.08% MFU\n",
      "step 8500 eval: val_loss 4.49\n",
      "step 8510: loss 4.57 132.08ms/step 124,045tok/s (total 139,427,840 tok), 27.69% MFU\n",
      "step 8520: loss 4.53 134.16ms/step 122,119tok/s (total 139,591,680 tok), 27.26% MFU\n",
      "step 8530: loss 4.58 133.45ms/step 122,776tok/s (total 139,755,520 tok), 27.41% MFU\n",
      "step 8540: loss 4.53 131.64ms/step 124,458tok/s (total 139,919,360 tok), 27.79% MFU\n",
      "step 8550: loss 4.52 131.42ms/step 124,673tok/s (total 140,083,200 tok), 27.83% MFU\n",
      "step 8550 eval: val_loss 4.50\n",
      "step 8560: loss 4.44 134.66ms/step 121,669tok/s (total 140,247,040 tok), 27.16% MFU\n",
      "step 8570: loss 4.47 132.70ms/step 123,465tok/s (total 140,410,880 tok), 27.56% MFU\n",
      "step 8580: loss 4.49 133.55ms/step 122,678tok/s (total 140,574,720 tok), 27.39% MFU\n",
      "step 8590: loss 4.49 134.00ms/step 122,269tok/s (total 140,738,560 tok), 27.30% MFU\n",
      "step 8600: loss 4.48 133.07ms/step 123,126tok/s (total 140,902,400 tok), 27.49% MFU\n",
      "step 8600 eval: val_loss 4.59\n",
      "step 8610: loss 4.56 133.79ms/step 122,457tok/s (total 141,066,240 tok), 27.34% MFU\n",
      "step 8620: loss 4.58 135.33ms/step 121,064tok/s (total 141,230,080 tok), 27.03% MFU\n",
      "step 8630: loss 4.51 137.32ms/step 119,313tok/s (total 141,393,920 tok), 26.64% MFU\n",
      "step 8640: loss 4.55 132.91ms/step 123,273tok/s (total 141,557,760 tok), 27.52% MFU\n",
      "step 8650: loss 4.53 136.79ms/step 119,775tok/s (total 141,721,600 tok), 26.74% MFU\n",
      "step 8650 eval: val_loss 4.50\n",
      "step 8660: loss 4.54 135.80ms/step 120,649tok/s (total 141,885,440 tok), 26.94% MFU\n",
      "step 8670: loss 4.59 135.37ms/step 121,028tok/s (total 142,049,280 tok), 27.02% MFU\n",
      "step 8680: loss 4.50 134.65ms/step 121,681tok/s (total 142,213,120 tok), 27.17% MFU\n",
      "step 8690: loss 4.50 136.99ms/step 119,599tok/s (total 142,376,960 tok), 26.70% MFU\n",
      "step 8700: loss 4.49 136.00ms/step 120,467tok/s (total 142,540,800 tok), 26.89% MFU\n",
      "step 8700 eval: val_loss 4.58\n",
      "step 8710: loss 4.53 134.38ms/step 121,921tok/s (total 142,704,640 tok), 27.22% MFU\n",
      "step 8720: loss 4.49 135.43ms/step 120,979tok/s (total 142,868,480 tok), 27.01% MFU\n",
      "step 8730: loss 4.56 134.45ms/step 121,861tok/s (total 143,032,320 tok), 27.21% MFU\n",
      "step 8740: loss 4.58 134.63ms/step 121,694tok/s (total 143,196,160 tok), 27.17% MFU\n",
      "step 8750: loss 4.44 133.66ms/step 122,580tok/s (total 143,360,000 tok), 27.37% MFU\n",
      "step 8750 eval: val_loss 4.48\n",
      "step 8760: loss 4.50 132.80ms/step 123,373tok/s (total 143,523,840 tok), 27.54% MFU\n",
      "step 8770: loss 4.46 135.17ms/step 121,208tok/s (total 143,687,680 tok), 27.06% MFU\n",
      "step 8780: loss 4.40 133.87ms/step 122,383tok/s (total 143,851,520 tok), 27.32% MFU\n",
      "step 8790: loss 4.54 135.20ms/step 121,181tok/s (total 144,015,360 tok), 27.05% MFU\n",
      "step 8800: loss 4.49 134.72ms/step 121,613tok/s (total 144,179,200 tok), 27.15% MFU\n",
      "step 8800 eval: val_loss 4.44\n",
      "step 8810: loss 4.55 132.32ms/step 123,818tok/s (total 144,343,040 tok), 27.64% MFU\n",
      "step 8820: loss 4.50 133.54ms/step 122,692tok/s (total 144,506,880 tok), 27.39% MFU\n",
      "step 8830: loss 4.55 135.46ms/step 120,953tok/s (total 144,670,720 tok), 27.00% MFU\n",
      "step 8840: loss 4.58 134.02ms/step 122,251tok/s (total 144,834,560 tok), 27.29% MFU\n",
      "step 8850: loss 4.55 133.83ms/step 122,428tok/s (total 144,998,400 tok), 27.33% MFU\n",
      "step 8850 eval: val_loss 4.47\n",
      "step 8860: loss 4.44 134.77ms/step 121,574tok/s (total 145,162,240 tok), 27.14% MFU\n",
      "step 8870: loss 4.58 134.36ms/step 121,939tok/s (total 145,326,080 tok), 27.22% MFU\n",
      "step 8880: loss 4.45 134.70ms/step 121,636tok/s (total 145,489,920 tok), 27.16% MFU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "for curr_step in range(1, 1_000_000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "\n",
    "    # evaluate the loss\n",
    "    step_start = time.time()\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "    \n",
    "    if curr_step % log_interval == 0:\n",
    "        mfu = model.calculate_mfu(step_time)\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):,.0f}tok/s (total {tok_total:,} tok), {mfu * 100:.2f}% MFU\")\n",
    "        wandb.log({\n",
    "            \"step\": curr_step,\n",
    "            \"tokens\": tok_total,\n",
    "            \"loss_train\": loss.detach().item(), \n",
    "            \"mfu\": mfu * 100, \n",
    "            \"tokens/s\": tok_step/step_time\n",
    "        })\n",
    "\n",
    "        \n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "        wandb.log({\n",
    "            \"step\": curr_step,\n",
    "            \"tokens\": tok_total,\n",
    "            \"loss_val\": loss.detach().item()\n",
    "        })\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(encode(\"hello my name is\").to(device).unsqueeze(0), 10)\n",
    "        #     print(decode(output))\n",
    "        model.train()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b477c-97b4-48b4-834d-6f334ea078a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"Blue is\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
