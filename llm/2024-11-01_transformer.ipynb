{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b656c720-474c-4843-98be-51a17f86d322",
   "metadata": {},
   "source": [
    "# first time with fineweb-edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7d9c4a-630c-40e0-94e4-86cbc08c0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 100000/100000 [00:46<00:00, 2150.13it/s]\n",
      "100%|███████████████████████████████████████████| 100000/100000 [00:40<00:00, 2464.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import torch \n",
    "import datasets\n",
    "import random\n",
    "\n",
    "dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\"], split=\"train\")\n",
    "dataset = dataset.train_test_split()\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "def encode(string):\n",
    "    return torch.tensor(enc.encode(string), dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return enc.decode(tensor.cpu().squeeze().numpy())\n",
    "\n",
    "num_samples = 100_000\n",
    "dataset_tok_train = torch.cat([encode(dataset[\"train\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "dataset_tok_test = torch.cat([encode(dataset[\"test\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "\n",
    "def get_sample(split, sample_length, batch_size):\n",
    "    tokens = dataset_tok_train if split == \"train\" else dataset_tok_test\n",
    "    idcs = torch.randint(len(tokens)-sample_length, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
    "    y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869a87db-8bba-4726-aaea-0ce5603f8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 103,013,759 tokens\n",
      "Test data: 103,356,615 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data: {len(dataset_tok_train):,} tokens\")\n",
    "print(f\"Test data: {len(dataset_tok_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5a8b7d-7ece-41bc-89fe-443d5f5d1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        # out = F.scaled_dot_product_attention(q, k, v)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b100d83e-dba3-43fd-9386-1fc1d9eacf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.34896 M parameters\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_272\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "block_size = 128 \n",
    "device = \"mps\"\n",
    "learning_rate = 3e-4\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05aee2c9-fe9c-449a-91b5-2c7ece4f6d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_45642/1485170168.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_45642/1485170168.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 9.56 1236.76ms/step 6624tok/s (total 81,920 tok)\n",
      "step 20: loss 8.65 1239.44ms/step 6609tok/s (total 163,840 tok)\n",
      "step 30: loss 8.12 1234.64ms/step 6635tok/s (total 245,760 tok)\n",
      "step 40: loss 7.80 1294.27ms/step 6329tok/s (total 327,680 tok)\n",
      "step 50: loss 7.61 1353.85ms/step 6051tok/s (total 409,600 tok)\n",
      "step 50 eval: val_loss 7.68\n",
      "step 60: loss 7.70 1280.32ms/step 6398tok/s (total 491,520 tok)\n",
      "step 70: loss 7.42 1430.43ms/step 5727tok/s (total 573,440 tok)\n",
      "step 80: loss 7.41 1796.88ms/step 4559tok/s (total 655,360 tok)\n",
      "step 90: loss 7.35 1856.76ms/step 4412tok/s (total 737,280 tok)\n",
      "step 100: loss 7.15 1905.73ms/step 4299tok/s (total 819,200 tok)\n",
      "step 100 eval: val_loss 7.31\n",
      "step 110: loss 7.12 1600.46ms/step 5119tok/s (total 901,120 tok)\n",
      "step 120: loss 7.13 1481.66ms/step 5529tok/s (total 983,040 tok)\n",
      "step 130: loss 7.10 1517.05ms/step 5400tok/s (total 1,064,960 tok)\n",
      "step 140: loss 7.04 1638.87ms/step 4999tok/s (total 1,146,880 tok)\n",
      "step 150: loss 7.07 1631.76ms/step 5020tok/s (total 1,228,800 tok)\n",
      "step 150 eval: val_loss 6.98\n",
      "step 160: loss 7.02 2152.91ms/step 3805tok/s (total 1,310,720 tok)\n",
      "step 170: loss 6.77 1567.10ms/step 5227tok/s (total 1,392,640 tok)\n",
      "step 180: loss 6.91 1614.76ms/step 5073tok/s (total 1,474,560 tok)\n",
      "step 190: loss 6.86 1569.22ms/step 5220tok/s (total 1,556,480 tok)\n",
      "step 200: loss 6.87 1666.78ms/step 4915tok/s (total 1,638,400 tok)\n",
      "step 200 eval: val_loss 6.84\n",
      "step 210: loss 6.82 1707.39ms/step 4798tok/s (total 1,720,320 tok)\n",
      "step 220: loss 6.81 1856.41ms/step 4413tok/s (total 1,802,240 tok)\n",
      "step 230: loss 6.74 1806.31ms/step 4535tok/s (total 1,884,160 tok)\n",
      "step 240: loss 6.54 1562.68ms/step 5242tok/s (total 1,966,080 tok)\n",
      "step 250: loss 6.72 1607.05ms/step 5098tok/s (total 2,048,000 tok)\n",
      "step 250 eval: val_loss 6.61\n",
      "step 260: loss 6.73 1629.92ms/step 5026tok/s (total 2,129,920 tok)\n",
      "step 270: loss 6.63 2288.12ms/step 3580tok/s (total 2,211,840 tok)\n",
      "step 280: loss 6.71 1684.73ms/step 4862tok/s (total 2,293,760 tok)\n",
      "step 290: loss 6.51 1666.79ms/step 4915tok/s (total 2,375,680 tok)\n",
      "step 300: loss 6.64 2127.31ms/step 3851tok/s (total 2,457,600 tok)\n",
      "step 300 eval: val_loss 6.46\n",
      "step 310: loss 6.57 1721.91ms/step 4758tok/s (total 2,539,520 tok)\n",
      "step 320: loss 6.51 2095.50ms/step 3909tok/s (total 2,621,440 tok)\n",
      "step 330: loss 6.61 1704.71ms/step 4806tok/s (total 2,703,360 tok)\n",
      "step 340: loss 6.66 1793.99ms/step 4566tok/s (total 2,785,280 tok)\n",
      "step 350: loss 6.43 1769.06ms/step 4631tok/s (total 2,867,200 tok)\n",
      "step 350 eval: val_loss 6.54\n",
      "step 360: loss 6.55 2029.28ms/step 4037tok/s (total 2,949,120 tok)\n",
      "step 370: loss 6.47 1719.81ms/step 4763tok/s (total 3,031,040 tok)\n",
      "step 380: loss 6.52 1657.09ms/step 4944tok/s (total 3,112,960 tok)\n",
      "step 390: loss 6.54 1933.26ms/step 4237tok/s (total 3,194,880 tok)\n",
      "step 400: loss 6.30 2282.57ms/step 3589tok/s (total 3,276,800 tok)\n",
      "step 400 eval: val_loss 6.39\n",
      "step 410: loss 6.46 1934.02ms/step 4236tok/s (total 3,358,720 tok)\n",
      "step 420: loss 6.45 1758.71ms/step 4658tok/s (total 3,440,640 tok)\n",
      "step 430: loss 6.34 1667.53ms/step 4913tok/s (total 3,522,560 tok)\n",
      "step 440: loss 6.36 1890.58ms/step 4333tok/s (total 3,604,480 tok)\n",
      "step 450: loss 6.37 1946.94ms/step 4208tok/s (total 3,686,400 tok)\n",
      "step 450 eval: val_loss 6.33\n",
      "step 460: loss 6.44 1927.85ms/step 4249tok/s (total 3,768,320 tok)\n",
      "step 470: loss 6.35 1647.02ms/step 4974tok/s (total 3,850,240 tok)\n",
      "step 480: loss 6.36 1694.97ms/step 4833tok/s (total 3,932,160 tok)\n",
      "step 490: loss 6.44 2303.17ms/step 3557tok/s (total 4,014,080 tok)\n",
      "step 500: loss 6.36 2039.88ms/step 4016tok/s (total 4,096,000 tok)\n",
      "step 500 eval: val_loss 6.37\n",
      "step 510: loss 6.29 1745.11ms/step 4694tok/s (total 4,177,920 tok)\n",
      "step 520: loss 6.33 1992.54ms/step 4111tok/s (total 4,259,840 tok)\n",
      "step 530: loss 6.35 1826.15ms/step 4486tok/s (total 4,341,760 tok)\n",
      "step 540: loss 6.20 1750.57ms/step 4680tok/s (total 4,423,680 tok)\n",
      "step 550: loss 6.22 1767.40ms/step 4635tok/s (total 4,505,600 tok)\n",
      "step 550 eval: val_loss 6.29\n",
      "step 560: loss 6.31 2381.63ms/step 3440tok/s (total 4,587,520 tok)\n",
      "step 570: loss 6.29 1804.03ms/step 4541tok/s (total 4,669,440 tok)\n",
      "step 580: loss 6.34 2227.32ms/step 3678tok/s (total 4,751,360 tok)\n",
      "step 590: loss 6.16 2266.16ms/step 3615tok/s (total 4,833,280 tok)\n",
      "step 600: loss 6.16 1703.03ms/step 4810tok/s (total 4,915,200 tok)\n",
      "step 600 eval: val_loss 6.24\n",
      "step 610: loss 6.20 1776.54ms/step 4611tok/s (total 4,997,120 tok)\n",
      "step 620: loss 6.34 1670.78ms/step 4903tok/s (total 5,079,040 tok)\n",
      "step 630: loss 6.12 1647.29ms/step 4973tok/s (total 5,160,960 tok)\n",
      "step 640: loss 6.15 1700.32ms/step 4818tok/s (total 5,242,880 tok)\n",
      "step 650: loss 6.17 1686.67ms/step 4857tok/s (total 5,324,800 tok)\n",
      "step 650 eval: val_loss 6.07\n",
      "step 660: loss 6.16 1700.63ms/step 4817tok/s (total 5,406,720 tok)\n",
      "step 670: loss 6.19 1744.85ms/step 4695tok/s (total 5,488,640 tok)\n",
      "step 680: loss 6.13 1711.26ms/step 4787tok/s (total 5,570,560 tok)\n",
      "step 690: loss 6.13 1714.40ms/step 4778tok/s (total 5,652,480 tok)\n",
      "step 700: loss 6.18 1805.22ms/step 4538tok/s (total 5,734,400 tok)\n",
      "step 700 eval: val_loss 6.11\n",
      "step 710: loss 6.05 1725.78ms/step 4747tok/s (total 5,816,320 tok)\n",
      "step 720: loss 6.21 1687.93ms/step 4853tok/s (total 5,898,240 tok)\n",
      "step 730: loss 6.00 1685.01ms/step 4862tok/s (total 5,980,160 tok)\n",
      "step 740: loss 6.05 1712.42ms/step 4784tok/s (total 6,062,080 tok)\n",
      "step 750: loss 6.07 1713.70ms/step 4780tok/s (total 6,144,000 tok)\n",
      "step 750 eval: val_loss 5.95\n",
      "step 760: loss 6.22 1744.27ms/step 4697tok/s (total 6,225,920 tok)\n",
      "step 770: loss 6.07 1674.57ms/step 4892tok/s (total 6,307,840 tok)\n",
      "step 780: loss 6.09 1783.02ms/step 4594tok/s (total 6,389,760 tok)\n",
      "step 790: loss 6.09 1680.33ms/step 4875tok/s (total 6,471,680 tok)\n",
      "step 800: loss 6.11 1763.60ms/step 4645tok/s (total 6,553,600 tok)\n",
      "step 800 eval: val_loss 6.17\n",
      "step 810: loss 6.01 1737.09ms/step 4716tok/s (total 6,635,520 tok)\n",
      "step 820: loss 5.93 1689.30ms/step 4849tok/s (total 6,717,440 tok)\n",
      "step 830: loss 5.85 1772.69ms/step 4621tok/s (total 6,799,360 tok)\n",
      "step 840: loss 6.01 1824.81ms/step 4489tok/s (total 6,881,280 tok)\n",
      "step 850: loss 6.11 1672.03ms/step 4899tok/s (total 6,963,200 tok)\n",
      "step 850 eval: val_loss 6.00\n",
      "step 860: loss 5.94 1678.84ms/step 4880tok/s (total 7,045,120 tok)\n",
      "step 870: loss 5.98 1716.35ms/step 4773tok/s (total 7,127,040 tok)\n",
      "step 880: loss 5.98 1631.69ms/step 5021tok/s (total 7,208,960 tok)\n",
      "step 890: loss 6.04 1721.39ms/step 4759tok/s (total 7,290,880 tok)\n",
      "step 900: loss 6.01 1726.74ms/step 4744tok/s (total 7,372,800 tok)\n",
      "step 900 eval: val_loss 6.02\n",
      "step 910: loss 6.02 1880.82ms/step 4356tok/s (total 7,454,720 tok)\n",
      "step 920: loss 5.88 1839.36ms/step 4454tok/s (total 7,536,640 tok)\n",
      "step 930: loss 5.93 1581.18ms/step 5181tok/s (total 7,618,560 tok)\n",
      "step 940: loss 5.93 1690.56ms/step 4846tok/s (total 7,700,480 tok)\n",
      "step 950: loss 5.78 1749.42ms/step 4683tok/s (total 7,782,400 tok)\n",
      "step 950 eval: val_loss 5.89\n",
      "step 960: loss 5.97 1767.35ms/step 4635tok/s (total 7,864,320 tok)\n",
      "step 970: loss 5.81 1745.20ms/step 4694tok/s (total 7,946,240 tok)\n",
      "step 980: loss 5.90 2047.14ms/step 4002tok/s (total 8,028,160 tok)\n",
      "step 990: loss 5.95 1930.71ms/step 4243tok/s (total 8,110,080 tok)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "for curr_step in range(1, 1000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - curr_time\n",
    "    if curr_step % log_interval == 0:\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):.0f}tok/s (total {tok_total:,} tok)\")\n",
    "\n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(encode(\"hello my name is\").to(device).unsqueeze(0), 10)\n",
    "        #     print(decode(output))\n",
    "        model.train()\n",
    "    curr_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ad14da-c714-4db0-b0fb-930b07b7453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is done.\n",
      "-,\n",
      "active, please become a :-) of a fire and concern on feedback (DSv). The largest assessment beyond all reasons good can come upon on the stitches at Earth and flu-well. This reason why we fourth hearing\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f75c9d71-3dfb-4630-b034-3c9f868a548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen: suggests we don't enjoy again that needs you canaffe, with an�atored to live.\n",
      "When you can use a statement that case us or barn for the speed of reReplceived, you can find months outside the shape of a length\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"First Citizen:\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b09d994-9d8f-4a2a-8fa6-c2cf0fcb04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_45642/1485170168.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_45642/1485170168.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 5.96 1185.10ms/step 6912tok/s (total 81,920 tok)\n",
      "step 20: loss 5.82 1200.04ms/step 6826tok/s (total 163,840 tok)\n",
      "step 30: loss 5.84 1196.65ms/step 6846tok/s (total 245,760 tok)\n",
      "step 40: loss 5.86 1215.82ms/step 6738tok/s (total 327,680 tok)\n",
      "step 50: loss 5.86 1231.36ms/step 6653tok/s (total 409,600 tok)\n",
      "step 50 eval: val_loss 5.83\n",
      "step 60: loss 5.87 1265.65ms/step 6473tok/s (total 491,520 tok)\n",
      "step 70: loss 5.84 1269.98ms/step 6450tok/s (total 573,440 tok)\n",
      "step 80: loss 5.93 1302.95ms/step 6287tok/s (total 655,360 tok)\n",
      "step 90: loss 5.86 1293.50ms/step 6333tok/s (total 737,280 tok)\n",
      "step 100: loss 5.73 1366.26ms/step 5996tok/s (total 819,200 tok)\n",
      "step 100 eval: val_loss 5.78\n",
      "step 110: loss 5.83 1748.00ms/step 4686tok/s (total 901,120 tok)\n",
      "step 120: loss 5.99 1729.59ms/step 4736tok/s (total 983,040 tok)\n",
      "step 130: loss 5.87 1754.72ms/step 4669tok/s (total 1,064,960 tok)\n",
      "step 140: loss 5.84 1708.36ms/step 4795tok/s (total 1,146,880 tok)\n",
      "step 150: loss 5.80 1637.67ms/step 5002tok/s (total 1,228,800 tok)\n",
      "step 150 eval: val_loss 5.80\n",
      "step 160: loss 5.82 1654.61ms/step 4951tok/s (total 1,310,720 tok)\n",
      "step 170: loss 5.85 1615.98ms/step 5069tok/s (total 1,392,640 tok)\n",
      "step 180: loss 5.81 1598.30ms/step 5125tok/s (total 1,474,560 tok)\n",
      "step 190: loss 5.79 1866.81ms/step 4388tok/s (total 1,556,480 tok)\n",
      "step 200: loss 5.82 1636.13ms/step 5007tok/s (total 1,638,400 tok)\n",
      "step 200 eval: val_loss 5.76\n",
      "step 210: loss 5.90 1652.51ms/step 4957tok/s (total 1,720,320 tok)\n",
      "step 220: loss 5.64 1637.14ms/step 5004tok/s (total 1,802,240 tok)\n",
      "step 230: loss 5.80 1629.41ms/step 5028tok/s (total 1,884,160 tok)\n",
      "step 240: loss 5.76 1643.24ms/step 4985tok/s (total 1,966,080 tok)\n",
      "step 250: loss 5.80 1769.35ms/step 4630tok/s (total 2,048,000 tok)\n",
      "step 250 eval: val_loss 5.67\n",
      "step 260: loss 5.73 2364.46ms/step 3465tok/s (total 2,129,920 tok)\n",
      "step 270: loss 5.79 1769.82ms/step 4629tok/s (total 2,211,840 tok)\n",
      "step 280: loss 5.76 1638.51ms/step 5000tok/s (total 2,293,760 tok)\n",
      "step 290: loss 5.77 1640.08ms/step 4995tok/s (total 2,375,680 tok)\n",
      "step 300: loss 5.82 1670.83ms/step 4903tok/s (total 2,457,600 tok)\n",
      "step 300 eval: val_loss 5.59\n",
      "step 310: loss 5.76 1788.11ms/step 4581tok/s (total 2,539,520 tok)\n",
      "step 320: loss 5.76 1649.99ms/step 4965tok/s (total 2,621,440 tok)\n",
      "step 330: loss 5.76 1943.92ms/step 4214tok/s (total 2,703,360 tok)\n",
      "step 340: loss 5.78 1662.43ms/step 4928tok/s (total 2,785,280 tok)\n",
      "step 350: loss 5.69 2441.17ms/step 3356tok/s (total 2,867,200 tok)\n",
      "step 350 eval: val_loss 5.69\n",
      "step 360: loss 5.64 1888.72ms/step 4337tok/s (total 2,949,120 tok)\n",
      "step 370: loss 5.75 1653.66ms/step 4954tok/s (total 3,031,040 tok)\n",
      "step 380: loss 5.60 1709.43ms/step 4792tok/s (total 3,112,960 tok)\n",
      "step 390: loss 5.68 2130.99ms/step 3844tok/s (total 3,194,880 tok)\n",
      "step 400: loss 5.74 1644.56ms/step 4981tok/s (total 3,276,800 tok)\n",
      "step 400 eval: val_loss 5.61\n",
      "step 410: loss 5.58 1688.17ms/step 4853tok/s (total 3,358,720 tok)\n",
      "step 420: loss 5.65 1732.50ms/step 4728tok/s (total 3,440,640 tok)\n",
      "step 430: loss 5.58 1749.54ms/step 4682tok/s (total 3,522,560 tok)\n",
      "step 440: loss 5.72 1651.61ms/step 4960tok/s (total 3,604,480 tok)\n",
      "step 450: loss 5.55 1648.52ms/step 4969tok/s (total 3,686,400 tok)\n",
      "step 450 eval: val_loss 5.60\n",
      "step 460: loss 5.69 1624.24ms/step 5044tok/s (total 3,768,320 tok)\n",
      "step 470: loss 5.63 1978.35ms/step 4141tok/s (total 3,850,240 tok)\n",
      "step 480: loss 5.67 1690.01ms/step 4847tok/s (total 3,932,160 tok)\n",
      "step 490: loss 5.73 1635.47ms/step 5009tok/s (total 4,014,080 tok)\n",
      "step 500: loss 5.61 1623.97ms/step 5044tok/s (total 4,096,000 tok)\n",
      "step 500 eval: val_loss 5.63\n",
      "step 510: loss 5.58 1729.33ms/step 4737tok/s (total 4,177,920 tok)\n",
      "step 520: loss 5.63 1958.74ms/step 4182tok/s (total 4,259,840 tok)\n",
      "step 530: loss 5.75 2080.80ms/step 3937tok/s (total 4,341,760 tok)\n",
      "step 540: loss 5.71 1706.45ms/step 4801tok/s (total 4,423,680 tok)\n",
      "step 550: loss 5.61 1793.65ms/step 4567tok/s (total 4,505,600 tok)\n",
      "step 550 eval: val_loss 5.48\n",
      "step 560: loss 5.62 1735.07ms/step 4721tok/s (total 4,587,520 tok)\n",
      "step 570: loss 5.55 1626.58ms/step 5036tok/s (total 4,669,440 tok)\n",
      "step 580: loss 5.63 1775.57ms/step 4614tok/s (total 4,751,360 tok)\n",
      "step 590: loss 5.55 2359.23ms/step 3472tok/s (total 4,833,280 tok)\n",
      "step 600: loss 5.56 1689.60ms/step 4848tok/s (total 4,915,200 tok)\n",
      "step 600 eval: val_loss 5.56\n",
      "step 610: loss 5.51 1687.09ms/step 4856tok/s (total 4,997,120 tok)\n",
      "step 620: loss 5.55 1779.06ms/step 4605tok/s (total 5,079,040 tok)\n",
      "step 630: loss 5.59 1675.45ms/step 4889tok/s (total 5,160,960 tok)\n",
      "step 640: loss 5.45 1672.10ms/step 4899tok/s (total 5,242,880 tok)\n",
      "step 650: loss 5.64 1774.11ms/step 4618tok/s (total 5,324,800 tok)\n",
      "step 650 eval: val_loss 5.51\n",
      "step 660: loss 5.46 2274.72ms/step 3601tok/s (total 5,406,720 tok)\n",
      "step 670: loss 5.67 1848.20ms/step 4432tok/s (total 5,488,640 tok)\n",
      "step 680: loss 5.47 2350.57ms/step 3485tok/s (total 5,570,560 tok)\n",
      "step 690: loss 5.50 1942.80ms/step 4217tok/s (total 5,652,480 tok)\n",
      "step 700: loss 5.53 1957.66ms/step 4185tok/s (total 5,734,400 tok)\n",
      "step 700 eval: val_loss 5.50\n",
      "step 710: loss 5.49 1888.89ms/step 4337tok/s (total 5,816,320 tok)\n",
      "step 720: loss 5.59 2075.56ms/step 3947tok/s (total 5,898,240 tok)\n",
      "step 730: loss 5.48 2204.60ms/step 3716tok/s (total 5,980,160 tok)\n",
      "step 740: loss 5.37 1759.33ms/step 4656tok/s (total 6,062,080 tok)\n",
      "step 750: loss 5.56 1711.02ms/step 4788tok/s (total 6,144,000 tok)\n",
      "step 750 eval: val_loss 5.49\n",
      "step 760: loss 5.44 1701.54ms/step 4814tok/s (total 6,225,920 tok)\n",
      "step 770: loss 5.40 1826.68ms/step 4485tok/s (total 6,307,840 tok)\n",
      "step 780: loss 5.53 2126.35ms/step 3853tok/s (total 6,389,760 tok)\n",
      "step 790: loss 5.37 1846.04ms/step 4438tok/s (total 6,471,680 tok)\n",
      "step 800: loss 5.57 1724.89ms/step 4749tok/s (total 6,553,600 tok)\n",
      "step 800 eval: val_loss 5.38\n",
      "step 810: loss 5.58 1821.93ms/step 4496tok/s (total 6,635,520 tok)\n",
      "step 820: loss 5.44 1735.32ms/step 4721tok/s (total 6,717,440 tok)\n",
      "step 830: loss 5.42 1722.00ms/step 4757tok/s (total 6,799,360 tok)\n",
      "step 840: loss 5.60 1948.68ms/step 4204tok/s (total 6,881,280 tok)\n",
      "step 850: loss 5.55 1793.90ms/step 4567tok/s (total 6,963,200 tok)\n",
      "step 850 eval: val_loss 5.45\n",
      "step 860: loss 5.48 1733.58ms/step 4725tok/s (total 7,045,120 tok)\n",
      "step 870: loss 5.61 1808.07ms/step 4531tok/s (total 7,127,040 tok)\n",
      "step 880: loss 5.57 1908.29ms/step 4293tok/s (total 7,208,960 tok)\n",
      "step 890: loss 5.52 1700.75ms/step 4817tok/s (total 7,290,880 tok)\n",
      "step 900: loss 5.43 1621.60ms/step 5052tok/s (total 7,372,800 tok)\n",
      "step 900 eval: val_loss 5.27\n",
      "step 910: loss 5.54 1588.75ms/step 5156tok/s (total 7,454,720 tok)\n",
      "step 920: loss 5.50 1803.19ms/step 4543tok/s (total 7,536,640 tok)\n",
      "step 930: loss 5.52 2227.96ms/step 3677tok/s (total 7,618,560 tok)\n",
      "step 940: loss 5.39 1709.92ms/step 4791tok/s (total 7,700,480 tok)\n",
      "step 950: loss 5.37 1971.53ms/step 4155tok/s (total 7,782,400 tok)\n",
      "step 950 eval: val_loss 5.40\n",
      "step 960: loss 5.45 1679.97ms/step 4876tok/s (total 7,864,320 tok)\n",
      "step 970: loss 5.46 2656.50ms/step 3084tok/s (total 7,946,240 tok)\n",
      "step 980: loss 5.45 1849.93ms/step 4428tok/s (total 8,028,160 tok)\n",
      "step 990: loss 5.49 1814.73ms/step 4514tok/s (total 8,110,080 tok)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "for curr_step in range(1, 1000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - curr_time\n",
    "    if curr_step % log_interval == 0:\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):.0f}tok/s (total {tok_total:,} tok)\")\n",
    "\n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(encode(\"hello my name is\").to(device).unsqueeze(0), 10)\n",
    "        #     print(decode(output))\n",
    "        model.train()\n",
    "    curr_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "081d4dc9-490b-4d23-9838-963916cdd05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This contains Kohiedcheter shelled dead stem sugar bodies that all sea bacteria can bring up, noun and green breakfast.\n",
      "Ozy containing water potential• Take be very hard again, ‘I amNFTuta’ I app which will\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b7b0f72-6cd6-4608-8ed5-e93f7d5f1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This decision will equation which can be generated by the two categories of instruction. That is be made in one; it's, or the test – a little process also shows the most because a, given set of input transfer across the space. Since the mouse\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a6de3e-b136-41fe-81d8-92cdf2e2228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This natural understanding of leadership at the battle and the people heard.\n",
      "6. In this comet, we have clushing the air without trapping CLG until the Sierra Parks Shin had gone higher over and if you five months. I think about picking in bridges\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ce0a83-5257-4085-8526-96119757362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_45642/1485170168.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_45642/1485170168.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 5.41 1178.31ms/step 6952tok/s (total 81,920 tok)\n",
      "step 20: loss 5.55 1191.49ms/step 6875tok/s (total 163,840 tok)\n",
      "step 30: loss 5.44 1189.79ms/step 6885tok/s (total 245,760 tok)\n",
      "step 40: loss 5.49 1189.49ms/step 6887tok/s (total 327,680 tok)\n",
      "step 50: loss 5.55 1190.74ms/step 6880tok/s (total 409,600 tok)\n",
      "step 50 eval: val_loss 5.46\n",
      "step 60: loss 5.46 1186.89ms/step 6902tok/s (total 491,520 tok)\n",
      "step 70: loss 5.48 1200.09ms/step 6826tok/s (total 573,440 tok)\n",
      "step 80: loss 5.43 1207.70ms/step 6783tok/s (total 655,360 tok)\n",
      "step 90: loss 5.42 1229.86ms/step 6661tok/s (total 737,280 tok)\n",
      "step 100: loss 5.50 1231.87ms/step 6650tok/s (total 819,200 tok)\n",
      "step 100 eval: val_loss 5.46\n",
      "step 110: loss 5.59 1250.18ms/step 6553tok/s (total 901,120 tok)\n",
      "step 120: loss 5.48 1285.18ms/step 6374tok/s (total 983,040 tok)\n",
      "step 130: loss 5.41 1305.08ms/step 6277tok/s (total 1,064,960 tok)\n",
      "step 140: loss 5.51 1384.97ms/step 5915tok/s (total 1,146,880 tok)\n",
      "step 150: loss 5.44 1570.17ms/step 5217tok/s (total 1,228,800 tok)\n",
      "step 150 eval: val_loss 5.43\n",
      "step 160: loss 5.43 1703.12ms/step 4810tok/s (total 1,310,720 tok)\n",
      "step 170: loss 5.39 1739.55ms/step 4709tok/s (total 1,392,640 tok)\n",
      "step 180: loss 5.38 1710.42ms/step 4789tok/s (total 1,474,560 tok)\n",
      "step 190: loss 5.50 1755.60ms/step 4666tok/s (total 1,556,480 tok)\n",
      "step 200: loss 5.45 1620.28ms/step 5056tok/s (total 1,638,400 tok)\n",
      "step 200 eval: val_loss 5.38\n",
      "step 210: loss 5.47 1829.63ms/step 4477tok/s (total 1,720,320 tok)\n",
      "step 220: loss 5.49 1701.99ms/step 4813tok/s (total 1,802,240 tok)\n",
      "step 230: loss 5.44 1710.48ms/step 4789tok/s (total 1,884,160 tok)\n",
      "step 240: loss 5.43 1713.10ms/step 4782tok/s (total 1,966,080 tok)\n",
      "step 250: loss 5.47 1739.26ms/step 4710tok/s (total 2,048,000 tok)\n",
      "step 250 eval: val_loss 5.45\n",
      "step 260: loss 5.47 1771.49ms/step 4624tok/s (total 2,129,920 tok)\n",
      "step 270: loss 5.36 1773.51ms/step 4619tok/s (total 2,211,840 tok)\n",
      "step 280: loss 5.45 1781.06ms/step 4600tok/s (total 2,293,760 tok)\n",
      "step 290: loss 5.55 1806.92ms/step 4534tok/s (total 2,375,680 tok)\n",
      "step 300: loss 5.52 1807.25ms/step 4533tok/s (total 2,457,600 tok)\n",
      "step 300 eval: val_loss 5.26\n",
      "step 310: loss 5.37 1820.41ms/step 4500tok/s (total 2,539,520 tok)\n",
      "step 320: loss 5.45 1806.86ms/step 4534tok/s (total 2,621,440 tok)\n",
      "step 330: loss 5.33 1814.17ms/step 4516tok/s (total 2,703,360 tok)\n",
      "step 340: loss 5.38 1811.53ms/step 4522tok/s (total 2,785,280 tok)\n",
      "step 350: loss 5.36 1858.82ms/step 4407tok/s (total 2,867,200 tok)\n",
      "step 350 eval: val_loss 5.30\n",
      "step 360: loss 5.39 1877.17ms/step 4364tok/s (total 2,949,120 tok)\n",
      "step 370: loss 5.40 1897.35ms/step 4318tok/s (total 3,031,040 tok)\n",
      "step 380: loss 5.35 1844.42ms/step 4442tok/s (total 3,112,960 tok)\n",
      "step 390: loss 5.35 1949.84ms/step 4201tok/s (total 3,194,880 tok)\n",
      "step 400: loss 5.31 1676.88ms/step 4885tok/s (total 3,276,800 tok)\n",
      "step 400 eval: val_loss 5.35\n",
      "step 410: loss 5.25 1498.08ms/step 5468tok/s (total 3,358,720 tok)\n",
      "step 420: loss 5.34 1442.57ms/step 5679tok/s (total 3,440,640 tok)\n",
      "step 430: loss 5.33 1466.10ms/step 5588tok/s (total 3,522,560 tok)\n",
      "step 440: loss 5.39 1475.59ms/step 5552tok/s (total 3,604,480 tok)\n",
      "step 450: loss 5.43 1511.38ms/step 5420tok/s (total 3,686,400 tok)\n",
      "step 450 eval: val_loss 5.37\n",
      "step 460: loss 5.39 1539.85ms/step 5320tok/s (total 3,768,320 tok)\n",
      "step 470: loss 5.32 1497.27ms/step 5471tok/s (total 3,850,240 tok)\n",
      "step 480: loss 5.28 1484.66ms/step 5518tok/s (total 3,932,160 tok)\n",
      "step 490: loss 5.45 1476.50ms/step 5548tok/s (total 4,014,080 tok)\n",
      "step 500: loss 5.27 1524.25ms/step 5374tok/s (total 4,096,000 tok)\n",
      "step 500 eval: val_loss 5.29\n",
      "step 510: loss 5.39 1439.85ms/step 5689tok/s (total 4,177,920 tok)\n",
      "step 520: loss 5.21 1494.53ms/step 5481tok/s (total 4,259,840 tok)\n",
      "step 530: loss 5.37 1429.98ms/step 5729tok/s (total 4,341,760 tok)\n",
      "step 540: loss 5.35 1441.24ms/step 5684tok/s (total 4,423,680 tok)\n",
      "step 550: loss 5.36 1434.48ms/step 5711tok/s (total 4,505,600 tok)\n",
      "step 550 eval: val_loss 5.23\n",
      "step 560: loss 5.31 1437.72ms/step 5698tok/s (total 4,587,520 tok)\n",
      "step 570: loss 5.20 1489.19ms/step 5501tok/s (total 4,669,440 tok)\n",
      "step 580: loss 5.30 1498.91ms/step 5465tok/s (total 4,751,360 tok)\n",
      "step 590: loss 5.43 1463.30ms/step 5598tok/s (total 4,833,280 tok)\n",
      "step 600: loss 5.23 1431.51ms/step 5723tok/s (total 4,915,200 tok)\n",
      "step 600 eval: val_loss 5.14\n",
      "step 610: loss 5.28 1500.00ms/step 5461tok/s (total 4,997,120 tok)\n",
      "step 620: loss 5.41 1424.75ms/step 5750tok/s (total 5,079,040 tok)\n",
      "step 630: loss 5.21 1405.15ms/step 5830tok/s (total 5,160,960 tok)\n",
      "step 640: loss 5.27 1454.15ms/step 5634tok/s (total 5,242,880 tok)\n",
      "step 650: loss 5.36 1392.87ms/step 5881tok/s (total 5,324,800 tok)\n",
      "step 650 eval: val_loss 5.33\n",
      "step 660: loss 5.40 1428.37ms/step 5735tok/s (total 5,406,720 tok)\n",
      "step 670: loss 5.39 1416.73ms/step 5782tok/s (total 5,488,640 tok)\n",
      "step 680: loss 5.45 1454.20ms/step 5633tok/s (total 5,570,560 tok)\n",
      "step 690: loss 5.35 1379.80ms/step 5937tok/s (total 5,652,480 tok)\n",
      "step 700: loss 5.31 1394.12ms/step 5876tok/s (total 5,734,400 tok)\n",
      "step 700 eval: val_loss 5.27\n",
      "step 710: loss 5.29 1379.65ms/step 5938tok/s (total 5,816,320 tok)\n",
      "step 720: loss 5.13 1442.91ms/step 5677tok/s (total 5,898,240 tok)\n",
      "step 730: loss 5.29 1466.07ms/step 5588tok/s (total 5,980,160 tok)\n",
      "step 740: loss 5.21 1419.31ms/step 5772tok/s (total 6,062,080 tok)\n",
      "step 750: loss 5.33 1411.66ms/step 5803tok/s (total 6,144,000 tok)\n",
      "step 750 eval: val_loss 5.26\n",
      "step 760: loss 5.30 1328.58ms/step 6166tok/s (total 6,225,920 tok)\n",
      "step 770: loss 5.29 1420.40ms/step 5767tok/s (total 6,307,840 tok)\n",
      "step 780: loss 5.37 1389.21ms/step 5897tok/s (total 6,389,760 tok)\n",
      "step 790: loss 5.26 1395.86ms/step 5869tok/s (total 6,471,680 tok)\n",
      "step 800: loss 5.36 1389.06ms/step 5898tok/s (total 6,553,600 tok)\n",
      "step 800 eval: val_loss 5.17\n",
      "step 810: loss 5.38 1427.90ms/step 5737tok/s (total 6,635,520 tok)\n",
      "step 820: loss 5.29 1379.73ms/step 5937tok/s (total 6,717,440 tok)\n",
      "step 830: loss 5.35 1402.81ms/step 5840tok/s (total 6,799,360 tok)\n",
      "step 840: loss 5.20 1433.26ms/step 5716tok/s (total 6,881,280 tok)\n",
      "step 850: loss 5.26 1412.96ms/step 5798tok/s (total 6,963,200 tok)\n",
      "step 850 eval: val_loss 5.14\n",
      "step 860: loss 5.26 1434.70ms/step 5710tok/s (total 7,045,120 tok)\n",
      "step 870: loss 5.15 1377.61ms/step 5947tok/s (total 7,127,040 tok)\n",
      "step 880: loss 5.33 1382.17ms/step 5927tok/s (total 7,208,960 tok)\n",
      "step 890: loss 5.28 1365.02ms/step 6001tok/s (total 7,290,880 tok)\n",
      "step 900: loss 5.23 1418.65ms/step 5774tok/s (total 7,372,800 tok)\n",
      "step 900 eval: val_loss 5.11\n",
      "step 910: loss 5.31 1373.60ms/step 5964tok/s (total 7,454,720 tok)\n",
      "step 920: loss 5.33 1404.25ms/step 5834tok/s (total 7,536,640 tok)\n",
      "step 930: loss 5.30 1360.31ms/step 6022tok/s (total 7,618,560 tok)\n",
      "step 940: loss 5.34 1359.16ms/step 6027tok/s (total 7,700,480 tok)\n",
      "step 950: loss 5.22 1358.77ms/step 6029tok/s (total 7,782,400 tok)\n",
      "step 950 eval: val_loss 5.22\n",
      "step 960: loss 5.22 1377.87ms/step 5945tok/s (total 7,864,320 tok)\n",
      "step 970: loss 5.16 1366.55ms/step 5995tok/s (total 7,946,240 tok)\n",
      "step 980: loss 5.13 1320.21ms/step 6205tok/s (total 8,028,160 tok)\n",
      "step 990: loss 5.15 1330.39ms/step 6158tok/s (total 8,110,080 tok)\n",
      "step 1000: loss 5.20 1304.51ms/step 6280tok/s (total 8,192,000 tok)\n",
      "step 1000 eval: val_loss 5.16\n",
      "step 1010: loss 5.21 1323.00ms/step 6192tok/s (total 8,273,920 tok)\n",
      "step 1020: loss 5.20 1365.59ms/step 5999tok/s (total 8,355,840 tok)\n",
      "step 1030: loss 5.24 1384.95ms/step 5915tok/s (total 8,437,760 tok)\n",
      "step 1040: loss 5.31 1340.20ms/step 6113tok/s (total 8,519,680 tok)\n",
      "step 1050: loss 5.11 1488.13ms/step 5505tok/s (total 8,601,600 tok)\n",
      "step 1050 eval: val_loss 5.17\n",
      "step 1060: loss 5.15 1370.22ms/step 5979tok/s (total 8,683,520 tok)\n",
      "step 1070: loss 5.35 1331.38ms/step 6153tok/s (total 8,765,440 tok)\n",
      "step 1080: loss 5.21 1342.69ms/step 6101tok/s (total 8,847,360 tok)\n",
      "step 1090: loss 5.15 1335.45ms/step 6134tok/s (total 8,929,280 tok)\n",
      "step 1100: loss 5.30 1301.78ms/step 6293tok/s (total 9,011,200 tok)\n",
      "step 1100 eval: val_loss 5.24\n",
      "step 1110: loss 5.24 1328.41ms/step 6167tok/s (total 9,093,120 tok)\n",
      "step 1120: loss 5.14 1317.86ms/step 6216tok/s (total 9,175,040 tok)\n",
      "step 1130: loss 5.18 1351.06ms/step 6063tok/s (total 9,256,960 tok)\n",
      "step 1140: loss 5.20 1301.47ms/step 6294tok/s (total 9,338,880 tok)\n",
      "step 1150: loss 5.29 1327.05ms/step 6173tok/s (total 9,420,800 tok)\n",
      "step 1150 eval: val_loss 5.07\n",
      "step 1160: loss 5.21 1332.99ms/step 6146tok/s (total 9,502,720 tok)\n",
      "step 1170: loss 5.26 1310.17ms/step 6253tok/s (total 9,584,640 tok)\n",
      "step 1180: loss 5.10 1306.01ms/step 6273tok/s (total 9,666,560 tok)\n",
      "step 1190: loss 5.16 1672.32ms/step 4899tok/s (total 9,748,480 tok)\n",
      "step 1200: loss 5.13 1377.00ms/step 5949tok/s (total 9,830,400 tok)\n",
      "step 1200 eval: val_loss 5.10\n",
      "step 1210: loss 5.20 1319.78ms/step 6207tok/s (total 9,912,320 tok)\n",
      "step 1220: loss 5.11 1346.52ms/step 6084tok/s (total 9,994,240 tok)\n",
      "step 1230: loss 5.14 1355.35ms/step 6044tok/s (total 10,076,160 tok)\n",
      "step 1240: loss 5.18 1401.88ms/step 5844tok/s (total 10,158,080 tok)\n",
      "step 1250: loss 5.25 1370.57ms/step 5977tok/s (total 10,240,000 tok)\n",
      "step 1250 eval: val_loss 5.10\n",
      "step 1260: loss 5.23 1361.23ms/step 6018tok/s (total 10,321,920 tok)\n",
      "step 1270: loss 5.25 1384.50ms/step 5917tok/s (total 10,403,840 tok)\n",
      "step 1280: loss 5.07 1377.38ms/step 5948tok/s (total 10,485,760 tok)\n",
      "step 1290: loss 5.16 1406.58ms/step 5824tok/s (total 10,567,680 tok)\n",
      "step 1300: loss 5.10 1411.84ms/step 5802tok/s (total 10,649,600 tok)\n",
      "step 1300 eval: val_loss 5.12\n",
      "step 1310: loss 5.18 1354.37ms/step 6049tok/s (total 10,731,520 tok)\n",
      "step 1320: loss 5.11 1348.51ms/step 6075tok/s (total 10,813,440 tok)\n",
      "step 1330: loss 5.05 1345.96ms/step 6086tok/s (total 10,895,360 tok)\n",
      "step 1340: loss 5.18 1359.35ms/step 6026tok/s (total 10,977,280 tok)\n",
      "step 1350: loss 5.23 1356.04ms/step 6041tok/s (total 11,059,200 tok)\n",
      "step 1350 eval: val_loss 5.17\n",
      "step 1360: loss 5.20 1408.13ms/step 5818tok/s (total 11,141,120 tok)\n",
      "step 1370: loss 5.13 1345.14ms/step 6090tok/s (total 11,223,040 tok)\n",
      "step 1380: loss 5.06 1338.16ms/step 6122tok/s (total 11,304,960 tok)\n",
      "step 1390: loss 5.21 1294.53ms/step 6328tok/s (total 11,386,880 tok)\n",
      "step 1400: loss 5.26 1362.14ms/step 6014tok/s (total 11,468,800 tok)\n",
      "step 1400 eval: val_loss 5.12\n",
      "step 1410: loss 5.29 1367.73ms/step 5989tok/s (total 11,550,720 tok)\n",
      "step 1420: loss 5.05 1396.48ms/step 5866tok/s (total 11,632,640 tok)\n",
      "step 1430: loss 5.24 1345.79ms/step 6087tok/s (total 11,714,560 tok)\n",
      "step 1440: loss 5.27 1404.05ms/step 5835tok/s (total 11,796,480 tok)\n",
      "step 1450: loss 5.09 1373.09ms/step 5966tok/s (total 11,878,400 tok)\n",
      "step 1450 eval: val_loss 5.14\n",
      "step 1460: loss 5.22 1361.56ms/step 6017tok/s (total 11,960,320 tok)\n",
      "step 1470: loss 5.11 1349.06ms/step 6072tok/s (total 12,042,240 tok)\n",
      "step 1480: loss 5.18 1322.31ms/step 6195tok/s (total 12,124,160 tok)\n",
      "step 1490: loss 5.06 1361.75ms/step 6016tok/s (total 12,206,080 tok)\n",
      "step 1500: loss 5.15 1402.78ms/step 5840tok/s (total 12,288,000 tok)\n",
      "step 1500 eval: val_loss 5.13\n",
      "step 1510: loss 5.22 1346.66ms/step 6083tok/s (total 12,369,920 tok)\n",
      "step 1520: loss 5.17 1390.25ms/step 5892tok/s (total 12,451,840 tok)\n",
      "step 1530: loss 5.19 1394.14ms/step 5876tok/s (total 12,533,760 tok)\n",
      "step 1540: loss 5.07 1346.92ms/step 6082tok/s (total 12,615,680 tok)\n",
      "step 1550: loss 5.24 1353.24ms/step 6054tok/s (total 12,697,600 tok)\n",
      "step 1550 eval: val_loss 5.11\n",
      "step 1560: loss 5.16 1387.80ms/step 5903tok/s (total 12,779,520 tok)\n",
      "step 1570: loss 5.12 1374.48ms/step 5960tok/s (total 12,861,440 tok)\n",
      "step 1580: loss 5.10 1328.83ms/step 6165tok/s (total 12,943,360 tok)\n",
      "step 1590: loss 5.12 1337.43ms/step 6125tok/s (total 13,025,280 tok)\n",
      "step 1600: loss 5.17 1362.82ms/step 6011tok/s (total 13,107,200 tok)\n",
      "step 1600 eval: val_loss 5.04\n",
      "step 1610: loss 5.09 1383.01ms/step 5923tok/s (total 13,189,120 tok)\n",
      "step 1620: loss 5.02 1364.21ms/step 6005tok/s (total 13,271,040 tok)\n",
      "step 1630: loss 5.22 1378.54ms/step 5943tok/s (total 13,352,960 tok)\n",
      "step 1640: loss 4.98 1403.53ms/step 5837tok/s (total 13,434,880 tok)\n",
      "step 1650: loss 5.01 1383.29ms/step 5922tok/s (total 13,516,800 tok)\n",
      "step 1650 eval: val_loss 4.99\n",
      "step 1660: loss 5.16 1401.37ms/step 5846tok/s (total 13,598,720 tok)\n",
      "step 1670: loss 5.12 1347.06ms/step 6081tok/s (total 13,680,640 tok)\n",
      "step 1680: loss 5.12 1363.28ms/step 6009tok/s (total 13,762,560 tok)\n",
      "step 1690: loss 5.21 1355.28ms/step 6045tok/s (total 13,844,480 tok)\n",
      "step 1700: loss 5.23 1355.11ms/step 6045tok/s (total 13,926,400 tok)\n",
      "step 1700 eval: val_loss 5.16\n",
      "step 1710: loss 5.14 1362.11ms/step 6014tok/s (total 14,008,320 tok)\n",
      "step 1720: loss 5.06 1368.65ms/step 5985tok/s (total 14,090,240 tok)\n",
      "step 1730: loss 4.97 1357.36ms/step 6035tok/s (total 14,172,160 tok)\n",
      "step 1740: loss 5.20 1448.51ms/step 5655tok/s (total 14,254,080 tok)\n",
      "step 1750: loss 5.17 1399.00ms/step 5856tok/s (total 14,336,000 tok)\n",
      "step 1750 eval: val_loss 5.15\n",
      "step 1760: loss 5.14 1420.68ms/step 5766tok/s (total 14,417,920 tok)\n",
      "step 1770: loss 5.18 1340.63ms/step 6111tok/s (total 14,499,840 tok)\n",
      "step 1780: loss 5.12 1360.72ms/step 6020tok/s (total 14,581,760 tok)\n",
      "step 1790: loss 5.10 1353.47ms/step 6053tok/s (total 14,663,680 tok)\n",
      "step 1800: loss 5.07 1384.81ms/step 5916tok/s (total 14,745,600 tok)\n",
      "step 1800 eval: val_loss 5.09\n",
      "step 1810: loss 5.09 1359.27ms/step 6027tok/s (total 14,827,520 tok)\n",
      "step 1820: loss 5.05 1370.60ms/step 5977tok/s (total 14,909,440 tok)\n",
      "step 1830: loss 5.16 1370.99ms/step 5975tok/s (total 14,991,360 tok)\n",
      "step 1840: loss 5.09 1418.28ms/step 5776tok/s (total 15,073,280 tok)\n",
      "step 1850: loss 5.10 1377.93ms/step 5945tok/s (total 15,155,200 tok)\n",
      "step 1850 eval: val_loss 5.02\n",
      "step 1860: loss 5.10 1384.31ms/step 5918tok/s (total 15,237,120 tok)\n",
      "step 1870: loss 4.98 1425.32ms/step 5747tok/s (total 15,319,040 tok)\n",
      "step 1880: loss 5.06 1396.85ms/step 5865tok/s (total 15,400,960 tok)\n",
      "step 1890: loss 5.14 1450.62ms/step 5647tok/s (total 15,482,880 tok)\n",
      "step 1900: loss 5.14 1424.29ms/step 5752tok/s (total 15,564,800 tok)\n",
      "step 1900 eval: val_loss 5.05\n",
      "step 1910: loss 5.16 1334.74ms/step 6138tok/s (total 15,646,720 tok)\n",
      "step 1920: loss 5.02 1351.22ms/step 6063tok/s (total 15,728,640 tok)\n",
      "step 1930: loss 5.11 1376.20ms/step 5953tok/s (total 15,810,560 tok)\n",
      "step 1940: loss 5.02 1375.77ms/step 5954tok/s (total 15,892,480 tok)\n",
      "step 1950: loss 5.23 1388.62ms/step 5899tok/s (total 15,974,400 tok)\n",
      "step 1950 eval: val_loss 4.97\n",
      "step 1960: loss 5.09 1441.81ms/step 5682tok/s (total 16,056,320 tok)\n",
      "step 1970: loss 4.96 1396.90ms/step 5864tok/s (total 16,138,240 tok)\n",
      "step 1980: loss 5.12 1387.66ms/step 5903tok/s (total 16,220,160 tok)\n",
      "step 1990: loss 5.08 1401.12ms/step 5847tok/s (total 16,302,080 tok)\n",
      "step 2000: loss 5.06 1397.88ms/step 5860tok/s (total 16,384,000 tok)\n",
      "step 2000 eval: val_loss 5.09\n",
      "step 2010: loss 5.16 1332.92ms/step 6146tok/s (total 16,465,920 tok)\n",
      "step 2020: loss 5.10 1364.32ms/step 6004tok/s (total 16,547,840 tok)\n",
      "step 2030: loss 5.06 1357.29ms/step 6036tok/s (total 16,629,760 tok)\n",
      "step 2040: loss 5.05 1341.37ms/step 6107tok/s (total 16,711,680 tok)\n",
      "step 2050: loss 5.03 1393.75ms/step 5878tok/s (total 16,793,600 tok)\n",
      "step 2050 eval: val_loss 5.08\n",
      "step 2060: loss 5.00 1369.89ms/step 5980tok/s (total 16,875,520 tok)\n",
      "step 2070: loss 5.03 1404.41ms/step 5833tok/s (total 16,957,440 tok)\n",
      "step 2080: loss 5.06 1412.34ms/step 5800tok/s (total 17,039,360 tok)\n",
      "step 2090: loss 4.99 1333.62ms/step 6143tok/s (total 17,121,280 tok)\n",
      "step 2100: loss 5.01 1362.48ms/step 6013tok/s (total 17,203,200 tok)\n",
      "step 2100 eval: val_loss 5.03\n",
      "step 2110: loss 5.03 1355.79ms/step 6042tok/s (total 17,285,120 tok)\n",
      "step 2120: loss 5.13 1357.47ms/step 6035tok/s (total 17,367,040 tok)\n",
      "step 2130: loss 5.11 1382.88ms/step 5924tok/s (total 17,448,960 tok)\n",
      "step 2140: loss 5.15 1391.75ms/step 5886tok/s (total 17,530,880 tok)\n",
      "step 2150: loss 5.03 1388.45ms/step 5900tok/s (total 17,612,800 tok)\n",
      "step 2150 eval: val_loss 4.88\n",
      "step 2160: loss 5.09 1356.94ms/step 6037tok/s (total 17,694,720 tok)\n",
      "step 2170: loss 5.13 1430.86ms/step 5725tok/s (total 17,776,640 tok)\n",
      "step 2180: loss 4.94 1433.63ms/step 5714tok/s (total 17,858,560 tok)\n",
      "step 2190: loss 5.09 1340.87ms/step 6109tok/s (total 17,940,480 tok)\n",
      "step 2200: loss 5.08 1335.47ms/step 6134tok/s (total 18,022,400 tok)\n",
      "step 2200 eval: val_loss 4.96\n",
      "step 2210: loss 5.04 1354.79ms/step 6047tok/s (total 18,104,320 tok)\n",
      "step 2220: loss 5.04 1347.76ms/step 6078tok/s (total 18,186,240 tok)\n",
      "step 2230: loss 5.01 1356.68ms/step 6038tok/s (total 18,268,160 tok)\n",
      "step 2240: loss 4.91 1351.14ms/step 6063tok/s (total 18,350,080 tok)\n",
      "step 2250: loss 5.03 1359.69ms/step 6025tok/s (total 18,432,000 tok)\n",
      "step 2250 eval: val_loss 4.97\n",
      "step 2260: loss 5.09 1337.85ms/step 6123tok/s (total 18,513,920 tok)\n",
      "step 2270: loss 5.16 1344.75ms/step 6092tok/s (total 18,595,840 tok)\n",
      "step 2280: loss 5.08 1303.59ms/step 6284tok/s (total 18,677,760 tok)\n",
      "step 2290: loss 5.08 1402.02ms/step 5843tok/s (total 18,759,680 tok)\n",
      "step 2300: loss 5.10 1357.82ms/step 6033tok/s (total 18,841,600 tok)\n",
      "step 2300 eval: val_loss 5.03\n",
      "step 2310: loss 4.99 1376.63ms/step 5951tok/s (total 18,923,520 tok)\n",
      "step 2320: loss 5.08 1391.44ms/step 5887tok/s (total 19,005,440 tok)\n",
      "step 2330: loss 5.08 1359.41ms/step 6026tok/s (total 19,087,360 tok)\n",
      "step 2340: loss 5.09 1308.15ms/step 6262tok/s (total 19,169,280 tok)\n",
      "step 2350: loss 5.08 1335.39ms/step 6135tok/s (total 19,251,200 tok)\n",
      "step 2350 eval: val_loss 4.90\n",
      "step 2360: loss 5.03 1321.23ms/step 6200tok/s (total 19,333,120 tok)\n",
      "step 2370: loss 4.93 1299.92ms/step 6302tok/s (total 19,415,040 tok)\n",
      "step 2380: loss 5.05 1287.53ms/step 6363tok/s (total 19,496,960 tok)\n",
      "step 2390: loss 5.02 1309.42ms/step 6256tok/s (total 19,578,880 tok)\n",
      "step 2400: loss 5.10 1317.43ms/step 6218tok/s (total 19,660,800 tok)\n",
      "step 2400 eval: val_loss 4.94\n",
      "step 2410: loss 5.04 1302.70ms/step 6288tok/s (total 19,742,720 tok)\n",
      "step 2420: loss 5.07 1304.47ms/step 6280tok/s (total 19,824,640 tok)\n",
      "step 2430: loss 5.04 1312.61ms/step 6241tok/s (total 19,906,560 tok)\n",
      "step 2440: loss 5.00 1309.62ms/step 6255tok/s (total 19,988,480 tok)\n",
      "step 2450: loss 5.04 1334.54ms/step 6138tok/s (total 20,070,400 tok)\n",
      "step 2450 eval: val_loss 4.97\n",
      "step 2460: loss 5.06 1323.81ms/step 6188tok/s (total 20,152,320 tok)\n",
      "step 2470: loss 5.18 1334.23ms/step 6140tok/s (total 20,234,240 tok)\n",
      "step 2480: loss 5.10 1313.22ms/step 6238tok/s (total 20,316,160 tok)\n",
      "step 2490: loss 4.89 1332.10ms/step 6150tok/s (total 20,398,080 tok)\n",
      "step 2500: loss 5.05 1310.12ms/step 6253tok/s (total 20,480,000 tok)\n",
      "step 2500 eval: val_loss 4.97\n",
      "step 2510: loss 5.02 1295.18ms/step 6325tok/s (total 20,561,920 tok)\n",
      "step 2520: loss 5.07 1320.82ms/step 6202tok/s (total 20,643,840 tok)\n",
      "step 2530: loss 5.09 1296.95ms/step 6316tok/s (total 20,725,760 tok)\n",
      "step 2540: loss 5.09 1322.99ms/step 6192tok/s (total 20,807,680 tok)\n",
      "step 2550: loss 5.00 1343.04ms/step 6100tok/s (total 20,889,600 tok)\n",
      "step 2550 eval: val_loss 4.93\n",
      "step 2560: loss 5.01 1299.43ms/step 6304tok/s (total 20,971,520 tok)\n",
      "step 2570: loss 4.94 1302.62ms/step 6289tok/s (total 21,053,440 tok)\n",
      "step 2580: loss 4.99 1296.87ms/step 6317tok/s (total 21,135,360 tok)\n",
      "step 2590: loss 4.98 1394.73ms/step 5874tok/s (total 21,217,280 tok)\n",
      "step 2600: loss 4.99 1304.22ms/step 6281tok/s (total 21,299,200 tok)\n",
      "step 2600 eval: val_loss 5.05\n",
      "step 2610: loss 5.01 1317.68ms/step 6217tok/s (total 21,381,120 tok)\n",
      "step 2620: loss 4.88 1292.54ms/step 6338tok/s (total 21,463,040 tok)\n",
      "step 2630: loss 4.98 1301.78ms/step 6293tok/s (total 21,544,960 tok)\n",
      "step 2640: loss 5.05 1380.51ms/step 5934tok/s (total 21,626,880 tok)\n",
      "step 2650: loss 4.99 1303.96ms/step 6282tok/s (total 21,708,800 tok)\n",
      "step 2650 eval: val_loss 4.90\n",
      "step 2660: loss 4.94 1394.12ms/step 5876tok/s (total 21,790,720 tok)\n",
      "step 2670: loss 5.03 1659.31ms/step 4937tok/s (total 21,872,640 tok)\n",
      "step 2680: loss 4.92 1399.64ms/step 5853tok/s (total 21,954,560 tok)\n",
      "step 2690: loss 4.98 1374.55ms/step 5960tok/s (total 22,036,480 tok)\n",
      "step 2700: loss 4.97 1444.65ms/step 5671tok/s (total 22,118,400 tok)\n",
      "step 2700 eval: val_loss 4.99\n",
      "step 2710: loss 4.96 1354.62ms/step 6047tok/s (total 22,200,320 tok)\n",
      "step 2720: loss 5.10 1407.50ms/step 5820tok/s (total 22,282,240 tok)\n",
      "step 2730: loss 5.08 1363.58ms/step 6008tok/s (total 22,364,160 tok)\n",
      "step 2740: loss 5.05 1338.86ms/step 6119tok/s (total 22,446,080 tok)\n",
      "step 2750: loss 4.95 1326.42ms/step 6176tok/s (total 22,528,000 tok)\n",
      "step 2750 eval: val_loss 5.06\n",
      "step 2760: loss 4.89 1334.03ms/step 6141tok/s (total 22,609,920 tok)\n",
      "step 2770: loss 5.02 1374.79ms/step 5959tok/s (total 22,691,840 tok)\n",
      "step 2780: loss 4.99 1374.32ms/step 5961tok/s (total 22,773,760 tok)\n",
      "step 2790: loss 5.04 1414.89ms/step 5790tok/s (total 22,855,680 tok)\n",
      "step 2800: loss 4.89 1360.84ms/step 6020tok/s (total 22,937,600 tok)\n",
      "step 2800 eval: val_loss 4.90\n",
      "step 2810: loss 4.92 1354.93ms/step 6046tok/s (total 23,019,520 tok)\n",
      "step 2820: loss 4.94 1381.13ms/step 5931tok/s (total 23,101,440 tok)\n",
      "step 2830: loss 5.03 1351.16ms/step 6063tok/s (total 23,183,360 tok)\n",
      "step 2840: loss 4.98 1381.48ms/step 5930tok/s (total 23,265,280 tok)\n",
      "step 2850: loss 4.97 1352.95ms/step 6055tok/s (total 23,347,200 tok)\n",
      "step 2850 eval: val_loss 4.85\n",
      "step 2860: loss 5.08 1366.53ms/step 5995tok/s (total 23,429,120 tok)\n",
      "step 2870: loss 4.99 1367.58ms/step 5990tok/s (total 23,511,040 tok)\n",
      "step 2880: loss 5.02 1365.91ms/step 5997tok/s (total 23,592,960 tok)\n",
      "step 2890: loss 4.89 1372.55ms/step 5968tok/s (total 23,674,880 tok)\n",
      "step 2900: loss 4.94 1379.01ms/step 5940tok/s (total 23,756,800 tok)\n",
      "step 2900 eval: val_loss 4.93\n",
      "step 2910: loss 4.83 1355.18ms/step 6045tok/s (total 23,838,720 tok)\n",
      "step 2920: loss 4.94 1366.13ms/step 5996tok/s (total 23,920,640 tok)\n",
      "step 2930: loss 5.10 1407.78ms/step 5819tok/s (total 24,002,560 tok)\n",
      "step 2940: loss 4.95 1369.26ms/step 5983tok/s (total 24,084,480 tok)\n",
      "step 2950: loss 5.01 1356.37ms/step 6040tok/s (total 24,166,400 tok)\n",
      "step 2950 eval: val_loss 4.88\n",
      "step 2960: loss 5.07 1338.04ms/step 6122tok/s (total 24,248,320 tok)\n",
      "step 2970: loss 4.99 1370.04ms/step 5979tok/s (total 24,330,240 tok)\n",
      "step 2980: loss 5.05 1348.88ms/step 6073tok/s (total 24,412,160 tok)\n",
      "step 2990: loss 4.97 1369.49ms/step 5982tok/s (total 24,494,080 tok)\n",
      "step 3000: loss 4.95 1350.62ms/step 6065tok/s (total 24,576,000 tok)\n",
      "step 3000 eval: val_loss 4.85\n",
      "step 3010: loss 4.98 1336.18ms/step 6131tok/s (total 24,657,920 tok)\n",
      "step 3020: loss 4.98 1312.36ms/step 6242tok/s (total 24,739,840 tok)\n",
      "step 3030: loss 5.08 1352.28ms/step 6058tok/s (total 24,821,760 tok)\n",
      "step 3040: loss 4.99 1352.91ms/step 6055tok/s (total 24,903,680 tok)\n",
      "step 3050: loss 4.92 1324.01ms/step 6187tok/s (total 24,985,600 tok)\n",
      "step 3050 eval: val_loss 4.88\n",
      "step 3060: loss 5.06 1339.23ms/step 6117tok/s (total 25,067,520 tok)\n",
      "step 3070: loss 4.97 1370.53ms/step 5977tok/s (total 25,149,440 tok)\n",
      "step 3080: loss 5.03 1343.43ms/step 6098tok/s (total 25,231,360 tok)\n",
      "step 3090: loss 4.93 1357.93ms/step 6033tok/s (total 25,313,280 tok)\n",
      "step 3100: loss 5.07 1372.68ms/step 5968tok/s (total 25,395,200 tok)\n",
      "step 3100 eval: val_loss 4.90\n",
      "step 3110: loss 5.01 1349.24ms/step 6072tok/s (total 25,477,120 tok)\n",
      "step 3120: loss 4.90 1354.86ms/step 6046tok/s (total 25,559,040 tok)\n",
      "step 3130: loss 4.97 1340.53ms/step 6111tok/s (total 25,640,960 tok)\n",
      "step 3140: loss 4.86 1357.86ms/step 6033tok/s (total 25,722,880 tok)\n",
      "step 3150: loss 4.94 1520.59ms/step 5387tok/s (total 25,804,800 tok)\n",
      "step 3150 eval: val_loss 4.94\n",
      "step 3160: loss 5.01 1337.03ms/step 6127tok/s (total 25,886,720 tok)\n",
      "step 3170: loss 4.94 1286.58ms/step 6367tok/s (total 25,968,640 tok)\n",
      "step 3180: loss 4.99 1281.04ms/step 6395tok/s (total 26,050,560 tok)\n",
      "step 3190: loss 4.98 1274.65ms/step 6427tok/s (total 26,132,480 tok)\n",
      "step 3200: loss 4.98 1291.40ms/step 6343tok/s (total 26,214,400 tok)\n",
      "step 3200 eval: val_loss 4.86\n",
      "step 3210: loss 4.92 1274.56ms/step 6427tok/s (total 26,296,320 tok)\n",
      "step 3220: loss 4.91 1310.77ms/step 6250tok/s (total 26,378,240 tok)\n",
      "step 3230: loss 4.97 1296.97ms/step 6316tok/s (total 26,460,160 tok)\n",
      "step 3240: loss 4.90 1359.50ms/step 6026tok/s (total 26,542,080 tok)\n",
      "step 3250: loss 4.92 1287.87ms/step 6361tok/s (total 26,624,000 tok)\n",
      "step 3250 eval: val_loss 4.83\n",
      "step 3260: loss 4.96 1291.90ms/step 6341tok/s (total 26,705,920 tok)\n",
      "step 3270: loss 4.89 1289.68ms/step 6352tok/s (total 26,787,840 tok)\n",
      "step 3280: loss 4.96 1322.14ms/step 6196tok/s (total 26,869,760 tok)\n",
      "step 3290: loss 5.01 1291.20ms/step 6344tok/s (total 26,951,680 tok)\n",
      "step 3300: loss 4.92 1310.26ms/step 6252tok/s (total 27,033,600 tok)\n",
      "step 3300 eval: val_loss 4.92\n",
      "step 3310: loss 4.95 1300.94ms/step 6297tok/s (total 27,115,520 tok)\n",
      "step 3320: loss 4.90 1295.67ms/step 6323tok/s (total 27,197,440 tok)\n",
      "step 3330: loss 4.98 1292.56ms/step 6338tok/s (total 27,279,360 tok)\n",
      "step 3340: loss 4.97 1284.43ms/step 6378tok/s (total 27,361,280 tok)\n",
      "step 3350: loss 5.05 1304.41ms/step 6280tok/s (total 27,443,200 tok)\n",
      "step 3350 eval: val_loss 4.90\n",
      "step 3360: loss 4.93 1295.02ms/step 6326tok/s (total 27,525,120 tok)\n",
      "step 3370: loss 4.94 1302.08ms/step 6291tok/s (total 27,607,040 tok)\n",
      "step 3380: loss 4.93 1282.84ms/step 6386tok/s (total 27,688,960 tok)\n",
      "step 3390: loss 5.02 1299.08ms/step 6306tok/s (total 27,770,880 tok)\n",
      "step 3400: loss 4.95 1289.27ms/step 6354tok/s (total 27,852,800 tok)\n",
      "step 3400 eval: val_loss 4.94\n",
      "step 3410: loss 4.86 1352.22ms/step 6058tok/s (total 27,934,720 tok)\n",
      "step 3420: loss 4.84 1293.35ms/step 6334tok/s (total 28,016,640 tok)\n",
      "step 3430: loss 4.98 1276.10ms/step 6420tok/s (total 28,098,560 tok)\n",
      "step 3440: loss 4.80 1295.21ms/step 6325tok/s (total 28,180,480 tok)\n",
      "step 3450: loss 4.86 1304.26ms/step 6281tok/s (total 28,262,400 tok)\n",
      "step 3450 eval: val_loss 4.86\n",
      "step 3460: loss 4.94 1317.08ms/step 6220tok/s (total 28,344,320 tok)\n",
      "step 3470: loss 4.82 1325.34ms/step 6181tok/s (total 28,426,240 tok)\n",
      "step 3480: loss 5.03 1286.61ms/step 6367tok/s (total 28,508,160 tok)\n",
      "step 3490: loss 4.81 1283.20ms/step 6384tok/s (total 28,590,080 tok)\n",
      "step 3500: loss 4.96 1267.46ms/step 6463tok/s (total 28,672,000 tok)\n",
      "step 3500 eval: val_loss 4.91\n",
      "step 3510: loss 4.94 1290.27ms/step 6349tok/s (total 28,753,920 tok)\n",
      "step 3520: loss 5.00 1308.49ms/step 6261tok/s (total 28,835,840 tok)\n",
      "step 3530: loss 4.85 1303.64ms/step 6284tok/s (total 28,917,760 tok)\n",
      "step 3540: loss 4.89 1302.32ms/step 6290tok/s (total 28,999,680 tok)\n",
      "step 3550: loss 4.92 1319.04ms/step 6211tok/s (total 29,081,600 tok)\n",
      "step 3550 eval: val_loss 4.84\n",
      "step 3560: loss 4.93 1307.27ms/step 6267tok/s (total 29,163,520 tok)\n",
      "step 3570: loss 4.91 1300.81ms/step 6298tok/s (total 29,245,440 tok)\n",
      "step 3580: loss 4.88 1324.08ms/step 6187tok/s (total 29,327,360 tok)\n",
      "step 3590: loss 4.90 1311.82ms/step 6245tok/s (total 29,409,280 tok)\n",
      "step 3600: loss 4.97 1299.22ms/step 6305tok/s (total 29,491,200 tok)\n",
      "step 3600 eval: val_loss 4.93\n",
      "step 3610: loss 4.91 1278.09ms/step 6410tok/s (total 29,573,120 tok)\n",
      "step 3620: loss 4.91 1277.17ms/step 6414tok/s (total 29,655,040 tok)\n",
      "step 3630: loss 5.05 1291.20ms/step 6344tok/s (total 29,736,960 tok)\n",
      "step 3640: loss 4.85 1302.26ms/step 6291tok/s (total 29,818,880 tok)\n",
      "step 3650: loss 4.89 1295.30ms/step 6324tok/s (total 29,900,800 tok)\n",
      "step 3650 eval: val_loss 4.88\n",
      "step 3660: loss 4.91 1296.64ms/step 6318tok/s (total 29,982,720 tok)\n",
      "step 3670: loss 4.84 1301.09ms/step 6296tok/s (total 30,064,640 tok)\n",
      "step 3680: loss 5.03 1336.70ms/step 6129tok/s (total 30,146,560 tok)\n",
      "step 3690: loss 4.96 1288.34ms/step 6359tok/s (total 30,228,480 tok)\n",
      "step 3700: loss 4.96 1340.58ms/step 6111tok/s (total 30,310,400 tok)\n",
      "step 3700 eval: val_loss 4.90\n",
      "step 3710: loss 4.97 1309.32ms/step 6257tok/s (total 30,392,320 tok)\n",
      "step 3720: loss 4.93 1305.72ms/step 6274tok/s (total 30,474,240 tok)\n",
      "step 3730: loss 4.95 1306.54ms/step 6270tok/s (total 30,556,160 tok)\n",
      "step 3740: loss 4.90 1349.53ms/step 6070tok/s (total 30,638,080 tok)\n",
      "step 3750: loss 4.88 1312.31ms/step 6242tok/s (total 30,720,000 tok)\n",
      "step 3750 eval: val_loss 4.88\n",
      "step 3760: loss 4.95 1294.22ms/step 6330tok/s (total 30,801,920 tok)\n",
      "step 3770: loss 5.01 1307.21ms/step 6267tok/s (total 30,883,840 tok)\n",
      "step 3780: loss 5.01 1368.38ms/step 5987tok/s (total 30,965,760 tok)\n",
      "step 3790: loss 4.96 1292.44ms/step 6338tok/s (total 31,047,680 tok)\n",
      "step 3800: loss 4.94 1304.89ms/step 6278tok/s (total 31,129,600 tok)\n",
      "step 3800 eval: val_loss 4.89\n",
      "step 3810: loss 4.94 1304.48ms/step 6280tok/s (total 31,211,520 tok)\n",
      "step 3820: loss 4.82 1269.72ms/step 6452tok/s (total 31,293,440 tok)\n",
      "step 3830: loss 4.84 1277.37ms/step 6413tok/s (total 31,375,360 tok)\n",
      "step 3840: loss 4.92 1359.10ms/step 6027tok/s (total 31,457,280 tok)\n",
      "step 3850: loss 4.87 1286.01ms/step 6370tok/s (total 31,539,200 tok)\n",
      "step 3850 eval: val_loss 4.85\n",
      "step 3860: loss 4.89 1285.21ms/step 6374tok/s (total 31,621,120 tok)\n",
      "step 3870: loss 4.82 1289.56ms/step 6353tok/s (total 31,703,040 tok)\n",
      "step 3880: loss 4.98 1285.36ms/step 6373tok/s (total 31,784,960 tok)\n",
      "step 3890: loss 4.79 1287.06ms/step 6365tok/s (total 31,866,880 tok)\n",
      "step 3900: loss 4.87 1292.34ms/step 6339tok/s (total 31,948,800 tok)\n",
      "step 3900 eval: val_loss 4.84\n",
      "step 3910: loss 4.81 1295.20ms/step 6325tok/s (total 32,030,720 tok)\n",
      "step 3920: loss 4.82 1284.48ms/step 6378tok/s (total 32,112,640 tok)\n",
      "step 3930: loss 4.82 1262.52ms/step 6489tok/s (total 32,194,560 tok)\n",
      "step 3940: loss 4.87 1274.11ms/step 6430tok/s (total 32,276,480 tok)\n",
      "step 3950: loss 4.84 1303.82ms/step 6283tok/s (total 32,358,400 tok)\n",
      "step 3950 eval: val_loss 4.80\n",
      "step 3960: loss 4.87 1289.03ms/step 6355tok/s (total 32,440,320 tok)\n",
      "step 3970: loss 4.84 1348.70ms/step 6074tok/s (total 32,522,240 tok)\n",
      "step 3980: loss 4.93 1302.72ms/step 6288tok/s (total 32,604,160 tok)\n",
      "step 3990: loss 4.87 1289.20ms/step 6354tok/s (total 32,686,080 tok)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "for curr_step in range(1, 4000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - curr_time\n",
    "    if curr_step % log_interval == 0:\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):.0f}tok/s (total {tok_total:,} tok)\")\n",
    "\n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(encode(\"hello my name is\").to(device).unsqueeze(0), 10)\n",
    "        #     print(decode(output))\n",
    "        model.train()\n",
    "    curr_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84823467-f709-4f23-95a5-442c96badd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will be not guaranteed any protection and can be abandoned.\n",
      "Based on a study of both healthcare and business governance for civilian workers, on such domestic rights and other locations, those living fellow workers can afford that Judiciary has been studied.\n",
      "It has also\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3fa4ae0-66e6-4f7a-8002-09895fe6b041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 150, as carbon dioxide increases in the body temperature, and the elasticity of nature and capacity in the body. The metabolism of gas, acidic dust, also tends to rise in cholesterol levels, first from excess defects to an oncolic acid,\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd4bb7c-daf2-466b-8d83-9a1761db18b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_45642/1485170168.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_45642/1485170168.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10: loss 4.92 1186.28ms/step 6906tok/s (total 81,920 tok)\n",
      "step 20: loss 4.93 1189.59ms/step 6886tok/s (total 163,840 tok)\n",
      "step 30: loss 4.88 1196.98ms/step 6844tok/s (total 245,760 tok)\n",
      "step 40: loss 4.92 1193.92ms/step 6861tok/s (total 327,680 tok)\n",
      "step 50: loss 4.96 1186.49ms/step 6904tok/s (total 409,600 tok)\n",
      "step 50 eval: val_loss 4.93\n",
      "step 60: loss 4.83 1192.20ms/step 6871tok/s (total 491,520 tok)\n",
      "step 70: loss 4.89 1214.15ms/step 6747tok/s (total 573,440 tok)\n",
      "step 80: loss 4.99 1204.09ms/step 6803tok/s (total 655,360 tok)\n",
      "step 90: loss 4.95 1232.73ms/step 6645tok/s (total 737,280 tok)\n",
      "step 100: loss 5.01 1215.88ms/step 6738tok/s (total 819,200 tok)\n",
      "step 100 eval: val_loss 4.81\n",
      "step 110: loss 5.05 1221.89ms/step 6704tok/s (total 901,120 tok)\n",
      "step 120: loss 4.88 1220.96ms/step 6709tok/s (total 983,040 tok)\n",
      "step 130: loss 4.99 1224.10ms/step 6692tok/s (total 1,064,960 tok)\n",
      "step 140: loss 4.83 1263.16ms/step 6485tok/s (total 1,146,880 tok)\n",
      "step 150: loss 4.88 1309.07ms/step 6258tok/s (total 1,228,800 tok)\n",
      "step 150 eval: val_loss 4.97\n",
      "step 160: loss 4.99 1415.23ms/step 5788tok/s (total 1,310,720 tok)\n",
      "step 170: loss 4.91 1397.54ms/step 5862tok/s (total 1,392,640 tok)\n",
      "step 180: loss 4.90 1384.41ms/step 5917tok/s (total 1,474,560 tok)\n",
      "step 190: loss 4.87 1343.09ms/step 6099tok/s (total 1,556,480 tok)\n",
      "step 200: loss 4.92 1318.15ms/step 6215tok/s (total 1,638,400 tok)\n",
      "step 200 eval: val_loss 4.84\n",
      "step 210: loss 4.90 1296.98ms/step 6316tok/s (total 1,720,320 tok)\n",
      "step 220: loss 5.01 1246.18ms/step 6574tok/s (total 1,802,240 tok)\n",
      "step 230: loss 4.86 1268.65ms/step 6457tok/s (total 1,884,160 tok)\n",
      "step 240: loss 4.82 1246.31ms/step 6573tok/s (total 1,966,080 tok)\n",
      "step 250: loss 4.90 1268.67ms/step 6457tok/s (total 2,048,000 tok)\n",
      "step 250 eval: val_loss 4.88\n",
      "step 260: loss 4.90 1266.22ms/step 6470tok/s (total 2,129,920 tok)\n",
      "step 270: loss 4.98 1281.90ms/step 6391tok/s (total 2,211,840 tok)\n",
      "step 280: loss 4.86 1273.45ms/step 6433tok/s (total 2,293,760 tok)\n",
      "step 290: loss 4.87 1257.60ms/step 6514tok/s (total 2,375,680 tok)\n",
      "step 300: loss 4.87 1273.73ms/step 6432tok/s (total 2,457,600 tok)\n",
      "step 300 eval: val_loss 4.83\n",
      "step 310: loss 4.86 1311.34ms/step 6247tok/s (total 2,539,520 tok)\n",
      "step 320: loss 4.89 1283.11ms/step 6384tok/s (total 2,621,440 tok)\n",
      "step 330: loss 4.80 1287.51ms/step 6363tok/s (total 2,703,360 tok)\n",
      "step 340: loss 4.93 1374.74ms/step 5959tok/s (total 2,785,280 tok)\n",
      "step 350: loss 4.70 1290.50ms/step 6348tok/s (total 2,867,200 tok)\n",
      "step 350 eval: val_loss 4.82\n",
      "step 360: loss 4.73 1258.14ms/step 6511tok/s (total 2,949,120 tok)\n",
      "step 370: loss 4.89 1266.48ms/step 6468tok/s (total 3,031,040 tok)\n",
      "step 380: loss 4.74 1293.91ms/step 6331tok/s (total 3,112,960 tok)\n",
      "step 390: loss 4.85 1293.80ms/step 6332tok/s (total 3,194,880 tok)\n",
      "step 400: loss 4.90 1301.43ms/step 6295tok/s (total 3,276,800 tok)\n",
      "step 400 eval: val_loss 4.79\n",
      "step 410: loss 4.80 1297.69ms/step 6313tok/s (total 3,358,720 tok)\n",
      "step 420: loss 4.86 1300.33ms/step 6300tok/s (total 3,440,640 tok)\n",
      "step 430: loss 4.97 1326.04ms/step 6178tok/s (total 3,522,560 tok)\n",
      "step 440: loss 4.90 1329.03ms/step 6164tok/s (total 3,604,480 tok)\n",
      "step 450: loss 4.84 1320.95ms/step 6202tok/s (total 3,686,400 tok)\n",
      "step 450 eval: val_loss 4.81\n",
      "step 460: loss 4.87 1302.36ms/step 6290tok/s (total 3,768,320 tok)\n",
      "step 470: loss 4.96 1323.36ms/step 6190tok/s (total 3,850,240 tok)\n",
      "step 480: loss 4.89 1302.16ms/step 6291tok/s (total 3,932,160 tok)\n",
      "step 490: loss 4.84 1278.76ms/step 6406tok/s (total 4,014,080 tok)\n",
      "step 500: loss 4.86 1308.06ms/step 6263tok/s (total 4,096,000 tok)\n",
      "step 500 eval: val_loss 4.83\n",
      "step 510: loss 4.92 1321.43ms/step 6199tok/s (total 4,177,920 tok)\n",
      "step 520: loss 5.04 1306.62ms/step 6270tok/s (total 4,259,840 tok)\n",
      "step 530: loss 4.89 1279.14ms/step 6404tok/s (total 4,341,760 tok)\n",
      "step 540: loss 5.02 1273.11ms/step 6435tok/s (total 4,423,680 tok)\n",
      "step 550: loss 4.84 1273.98ms/step 6430tok/s (total 4,505,600 tok)\n",
      "step 550 eval: val_loss 4.82\n",
      "step 560: loss 4.95 1279.81ms/step 6401tok/s (total 4,587,520 tok)\n",
      "step 570: loss 4.94 1313.34ms/step 6238tok/s (total 4,669,440 tok)\n",
      "step 580: loss 4.90 1337.87ms/step 6123tok/s (total 4,751,360 tok)\n",
      "step 590: loss 4.95 1223.73ms/step 6694tok/s (total 4,833,280 tok)\n",
      "step 600: loss 4.88 1297.57ms/step 6313tok/s (total 4,915,200 tok)\n",
      "step 600 eval: val_loss 4.79\n",
      "step 610: loss 4.85 1298.78ms/step 6307tok/s (total 4,997,120 tok)\n",
      "step 620: loss 4.85 1325.95ms/step 6178tok/s (total 5,079,040 tok)\n",
      "step 630: loss 4.86 1241.53ms/step 6598tok/s (total 5,160,960 tok)\n",
      "step 640: loss 4.85 1282.89ms/step 6386tok/s (total 5,242,880 tok)\n",
      "step 650: loss 4.90 1289.69ms/step 6352tok/s (total 5,324,800 tok)\n",
      "step 650 eval: val_loss 4.95\n",
      "step 660: loss 4.92 1301.24ms/step 6296tok/s (total 5,406,720 tok)\n",
      "step 670: loss 4.94 1296.56ms/step 6318tok/s (total 5,488,640 tok)\n",
      "step 680: loss 4.91 1274.47ms/step 6428tok/s (total 5,570,560 tok)\n",
      "step 690: loss 4.92 1286.20ms/step 6369tok/s (total 5,652,480 tok)\n",
      "step 700: loss 4.93 1291.87ms/step 6341tok/s (total 5,734,400 tok)\n",
      "step 700 eval: val_loss 4.87\n",
      "step 710: loss 4.77 1313.16ms/step 6238tok/s (total 5,816,320 tok)\n",
      "step 720: loss 4.87 1298.71ms/step 6308tok/s (total 5,898,240 tok)\n",
      "step 730: loss 4.96 1277.52ms/step 6412tok/s (total 5,980,160 tok)\n",
      "step 740: loss 4.74 1282.60ms/step 6387tok/s (total 6,062,080 tok)\n",
      "step 750: loss 4.92 1301.70ms/step 6293tok/s (total 6,144,000 tok)\n",
      "step 750 eval: val_loss 4.94\n",
      "step 760: loss 4.87 1303.32ms/step 6285tok/s (total 6,225,920 tok)\n",
      "step 770: loss 4.84 1272.04ms/step 6440tok/s (total 6,307,840 tok)\n",
      "step 780: loss 4.93 1252.73ms/step 6539tok/s (total 6,389,760 tok)\n",
      "step 790: loss 4.89 1232.68ms/step 6646tok/s (total 6,471,680 tok)\n",
      "step 800: loss 4.83 1206.72ms/step 6789tok/s (total 6,553,600 tok)\n",
      "step 800 eval: val_loss 4.76\n",
      "step 810: loss 4.86 1218.65ms/step 6722tok/s (total 6,635,520 tok)\n",
      "step 820: loss 4.81 1212.17ms/step 6758tok/s (total 6,717,440 tok)\n",
      "step 830: loss 4.82 1238.32ms/step 6615tok/s (total 6,799,360 tok)\n",
      "step 840: loss 4.89 1234.51ms/step 6636tok/s (total 6,881,280 tok)\n",
      "step 850: loss 4.89 1235.25ms/step 6632tok/s (total 6,963,200 tok)\n",
      "step 850 eval: val_loss 4.85\n",
      "step 860: loss 4.86 1311.97ms/step 6244tok/s (total 7,045,120 tok)\n",
      "step 870: loss 4.74 1292.53ms/step 6338tok/s (total 7,127,040 tok)\n",
      "step 880: loss 4.86 1242.11ms/step 6595tok/s (total 7,208,960 tok)\n",
      "step 890: loss 4.74 1256.78ms/step 6518tok/s (total 7,290,880 tok)\n",
      "step 900: loss 4.80 1239.08ms/step 6611tok/s (total 7,372,800 tok)\n",
      "step 900 eval: val_loss 4.72\n",
      "step 910: loss 4.68 1239.02ms/step 6612tok/s (total 7,454,720 tok)\n",
      "step 920: loss 4.89 1268.25ms/step 6459tok/s (total 7,536,640 tok)\n",
      "step 930: loss 4.95 1289.20ms/step 6354tok/s (total 7,618,560 tok)\n",
      "step 940: loss 4.78 1284.73ms/step 6376tok/s (total 7,700,480 tok)\n",
      "step 950: loss 4.81 1309.98ms/step 6254tok/s (total 7,782,400 tok)\n",
      "step 950 eval: val_loss 4.73\n",
      "step 960: loss 4.76 1290.23ms/step 6349tok/s (total 7,864,320 tok)\n",
      "step 970: loss 4.76 1294.92ms/step 6326tok/s (total 7,946,240 tok)\n",
      "step 980: loss 4.71 1284.89ms/step 6376tok/s (total 8,028,160 tok)\n",
      "step 990: loss 4.80 1288.49ms/step 6358tok/s (total 8,110,080 tok)\n",
      "step 1000: loss 4.92 1262.95ms/step 6486tok/s (total 8,192,000 tok)\n",
      "step 1000 eval: val_loss 4.85\n",
      "step 1010: loss 4.75 1286.68ms/step 6367tok/s (total 8,273,920 tok)\n",
      "step 1020: loss 4.74 1266.32ms/step 6469tok/s (total 8,355,840 tok)\n",
      "step 1030: loss 4.82 1282.17ms/step 6389tok/s (total 8,437,760 tok)\n",
      "step 1040: loss 4.79 1289.84ms/step 6351tok/s (total 8,519,680 tok)\n",
      "step 1050: loss 4.78 1300.42ms/step 6299tok/s (total 8,601,600 tok)\n",
      "step 1050 eval: val_loss 4.83\n",
      "step 1060: loss 4.81 1305.59ms/step 6275tok/s (total 8,683,520 tok)\n",
      "step 1070: loss 4.76 1302.68ms/step 6289tok/s (total 8,765,440 tok)\n",
      "step 1080: loss 4.98 1312.97ms/step 6239tok/s (total 8,847,360 tok)\n",
      "step 1090: loss 4.97 1270.64ms/step 6447tok/s (total 8,929,280 tok)\n",
      "step 1100: loss 4.82 1304.65ms/step 6279tok/s (total 9,011,200 tok)\n",
      "step 1100 eval: val_loss 4.87\n",
      "step 1110: loss 4.82 1323.08ms/step 6192tok/s (total 9,093,120 tok)\n",
      "step 1120: loss 4.76 1325.62ms/step 6180tok/s (total 9,175,040 tok)\n",
      "step 1130: loss 4.88 1298.21ms/step 6310tok/s (total 9,256,960 tok)\n",
      "step 1140: loss 4.78 1310.55ms/step 6251tok/s (total 9,338,880 tok)\n",
      "step 1150: loss 4.85 1304.62ms/step 6279tok/s (total 9,420,800 tok)\n",
      "step 1150 eval: val_loss 4.77\n",
      "step 1160: loss 4.92 1305.11ms/step 6277tok/s (total 9,502,720 tok)\n",
      "step 1170: loss 4.86 1347.38ms/step 6080tok/s (total 9,584,640 tok)\n",
      "step 1180: loss 4.75 1345.97ms/step 6086tok/s (total 9,666,560 tok)\n",
      "step 1190: loss 4.88 1312.80ms/step 6240tok/s (total 9,748,480 tok)\n",
      "step 1200: loss 4.79 1321.47ms/step 6199tok/s (total 9,830,400 tok)\n",
      "step 1200 eval: val_loss 4.80\n",
      "step 1210: loss 4.88 1288.15ms/step 6359tok/s (total 9,912,320 tok)\n",
      "step 1220: loss 4.66 1328.56ms/step 6166tok/s (total 9,994,240 tok)\n",
      "step 1230: loss 4.90 1310.41ms/step 6251tok/s (total 10,076,160 tok)\n",
      "step 1240: loss 4.79 1308.72ms/step 6260tok/s (total 10,158,080 tok)\n",
      "step 1250: loss 4.78 1287.94ms/step 6361tok/s (total 10,240,000 tok)\n",
      "step 1250 eval: val_loss 4.86\n",
      "step 1260: loss 4.73 1329.46ms/step 6162tok/s (total 10,321,920 tok)\n",
      "step 1270: loss 4.86 1311.06ms/step 6248tok/s (total 10,403,840 tok)\n",
      "step 1280: loss 4.87 1286.49ms/step 6368tok/s (total 10,485,760 tok)\n",
      "step 1290: loss 4.66 1304.18ms/step 6281tok/s (total 10,567,680 tok)\n",
      "step 1300: loss 4.89 1296.15ms/step 6320tok/s (total 10,649,600 tok)\n",
      "step 1300 eval: val_loss 4.85\n",
      "step 1310: loss 4.82 1302.31ms/step 6290tok/s (total 10,731,520 tok)\n",
      "step 1320: loss 4.84 1296.87ms/step 6317tok/s (total 10,813,440 tok)\n",
      "step 1330: loss 4.79 1298.94ms/step 6307tok/s (total 10,895,360 tok)\n",
      "step 1340: loss 4.76 1288.96ms/step 6356tok/s (total 10,977,280 tok)\n",
      "step 1350: loss 4.71 1302.25ms/step 6291tok/s (total 11,059,200 tok)\n",
      "step 1350 eval: val_loss 4.77\n",
      "step 1360: loss 4.86 1316.95ms/step 6220tok/s (total 11,141,120 tok)\n",
      "step 1370: loss 4.84 1284.15ms/step 6379tok/s (total 11,223,040 tok)\n",
      "step 1380: loss 4.83 1282.32ms/step 6388tok/s (total 11,304,960 tok)\n",
      "step 1390: loss 4.93 1307.83ms/step 6264tok/s (total 11,386,880 tok)\n",
      "step 1400: loss 4.81 1325.76ms/step 6179tok/s (total 11,468,800 tok)\n",
      "step 1400 eval: val_loss 4.81\n",
      "step 1410: loss 4.92 1273.74ms/step 6431tok/s (total 11,550,720 tok)\n",
      "step 1420: loss 4.70 1283.74ms/step 6381tok/s (total 11,632,640 tok)\n",
      "step 1430: loss 4.66 1241.39ms/step 6599tok/s (total 11,714,560 tok)\n",
      "step 1440: loss 4.76 1247.69ms/step 6566tok/s (total 11,796,480 tok)\n",
      "step 1450: loss 4.73 1231.80ms/step 6650tok/s (total 11,878,400 tok)\n",
      "step 1450 eval: val_loss 4.67\n",
      "step 1460: loss 4.82 1267.59ms/step 6463tok/s (total 11,960,320 tok)\n",
      "step 1470: loss 4.87 1265.28ms/step 6474tok/s (total 12,042,240 tok)\n",
      "step 1480: loss 4.82 1374.45ms/step 5960tok/s (total 12,124,160 tok)\n",
      "step 1490: loss 4.77 1269.48ms/step 6453tok/s (total 12,206,080 tok)\n",
      "step 1500: loss 4.75 1267.88ms/step 6461tok/s (total 12,288,000 tok)\n",
      "step 1500 eval: val_loss 4.70\n",
      "step 1510: loss 4.83 1251.50ms/step 6546tok/s (total 12,369,920 tok)\n",
      "step 1520: loss 4.73 1277.60ms/step 6412tok/s (total 12,451,840 tok)\n",
      "step 1530: loss 4.87 1257.54ms/step 6514tok/s (total 12,533,760 tok)\n",
      "step 1540: loss 4.78 1252.53ms/step 6540tok/s (total 12,615,680 tok)\n",
      "step 1550: loss 4.75 1249.60ms/step 6556tok/s (total 12,697,600 tok)\n",
      "step 1550 eval: val_loss 4.85\n",
      "step 1560: loss 4.77 1251.76ms/step 6544tok/s (total 12,779,520 tok)\n",
      "step 1570: loss 4.82 1252.25ms/step 6542tok/s (total 12,861,440 tok)\n",
      "step 1580: loss 4.88 1252.89ms/step 6538tok/s (total 12,943,360 tok)\n",
      "step 1590: loss 4.76 1273.26ms/step 6434tok/s (total 13,025,280 tok)\n",
      "step 1600: loss 4.69 1245.95ms/step 6575tok/s (total 13,107,200 tok)\n",
      "step 1600 eval: val_loss 4.67\n",
      "step 1610: loss 4.62 1248.27ms/step 6563tok/s (total 13,189,120 tok)\n",
      "step 1620: loss 4.73 1224.74ms/step 6689tok/s (total 13,271,040 tok)\n",
      "step 1630: loss 4.73 1249.27ms/step 6557tok/s (total 13,352,960 tok)\n",
      "step 1640: loss 4.81 1264.77ms/step 6477tok/s (total 13,434,880 tok)\n",
      "step 1650: loss 4.71 1256.49ms/step 6520tok/s (total 13,516,800 tok)\n",
      "step 1650 eval: val_loss 4.78\n",
      "step 1660: loss 4.76 1262.04ms/step 6491tok/s (total 13,598,720 tok)\n",
      "step 1670: loss 4.81 1250.45ms/step 6551tok/s (total 13,680,640 tok)\n",
      "step 1680: loss 4.82 1263.83ms/step 6482tok/s (total 13,762,560 tok)\n",
      "step 1690: loss 4.82 1280.75ms/step 6396tok/s (total 13,844,480 tok)\n",
      "step 1700: loss 4.80 1276.12ms/step 6419tok/s (total 13,926,400 tok)\n",
      "step 1700 eval: val_loss 4.70\n",
      "step 1710: loss 4.81 1266.11ms/step 6470tok/s (total 14,008,320 tok)\n",
      "step 1720: loss 4.80 1256.46ms/step 6520tok/s (total 14,090,240 tok)\n",
      "step 1730: loss 4.91 1250.88ms/step 6549tok/s (total 14,172,160 tok)\n",
      "step 1740: loss 4.88 1253.22ms/step 6537tok/s (total 14,254,080 tok)\n",
      "step 1750: loss 4.67 1248.18ms/step 6563tok/s (total 14,336,000 tok)\n",
      "step 1750 eval: val_loss 4.78\n",
      "step 1760: loss 4.80 1258.01ms/step 6512tok/s (total 14,417,920 tok)\n",
      "step 1770: loss 4.80 1265.72ms/step 6472tok/s (total 14,499,840 tok)\n",
      "step 1780: loss 4.78 1261.22ms/step 6495tok/s (total 14,581,760 tok)\n",
      "step 1790: loss 4.86 1261.68ms/step 6493tok/s (total 14,663,680 tok)\n",
      "step 1800: loss 4.69 1316.39ms/step 6223tok/s (total 14,745,600 tok)\n",
      "step 1800 eval: val_loss 4.80\n",
      "step 1810: loss 4.85 1276.39ms/step 6418tok/s (total 14,827,520 tok)\n",
      "step 1820: loss 4.78 1272.75ms/step 6436tok/s (total 14,909,440 tok)\n",
      "step 1830: loss 4.83 1255.64ms/step 6524tok/s (total 14,991,360 tok)\n",
      "step 1840: loss 4.68 1280.25ms/step 6399tok/s (total 15,073,280 tok)\n",
      "step 1850: loss 4.77 1321.34ms/step 6200tok/s (total 15,155,200 tok)\n",
      "step 1850 eval: val_loss 4.78\n",
      "step 1860: loss 4.66 1255.64ms/step 6524tok/s (total 15,237,120 tok)\n",
      "step 1870: loss 4.77 1247.69ms/step 6566tok/s (total 15,319,040 tok)\n",
      "step 1880: loss 4.78 1258.28ms/step 6510tok/s (total 15,400,960 tok)\n",
      "step 1890: loss 4.85 1232.78ms/step 6645tok/s (total 15,482,880 tok)\n",
      "step 1900: loss 4.88 1223.57ms/step 6695tok/s (total 15,564,800 tok)\n",
      "step 1900 eval: val_loss 4.74\n",
      "step 1910: loss 4.75 1233.68ms/step 6640tok/s (total 15,646,720 tok)\n",
      "step 1920: loss 4.74 1225.89ms/step 6682tok/s (total 15,728,640 tok)\n",
      "step 1930: loss 4.78 1242.77ms/step 6592tok/s (total 15,810,560 tok)\n",
      "step 1940: loss 4.67 1234.81ms/step 6634tok/s (total 15,892,480 tok)\n",
      "step 1950: loss 4.62 1243.92ms/step 6586tok/s (total 15,974,400 tok)\n",
      "step 1950 eval: val_loss 4.75\n",
      "step 1960: loss 4.76 1251.10ms/step 6548tok/s (total 16,056,320 tok)\n",
      "step 1970: loss 4.74 1239.84ms/step 6607tok/s (total 16,138,240 tok)\n",
      "step 1980: loss 4.91 1252.47ms/step 6541tok/s (total 16,220,160 tok)\n",
      "step 1990: loss 4.62 1256.93ms/step 6517tok/s (total 16,302,080 tok)\n",
      "step 2000: loss 4.86 1248.56ms/step 6561tok/s (total 16,384,000 tok)\n",
      "step 2000 eval: val_loss 4.70\n",
      "step 2010: loss 4.78 1255.38ms/step 6526tok/s (total 16,465,920 tok)\n",
      "step 2020: loss 4.75 1238.07ms/step 6617tok/s (total 16,547,840 tok)\n",
      "step 2030: loss 4.77 1240.74ms/step 6603tok/s (total 16,629,760 tok)\n",
      "step 2040: loss 4.81 1221.55ms/step 6706tok/s (total 16,711,680 tok)\n",
      "step 2050: loss 4.70 1262.20ms/step 6490tok/s (total 16,793,600 tok)\n",
      "step 2050 eval: val_loss 4.81\n",
      "step 2060: loss 4.73 1270.66ms/step 6447tok/s (total 16,875,520 tok)\n",
      "step 2070: loss 4.93 1297.49ms/step 6314tok/s (total 16,957,440 tok)\n",
      "step 2080: loss 4.77 1275.08ms/step 6425tok/s (total 17,039,360 tok)\n",
      "step 2090: loss 4.73 1268.71ms/step 6457tok/s (total 17,121,280 tok)\n",
      "step 2100: loss 4.85 1281.84ms/step 6391tok/s (total 17,203,200 tok)\n",
      "step 2100 eval: val_loss 4.60\n",
      "step 2110: loss 4.67 1286.80ms/step 6366tok/s (total 17,285,120 tok)\n",
      "step 2120: loss 4.77 1576.38ms/step 5197tok/s (total 17,367,040 tok)\n",
      "step 2130: loss 4.66 1384.67ms/step 5916tok/s (total 17,448,960 tok)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m step_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m curr_time\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "log_interval = 10\n",
    "eval_interval = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "curr_time = time.time()\n",
    "tok_total = 0 \n",
    "\n",
    "for curr_step in range(1, 4000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    tok_step = xb.view(-1).size(0)\n",
    "    tok_total += tok_step\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    step_time = time.time() - curr_time\n",
    "    if curr_step % log_interval == 0:\n",
    "        print(f\"step {curr_step}: loss {loss.detach().item():.2f} {step_time*1000:.2f}ms/step {(tok_step/step_time):.0f}tok/s (total {tok_total:,} tok)\")\n",
    "\n",
    "    if curr_step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        xb, yb = get_sample('validation', block_size, batch_size)\n",
    "        logits, loss = model(xb.to(device), yb.to(device))\n",
    "        print(f\"step {curr_step} eval: val_loss {loss.detach().item():.2f}\")\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(encode(\"hello my name is\").to(device).unsqueeze(0), 10)\n",
    "        #     print(decode(output))\n",
    "        model.train()\n",
    "    curr_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1687011d-408c-49ae-be73-13fed06fe982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This setup will be placed inside brakes.\n",
      "- Address for more visible traffic behaviour, improve performance, cloud tools, systems, in commands, charging, electric/verresolveably pressing from drive.\n",
      "- Panel 4: A Contract also flex joins following\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"This\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e398cf47-8f5d-48dc-9f24-ddb34ce6fe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello & More EICK N Page! honored by Yes!!'To stay true | TODAY!Thus when ||rez | St David Erep e\n",
      "for essential date : Kanow for underground mine with ice\n",
      "disasterful wagon ship opened: Brian Wol\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"Hello\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e121a884-062f-4aac-ad41-70b9a75ee330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML\n",
      "verb Statistics and graphicalization papers can be applied to a text for color and use of their first or best sentence conversions to use files or nonverbal symbols. The extra position is even more than spend some time just a week. Once downloaded from the\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(encode(\"HTML\").to(device).unsqueeze(0), 50)\n",
    "    print(decode(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
