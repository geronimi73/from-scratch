{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b656c720-474c-4843-98be-51a17f86d322",
   "metadata": {},
   "source": [
    "* add MFU calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7d9c4a-630c-40e0-94e4-86cbc08c0581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 10000/10000 [00:06<00:00, 1643.43it/s]\n",
      "100%|█████████████████████████████████████████████| 10000/10000 [00:05<00:00, 1985.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import torch \n",
    "import datasets\n",
    "import random\n",
    "\n",
    "dataset = datasets.load_dataset(\"HuggingFaceFW/fineweb-edu\", data_files=[\"sample/10BT/000_00000.parquet\"], split=\"train\")\n",
    "dataset = dataset.train_test_split()\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
    "\n",
    "def encode(string):\n",
    "    return torch.tensor(enc.encode(string), dtype=torch.long)\n",
    "\n",
    "def decode(tensor):\n",
    "    return enc.decode(tensor.cpu().squeeze().numpy())\n",
    "\n",
    "num_samples = 10_000\n",
    "dataset_tok_train = torch.cat([encode(dataset[\"train\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "dataset_tok_test = torch.cat([encode(dataset[\"test\"][i][\"text\"]) for i in tqdm(range(num_samples))])\n",
    "\n",
    "def get_sample(split, sample_length, batch_size):\n",
    "    tokens = dataset_tok_train if split == \"train\" else dataset_tok_test\n",
    "    idcs = torch.randint(len(tokens)-sample_length, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
    "    y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "869a87db-8bba-4726-aaea-0ce5603f8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 10,276,684 tokens\n",
      "Test data: 10,426,816 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data: {len(dataset_tok_train):,} tokens\")\n",
    "print(f\"Test data: {len(dataset_tok_test):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5a8b7d-7ece-41bc-89fe-443d5f5d1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        # out = F.scaled_dot_product_attention(q, k, v)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b100d83e-dba3-43fd-9386-1fc1d9eacf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.34896 M parameters\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50_272\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "block_size = 128 \n",
    "device = \"mps\"\n",
    "learning_rate = 3e-4\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da080e46-055e-49a5-82ea-033a6d3d465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.004555249994155\n",
      "3.005257291981252\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with elapsed_timer() as elapsed:\n",
    "    time.sleep(1)\n",
    "    print(elapsed())\n",
    "    time.sleep(2)\n",
    "    print(elapsed())\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "82666bdb-eb92-4cb5-82c2-b5ea3cd66ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "\n",
    "def get_flops_achieved(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    total_flops = flop_counter.get_total_flops()\n",
    "\n",
    "    \n",
    "    with elapsed_timer() as elapsed:\n",
    "        f()\n",
    "    \n",
    "    s_per_iter = elapsed()\n",
    "    # iters_per_second = 1/s_per_iter\n",
    "    print(f\"{s_per_iter}s/iter, {total_flops}flops, {total_flops / s_per_iter / 1e12} TF/s\")\n",
    "\n",
    "def train_one_sample():\n",
    "    xb, yb = get_sample('train', block_size, batch_size)\n",
    "    logits, loss = model(xb.to(device), yb.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "train_one_sample()\n",
    "# get_flops_achieved(train_one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "de86d694-7d39-409f-a0b9-9e1f319797ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8047828749986365, 0.03602341718323606 TF/s\n"
     ]
    }
   ],
   "source": [
    "model = model.half()\n",
    "get_flops_achieved(train_one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d354c97a-bd11-487e-97ba-c79791a9bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0035593509674072266\n",
      "0.004293918609619141\n",
      "0.0035490989685058594\n",
      "0.0035119056701660156\n",
      "0.004039287567138672\n",
      "0.0033559799194335938\n",
      "0.0034139156341552734\n",
      "0.0032520294189453125\n",
      "0.0016567707061767578\n",
      "0.0016558170318603516\n",
      "0.0016429424285888672\n",
      "0.0016100406646728516\n",
      "0.0015971660614013672\n",
      "0.00144195556640625\n",
      "0.0014500617980957031\n",
      "0.0013990402221679688\n",
      "0.0014350414276123047\n",
      "0.00144195556640625\n",
      "0.0014979839324951172\n",
      "0.0015919208526611328\n",
      "0.0016698837280273438\n",
      "0.0016667842864990234\n",
      "0.0016551017761230469\n",
      "0.0015749931335449219\n",
      "0.0014240741729736328\n",
      "0.0014469623565673828\n",
      "0.0014700889587402344\n",
      "0.0014073848724365234\n",
      "0.0014209747314453125\n",
      "0.001592874526977539\n",
      "0.0016260147094726562\n",
      "0.0016450881958007812\n",
      "0.0017039775848388672\n",
      "0.0019021034240722656\n",
      "0.001589059829711914\n",
      "0.0014939308166503906\n",
      "0.0014808177947998047\n",
      "0.001483917236328125\n",
      "0.0014750957489013672\n",
      "0.0015442371368408203\n",
      "0.0016360282897949219\n",
      "0.0016732215881347656\n",
      "0.0016379356384277344\n",
      "0.001672983169555664\n",
      "0.0015940666198730469\n",
      "0.0014557838439941406\n",
      "0.001447916030883789\n",
      "0.0014431476593017578\n",
      "0.0014619827270507812\n",
      "0.0014488697052001953\n",
      "0.00167083740234375\n",
      "0.0016760826110839844\n",
      "0.001667022705078125\n",
      "0.0016889572143554688\n",
      "0.0016739368438720703\n",
      "0.0017659664154052734\n",
      "0.0015990734100341797\n",
      "0.0015549659729003906\n",
      "0.0016241073608398438\n",
      "0.0015630722045898438\n",
      "0.0017359256744384766\n",
      "0.0017218589782714844\n",
      "0.0018181800842285156\n",
      "0.0017099380493164062\n",
      "0.0016832351684570312\n",
      "0.00164794921875\n",
      "0.0015146732330322266\n",
      "0.0014579296112060547\n",
      "0.0014719963073730469\n",
      "0.0014300346374511719\n",
      "0.001573801040649414\n",
      "0.0016620159149169922\n",
      "0.0017619132995605469\n",
      "0.0017421245574951172\n",
      "0.0017337799072265625\n",
      "0.0017242431640625\n",
      "0.0016279220581054688\n",
      "0.0014390945434570312\n",
      "0.0014739036560058594\n",
      "0.00146484375\n",
      "0.001547098159790039\n",
      "0.0016641616821289062\n",
      "0.0016620159149169922\n",
      "0.0016608238220214844\n",
      "0.0016770362854003906\n",
      "0.0016298294067382812\n",
      "0.0016198158264160156\n",
      "0.0014750957489013672\n",
      "0.0014100074768066406\n",
      "0.001402139663696289\n",
      "0.0013880729675292969\n",
      "0.0015518665313720703\n",
      "0.0015921592712402344\n",
      "0.0015769004821777344\n",
      "0.0015859603881835938\n",
      "0.0015709400177001953\n",
      "0.0015599727630615234\n",
      "0.0015532970428466797\n",
      "0.0015459060668945312\n",
      "0.0013990402221679688\n",
      "0.0017403817176818847s/iter, 2147483648flops, 1.2339153107516885 TF/s\n"
     ]
    }
   ],
   "source": [
    "from torch import mps\n",
    "import time \n",
    "n = 1024\n",
    "a = torch.rand(n, n, device=\"mps\")\n",
    "num_timetrials = 100\n",
    "\n",
    "def get_flops_achieved(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    total_flops = flop_counter.get_total_flops()\n",
    "\n",
    "    s_total = 0\n",
    "    for _ in range(num_timetrials):\n",
    "        mps.synchronize()\n",
    "        start = time.time()\n",
    "        f()\n",
    "        mps.synchronize()\n",
    "        s_total += time.time() - start\n",
    "        print(time.time() - start)\n",
    "    \n",
    "    s_per_iter = s_total / num_timetrials\n",
    "    # iters_per_second = 1/s_per_iter\n",
    "    print(f\"{s_per_iter}s/iter, {total_flops}flops, {total_flops / s_per_iter / 1e12} TF/s\")\n",
    "\n",
    "def single_matmul():\n",
    "    b = torch.matmul(a, a)\n",
    "\n",
    "get_flops_achieved(single_matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "0997ba8b-2132-4d99-89cd-bedcad9b0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2116630792617798s/iter, 9663676416flops, 0.04565593796378724 TF/s\n"
     ]
    }
   ],
   "source": [
    "from torch import mps\n",
    "import time \n",
    "n = 1024\n",
    "a = torch.rand(n, n, device=\"mps\")\n",
    "num_timetrials = 10\n",
    "\n",
    "def get_flops_achieved(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    total_flops = flop_counter.get_total_flops()\n",
    "\n",
    "    s_total = 0\n",
    "    for _ in range(num_timetrials):\n",
    "        mps.synchronize()\n",
    "        start = time.time()\n",
    "        f()\n",
    "        mps.synchronize()\n",
    "        s_total += time.time() - start\n",
    "    \n",
    "    s_per_iter = s_total / num_timetrials\n",
    "    # iters_per_second = 1/s_per_iter\n",
    "    print(f\"{s_per_iter}s/iter, {total_flops}flops, {total_flops / s_per_iter / 1e12} TF/s\")\n",
    "\n",
    "xb, yb = get_sample('train', block_size, batch_size)\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)\n",
    "\n",
    "def train_one_sample():\n",
    "    model(xb)\n",
    "    # logits, loss = model(xb, yb)\n",
    "    # loss.backward()\n",
    "\n",
    "get_flops_achieved(train_one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "65345f94-3885-493b-a035-9359967054b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25863964557647706s/iter, 9663676416flops, 0.037363476873241194 TF/s\n"
     ]
    }
   ],
   "source": [
    "def train_one_sample():\n",
    "    # model(xb)\n",
    "    logits, loss = model(xb, yb)\n",
    "    # loss.backward()\n",
    "\n",
    "get_flops_achieved(train_one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "eea4ba31-b0c3-4ed6-ba54-239c06e958aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8026062965393066s/iter, 28991029248flops, 0.03612110865938142 TF/s\n"
     ]
    }
   ],
   "source": [
    "def train_one_sample():\n",
    "    logits, loss = model(xb, yb)\n",
    "    loss.backward()\n",
    "\n",
    "get_flops_achieved(train_one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "55c16302-5d56-48d5-a411-febe682e3cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2135932445526123s/iter, 9663676416flops, 0.04524336168141143 TF/s\n",
      "0.2108713150024414s/iter, 9663676416flops, 0.04582736355529493 TF/s\n",
      "0.21042635440826415s/iter, 9663676416flops, 0.045924268579261546 TF/s\n",
      "0.2622026205062866s/iter, 9663676416flops, 0.036855758334300485 TF/s\n",
      "0.21017711162567138s/iter, 9663676416flops, 0.045978728802835364 TF/s\n",
      "0.2583198308944702s/iter, 9663676416flops, 0.03740973498835961 TF/s\n",
      "0.2100447654724121s/iter, 9663676416flops, 0.046007699331451594 TF/s\n",
      "0.23678829669952392s/iter, 9663676416flops, 0.040811461337816324 TF/s\n",
      "0.2257081985473633s/iter, 9663676416flops, 0.042814910925675326 TF/s\n",
      "0.22636716365814208s/iter, 9663676416flops, 0.042690274772334065 TF/s\n"
     ]
    }
   ],
   "source": [
    "from torch import mps\n",
    "import time \n",
    "n = 1024\n",
    "a = torch.rand(n, n, device=\"mps\")\n",
    "num_timetrials = 10\n",
    "\n",
    "def get_flops_achieved(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    total_flops = flop_counter.get_total_flops()\n",
    "\n",
    "    s_total = 0\n",
    "    for _ in range(num_timetrials):\n",
    "        # mps.synchronize()\n",
    "        start = time.time()\n",
    "        f()\n",
    "        # mps.synchronize()\n",
    "        s_total += time.time() - start\n",
    "    \n",
    "    s_per_iter = s_total / num_timetrials\n",
    "    # iters_per_second = 1/s_per_iter\n",
    "    print(f\"{s_per_iter}s/iter, {total_flops}flops, {total_flops / s_per_iter / 1e12} TF/s\")\n",
    "\n",
    "xb, yb = get_sample('train', block_size, batch_size)\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)\n",
    "\n",
    "def train_one_sample():\n",
    "    model(xb)\n",
    "    # logits, loss = model(xb, yb)\n",
    "    # loss.backward()\n",
    "\n",
    "for _ in range(10):\n",
    "    get_flops_achieved(train_one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "8b102739-4241-46ee-81d3-64842beb160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.stack([torch.tensor(tokens[x:x+sample_length]) for x in idcs])\n",
      "/var/folders/fd/9xbk36z13vx8c3h6tbf6qchm0000gn/T/ipykernel_51261/3346530761.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.stack([torch.tensor(tokens[x+1:x+sample_length+1]) for x in idcs])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21155788898468017s/iter, 9663676416flops, 0.04567863889348882 TF/s\n",
      "0.23959777355194092s/iter, 9663676416flops, 0.04033291408655378 TF/s\n",
      "0.23498382568359374s/iter, 9663676416flops, 0.04112485779771142 TF/s\n",
      "0.22499163150787355s/iter, 9663676416flops, 0.04295127045941627 TF/s\n",
      "0.24395358562469482s/iter, 9663676416flops, 0.03961276646643299 TF/s\n",
      "0.2253368616104126s/iter, 9663676416flops, 0.042885466438720696 TF/s\n",
      "0.256885027885437s/iter, 9663676416flops, 0.037618682939784674 TF/s\n",
      "0.24661767482757568s/iter, 9663676416flops, 0.03918484927228521 TF/s\n",
      "0.24898619651794435s/iter, 9663676416flops, 0.03881209702042074 TF/s\n",
      "0.33020141124725344s/iter, 9663676416flops, 0.02926600579778831 TF/s\n"
     ]
    }
   ],
   "source": [
    "from torch import mps\n",
    "import time \n",
    "n = 1024\n",
    "a = torch.rand(n, n, device=\"mps\")\n",
    "num_timetrials = 10\n",
    "\n",
    "def get_flops_achieved(f):\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        f()\n",
    "    total_flops = flop_counter.get_total_flops()\n",
    "\n",
    "    s_total = 0\n",
    "    for _ in range(num_timetrials):\n",
    "        mps.synchronize()\n",
    "        start = time.time()\n",
    "        f()\n",
    "        mps.synchronize()\n",
    "        s_total += time.time() - start\n",
    "    \n",
    "    s_per_iter = s_total / num_timetrials\n",
    "    # iters_per_second = 1/s_per_iter\n",
    "    print(f\"{s_per_iter}s/iter, {total_flops}flops, {total_flops / s_per_iter / 1e12} TF/s\")\n",
    "\n",
    "xb, yb = get_sample('train', block_size, batch_size)\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)\n",
    "\n",
    "def train_one_sample():\n",
    "    model(xb)\n",
    "    # logits, loss = model(xb, yb)\n",
    "    # loss.backward()\n",
    "\n",
    "for _ in range(10):\n",
    "    get_flops_achieved(train_one_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
